{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2ea9903-2586-4537-bce7-bf5813c8a9da",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85054505-4c28-4331-8cb0-0b6b5cf375e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import datasets\n",
    "import asyncio\n",
    "from tqdm import tqdm\n",
    "from typing import Callable\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "\n",
    "import os\n",
    "import psutil\n",
    "import GPUtil\n",
    "import gc\n",
    "\n",
    "from groq import AsyncGroq, RateLimitError\n",
    "\n",
    "import _config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6080de50-c965-4358-9321-ce8ce83cb47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENABLE_THINKING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "701e04b4-4b81-4d00-aa1e-662478029fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = _config.WANDB_API_KEY\n",
    "os.environ[\"WANDB_PROJECT\"] = _config.WANDB_PROJECT\n",
    "os.environ[\"GROQ_API_KEY\"] = _config.GROQ_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72715348-b3bb-4055-be2d-e6cdeecfb785",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31ea64fe-5683-4ee8-a127-d8e365308a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "CPU 0 load: 1.00\n",
      "RAM Total: 1.86 GB, Used: 0.84 GB\n",
      "Disk Total: 28.02 GB, Used: 19.70 GB\n"
     ]
    }
   ],
   "source": [
    "def get_vm_usage_metrics():\n",
    "    # CPU usage\n",
    "    cpu_load = psutil.cpu_percent(interval=1, percpu=True)\n",
    "    for id, load in enumerate(cpu_load):\n",
    "        print(f\"CPU {id} load: {load:.2f}\")\n",
    "    # RAM usage\n",
    "    ram = psutil.virtual_memory()\n",
    "    print(f\"RAM Total: {ram.total/(1024**3):.2f} GB, Used: {(ram.used)/(1024**3):.2f} GB\")\n",
    "    # GPU\n",
    "    if torch.cuda.is_available():\n",
    "        gpus = GPUtil.getGPUs()\n",
    "        for gpu in gpus:\n",
    "            print(f\"GPU {gpu.id} ({gpu.name}) load: {gpu.load*100}%\")\n",
    "            print(f\"GPU {gpu.id} ({gpu.name}) VRAM Total: {gpu.memoryTotal} MB, Used {gpu.memoryUsed} MB\")\n",
    "    # Disk \n",
    "    disk = psutil.disk_usage('/')\n",
    "    print(f\"Disk Total: {disk.total/(1024**3):.2f} GB, Used: {(disk.used)/(1024**3):.2f} GB\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f'Device: {device}')\n",
    "get_vm_usage_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5741a69-4855-4a3d-ab7b-b092f35f4a3c",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "744641d8-17ae-469a-a0a4-339a50eeb3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU 0 load: 1.00\n",
      "RAM Total: 1.86 GB, Used: 0.84 GB\n",
      "Disk Total: 28.02 GB, Used: 19.70 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'domain', 'domain_description', 'sql_complexity', 'sql_complexity_description', 'sql_task_type', 'sql_task_type_description', 'sql_prompt', 'sql_context', 'sql', 'sql_explanation'],\n",
       "    num_rows: 97500\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = datasets.load_dataset('gretelai/synthetic_text_to_sql', streaming=False)\n",
    "ds_train, ds_test = ds['train'], ds['test']\n",
    "\n",
    "split = ds_train.train_test_split(test_size=0.025, seed=42)\n",
    "ds_train = split['train']\n",
    "ds_valid = split['test']\n",
    "\n",
    "get_vm_usage_metrics()\n",
    "ds_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f2d333-76ad-4ffc-86c9-446b9febea69",
   "metadata": {},
   "source": [
    "# Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f279714-b099-4528-9b22-656b3fad9e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYS_PROMPT = \"\"\"You are a text-to-SQL data generator. You will be provided with a user prompt (sql_prompt), \n",
    "the query context (sql_context), and the correct SQL query (sql) that answers the user's question.\n",
    "Your task is to generate an alternative SQL query that is worse than the provided SQL. Your query must \n",
    "contain at least one error - this is very important. Do not return the original query.\n",
    "Return only the changed query, do not name it, do not explain it. Remeber to add at least one error.\n",
    "\n",
    "SQL_PROMPT: \"{sql_prompt}\"\n",
    "SQL_CONTEXT: \"{sql_context}\"\n",
    "SQL: \"{sql}\"\n",
    "\"\"\"\n",
    "\n",
    "models_available = [\n",
    "    'llama-3.1-8b-instant',\n",
    "    'llama-3.3-70b-versatile',\n",
    "    'meta-llama/llama-4-maverick-17b-128e-instruct',\n",
    "    'meta-llama/llama-4-scout-17b-16e-instruct',\n",
    "    'moonshotai/kimi-k2-instruct',\n",
    "    'moonshotai/kimi-k2-instruct-0905',\n",
    "    'openai/gpt-oss-120b',\n",
    "    'openai/gpt-oss-20b',\n",
    "    'qwen/qwen3-32b'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05259057-8796-4291-81a5-5f3ac5fc1eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:   3%|█▊                                                                 | 2671/97500 [14:18<12:10:33,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model llama-3.3-70b-versatile is no longer available (Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01kep40x18enbt3npfx9edhkwf` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99874, Requested 209. Please try again in 1m11.712s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:   3%|█▉                                                                  | 2776/97500 [15:00<8:50:54,  2.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model openai/gpt-oss-20b is no longer available (Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01kep40x18enbt3npfx9edhkwf` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199713, Requested 309. Please try again in 9.504s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:   4%|██▍                                                                | 3531/97500 [20:06<10:15:12,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model openai/gpt-oss-120b is no longer available (Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-120b` in organization `org_01kep40x18enbt3npfx9edhkwf` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199759, Requested 481. Please try again in 1m43.68s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:   4%|██▋                                                                | 3829/97500 [22:20<15:34:45,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other error: Error code: 503 - {'error': {'message': 'meta-llama/llama-4-maverick-17b-128e-instruct is currently over capacity. Please try again and back off exponentially. Visit https://groqstatus.com to see if there is an active incident.', 'type': 'internal_server_error'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:   4%|██▋                                                                | 3999/97500 [23:34<14:06:39,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model qwen/qwen3-32b is no longer available (Error code: 429 - {'error': {'message': 'Rate limit reached for model `qwen/qwen3-32b` in organization `org_01kep40x18enbt3npfx9edhkwf` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499905, Requested 475. Please try again in 1m5.664s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:   6%|███▋                                                               | 5424/97500 [33:57<16:32:13,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model moonshotai/kimi-k2-instruct is no longer available (Error code: 429 - {'error': {'message': 'Rate limit reached for model `moonshotai/kimi-k2-instruct` in organization `org_01kep40x18enbt3npfx9edhkwf` service tier `on_demand` on requests per day (RPD): Limit 1000, Used 1000, Requested 1. Please try again in 1m26.4s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'requests', 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:   6%|███▋                                                               | 5430/97500 [34:00<12:34:47,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model moonshotai/kimi-k2-instruct-0905 is no longer available (Error code: 429 - {'error': {'message': 'Rate limit reached for model `moonshotai/kimi-k2-instruct-0905` in organization `org_01kep40x18enbt3npfx9edhkwf` service tier `on_demand` on requests per day (RPD): Limit 1000, Used 1000, Requested 1. Please try again in 1m26.4s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'requests', 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:   6%|████                                                               | 5980/97500 [42:12<29:06:54,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/llama-4-scout-17b-16e-instruct is no longer available (Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `org_01kep40x18enbt3npfx9edhkwf` service tier `on_demand` on requests per day (RPD): Limit 1000, Used 1000, Requested 1. Please try again in 1m26.4s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'requests', 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:   6%|████▏                                                              | 6175/97500 [46:59<40:36:15,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/llama-4-maverick-17b-128e-instruct is no longer available (Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-maverick-17b-128e-instruct` in organization `org_01kep40x18enbt3npfx9edhkwf` service tier `on_demand` on requests per day (RPD): Limit 1000, Used 1000, Requested 1. Please try again in 1m26.4s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'requests', 'code': 'rate_limit_exceeded'}})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:   7%|████▍                                                              | 6411/97500 [59:06<82:01:02,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 6: Task failed with error: Exceeded maximum number of retries (50) or no models available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:   7%|████▍                                                              | 6416/97500 [59:23<81:51:21,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 9: Task failed with error: Exceeded maximum number of retries (50) or no models available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:   7%|████▎                                                            | 6431/97500 [1:00:07<71:22:33,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 3: Task failed with error: Exceeded maximum number of retries (50) or no models available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:   7%|████▎                                                            | 6501/97500 [1:03:47<78:55:07,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 1: Task failed with error: Exceeded maximum number of retries (50) or no models available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:   7%|████▎                                                            | 6503/97500 [1:03:52<74:24:07,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 7: Task failed with error: Exceeded maximum number of retries (50) or no models available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:   7%|████▎                                                            | 6554/97500 [1:06:28<74:09:23,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 2: Task failed with error: Exceeded maximum number of retries (50) or no models available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:   7%|████▍                                                            | 6563/97500 [1:06:59<85:07:29,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 4: Task failed with error: Exceeded maximum number of retries (50) or no models available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:   7%|████▍                                                            | 6607/97500 [1:09:09<79:41:55,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 3: Task failed with error: Exceeded maximum number of retries (50) or no models available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:   7%|████▍                                                            | 6633/97500 [1:10:27<78:31:07,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 9: Task failed with error: Exceeded maximum number of retries (50) or no models available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:   7%|████▍                                                            | 6660/97500 [1:11:45<74:04:29,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 5: Task failed with error: Exceeded maximum number of retries (50) or no models available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:   7%|████▍                                                            | 6691/97500 [1:13:18<72:45:06,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 0: Task failed with error: Exceeded maximum number of retries (50) or no models available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:   7%|████▍                                                            | 6704/97500 [1:13:57<74:43:44,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 2: Task failed with error: Exceeded maximum number of retries (50) or no models available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:   7%|████▍                                                            | 6719/97500 [1:14:41<82:31:37,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 1: Task failed with error: Exceeded maximum number of retries (50) or no models available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:   7%|████▍                                                            | 6748/97500 [1:16:11<76:44:16,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 4: Task failed with error: Exceeded maximum number of retries (50) or no models available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:   7%|████▌                                                            | 6788/97500 [1:18:15<82:36:07,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 8: Task failed with error: Exceeded maximum number of retries (50) or no models available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:   7%|████▌                                                            | 6847/97500 [1:21:15<75:22:48,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 9: Task failed with error: Exceeded maximum number of retries (50) or no models available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:   7%|████▌                                                            | 6874/97500 [1:22:32<69:04:49,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 0: Task failed with error: Exceeded maximum number of retries (50) or no models available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:   7%|████▌                                                            | 6883/97500 [1:22:56<72:39:55,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 1: Task failed with error: Exceeded maximum number of retries (50) or no models available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:   7%|████▌                                                            | 6930/97500 [1:25:16<71:40:19,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 5: Task failed with error: Exceeded maximum number of retries (50) or no models available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:   7%|████▌                                                            | 6933/97500 [1:25:24<69:44:47,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 6: Task failed with error: Exceeded maximum number of retries (50) or no models available\n",
      "Worker 3: Task failed with error: Exceeded maximum number of retries (50) or no models available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:   7%|████▋                                                            | 6942/97500 [1:25:52<75:52:50,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model llama-3.1-8b-instant is no longer available (Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01kep40x18enbt3npfx9edhkwf` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499734, Requested 346. Please try again in 13.824s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}})\n",
      "Worker 2: Task failed with error: Exceeded maximum number of retries (50) or no models available\n",
      "Worker 2: No models available. Stopping.\n",
      "No models available, stopping workers...\n",
      "Stop event triggered, waiting for workers to finish current tasks...\n",
      "Worker 0: Task failed with error: Exceeded maximum number of retries (50) or no models available\n",
      "Worker 9: Task failed with error: Exceeded maximum number of retries (50) or no models available\n",
      "Worker 3: Task failed with error: Exceeded maximum number of retries (50) or no models available\n",
      "Worker 1: Task failed with error: Exceeded maximum number of retries (50) or no models available\n",
      "Worker 8: Task failed with error: Exceeded maximum number of retries (50) or no models available\n",
      "Worker 5: Task failed with error: Exceeded maximum number of retries (50) or no models available\n",
      "Worker 6: Task failed with error: Exceeded maximum number of retries (50) or no models available\n",
      "Worker 7: Task failed with error: Exceeded maximum number of retries (50) or no models available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples:   7%|████▋                                                            | 6943/97500 [1:25:54<69:58:43,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 90527 tasks were not processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples: 100%|████████████████████████████████████████████████████████████████████▋| 6943/6973 [1:25:54<00:22,  1.35it/s]\n"
     ]
    }
   ],
   "source": [
    "N_TASKS = len(ds_train)\n",
    "N_CONCURRENT_TASKS = 10\n",
    "\n",
    "client = AsyncGroq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "api_semaphore = asyncio.Semaphore(N_CONCURRENT_TASKS)\n",
    "\n",
    "\n",
    "async def create_chat_completion(prompt_args, callback: Callable = None, client=client, base_delay=5, max_retries=50):\n",
    "    if not models_available:\n",
    "        raise Exception(\"No models available\")\n",
    "        \n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        if not models_available:\n",
    "            break\n",
    "            \n",
    "        async with api_semaphore:\n",
    "            try:\n",
    "                model = random.choice(models_available)\n",
    "                prompt = SYS_PROMPT.format(\n",
    "                    sql_prompt=prompt_args['sql_prompt'],\n",
    "                    sql_context=prompt_args['sql_context'],\n",
    "                    sql=prompt_args['sql']\n",
    "                )\n",
    "                chat_completion = await client.chat.completions.create(\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": prompt,\n",
    "                        }\n",
    "                    ],\n",
    "                    model=model,\n",
    "                )\n",
    "                if chat_completion.choices[0].message.content != prompt_args['sql']:\n",
    "                    if callback:\n",
    "                        callback()\n",
    "                    return {\n",
    "                        'sql_prompt': prompt_args['sql_prompt'],\n",
    "                        'sql_context': prompt_args['sql_context'],\n",
    "                        'sql': prompt_args['sql'],\n",
    "                        'model_used': model,\n",
    "                        'completion': chat_completion.choices[0].message.content\n",
    "                    }\n",
    "                \n",
    "            except RateLimitError as e:\n",
    "                msg = str(e)\n",
    "                if (\"TPD\" in msg) or (\"RPD\" in msg):\n",
    "                    if model in models_available:\n",
    "                        models_available.remove(model)\n",
    "                        print(f\"Model {model} is no longer available ({e})\", flush=True)\n",
    "                else:    \n",
    "                    wait_time = base_delay * (2 ** (attempt - 1))\n",
    "                    retry_after_match = re.search(r'Please try again in (\\d+(?:\\.\\d+)?)s', msg)\n",
    "                    if retry_after_match:\n",
    "                        wait_time = float(retry_after_match.group(1))\n",
    "                    # print(f'RateLimitError: attempt {attempt} out of {max_retries}, waiting {wait_time}s...', flush=True)\n",
    "                    await asyncio.sleep(wait_time)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f'Other error: {e}', flush=True)\n",
    "                \n",
    "    # print(f'Exceeded maximum number of retries ({max_retries}) or no models available', flush=True)\n",
    "    raise Exception(f'Exceeded maximum number of retries ({max_retries}) or no models available')\n",
    "\n",
    "\n",
    "\n",
    "async def worker(worker_id, task_queue, results, pbar, stop_event):\n",
    "    \"\"\"Worker that processes tasks from the queue\"\"\"\n",
    "    while not task_queue.empty() and not stop_event.is_set():\n",
    "        try:\n",
    "            try:\n",
    "                prompt_args = await asyncio.wait_for(task_queue.get(), timeout=1.0)\n",
    "            except asyncio.TimeoutError: # check stop_event\n",
    "                continue\n",
    "            \n",
    "            if not models_available:\n",
    "                print(f\"Worker {worker_id}: No models available. Stopping.\", flush=True)\n",
    "                task_queue.put_nowait(prompt_args)\n",
    "                stop_event.set()\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                result = await create_chat_completion(\n",
    "                    prompt_args=prompt_args,\n",
    "                    callback=lambda: pbar.update(1)\n",
    "                )\n",
    "                results.append(result)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Worker {worker_id}: Task failed with error: {e}\", flush=True)\n",
    "                \n",
    "                if \"No models available\" in str(e):\n",
    "                    print(f\"Worker {worker_id}: Stopping due to no models available\", flush=True)\n",
    "                    task_queue.put_nowait(prompt_args)\n",
    "                    stop_event.set()\n",
    "                    break\n",
    "                    \n",
    "            finally:\n",
    "                task_queue.task_done()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Worker {worker_id}: Unexpected error: {e}\", flush=True)\n",
    "\n",
    "    \n",
    "\n",
    "async def main(dataset):\n",
    "    \"\"\"Main execution using queue-based approach\"\"\"\n",
    "    if not models_available:\n",
    "        print(\"ERROR: No models available at start. Cannot run any tasks.\", flush=True)\n",
    "        return []\n",
    "    \n",
    "    with tqdm(total=N_TASKS, desc='Generating samples') as pbar:\n",
    "        task_queue = asyncio.Queue()\n",
    "        results = []\n",
    "        stop_event = asyncio.Event()\n",
    "        \n",
    "        for i in range(N_TASKS):\n",
    "            await task_queue.put({\n",
    "                'sql_prompt': dataset[i]['sql_prompt'],\n",
    "                'sql_context': dataset[i]['sql_context'],\n",
    "                'sql': dataset[i]['sql']\n",
    "            })\n",
    "        \n",
    "        workers = []\n",
    "        for i in range(N_CONCURRENT_TASKS):\n",
    "            worker_task = asyncio.create_task(\n",
    "                worker(i, task_queue, results, pbar, stop_event)\n",
    "            )\n",
    "            workers.append(worker_task)\n",
    "        \n",
    "        try:\n",
    "            # Wait for queue to be empty OR stop event is set\n",
    "            while not task_queue.empty() and not stop_event.is_set():\n",
    "                # Check queue size periodically\n",
    "                await asyncio.sleep(1)\n",
    "                \n",
    "                # Check if models are still available\n",
    "                if not models_available:\n",
    "                    print(\"No models available, stopping workers...\", flush=True)\n",
    "                    stop_event.set()\n",
    "                    break\n",
    "            \n",
    "            # Wait for queue to empty or stop event\n",
    "            if not stop_event.is_set():\n",
    "                await task_queue.join()\n",
    "            else:\n",
    "                print(\"Stop event triggered, waiting for workers to finish current tasks...\", flush=True)\n",
    "                # Give workers time to finish current tasks\n",
    "                await asyncio.sleep(2)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in main loop: {e}\", flush=True)\n",
    "            stop_event.set()\n",
    "        \n",
    "        finally:\n",
    "            # Cancel all worker tasks\n",
    "            for w in workers:\n",
    "                w.cancel()\n",
    "            \n",
    "            # Wait for workers to be cancelled\n",
    "            if workers:\n",
    "                await asyncio.gather(*workers, return_exceptions=True)\n",
    "        \n",
    "        # Final check for remaining tasks in queue\n",
    "        remaining_tasks = task_queue.qsize()\n",
    "        if remaining_tasks > 0:\n",
    "            print(f\"Warning: {remaining_tasks} tasks were not processed\", flush=True)\n",
    "            # Update progress bar to reflect actual progress\n",
    "            pbar.total = N_TASKS - remaining_tasks\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "# Run the main function\n",
    "results = await main(ds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de0b368e-8e26-42b1-9de1-eb5d9fa4a32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6943, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sql_prompt</th>\n",
       "      <th>sql_context</th>\n",
       "      <th>sql</th>\n",
       "      <th>model_used</th>\n",
       "      <th>completion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the average moisture level for each cr...</td>\n",
       "      <td>CREATE TABLE crop_moisture (id INT, crop_id IN...</td>\n",
       "      <td>SELECT type, AVG(moisture) as avg_moisture FRO...</td>\n",
       "      <td>meta-llama/llama-4-maverick-17b-128e-instruct</td>\n",
       "      <td>SELECT type, moisture as avg_moisture FROM cro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Add a new job title called 'Data Science Manag...</td>\n",
       "      <td>CREATE TABLE JobTitle (JobTitleID INT PRIMARY ...</td>\n",
       "      <td>INSERT INTO JobTitle (JobTitleID, JobTitleName...</td>\n",
       "      <td>meta-llama/llama-4-maverick-17b-128e-instruct</td>\n",
       "      <td>INSERT INTO JobTitel (JobTitleID, JobTitleName...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the total number of military equipment...</td>\n",
       "      <td>CREATE TABLE MaintenanceRequests (RequestID IN...</td>\n",
       "      <td>SELECT COUNT(*) FROM MaintenanceRequests WHERE...</td>\n",
       "      <td>meta-llama/llama-4-scout-17b-16e-instruct</td>\n",
       "      <td>SELECT COUNT(*) FROM MaintenanceRequests WHERE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Insert a new record into the 'community_educat...</td>\n",
       "      <td>CREATE TABLE community_education (id INT, prog...</td>\n",
       "      <td>INSERT INTO community_education (id, program, ...</td>\n",
       "      <td>moonshotai/kimi-k2-instruct</td>\n",
       "      <td>\"INSERT INTO community_education (id, program,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How many users signed up daily in the 'games' ...</td>\n",
       "      <td>CREATE TABLE signups (user_id INT, category TE...</td>\n",
       "      <td>SELECT DATE(timestamp) as signup_date, COUNT(D...</td>\n",
       "      <td>moonshotai/kimi-k2-instruct</td>\n",
       "      <td>SELECT DATE(timestamp) as signup_date, COUNT(u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6938</th>\n",
       "      <td>What are the top 5 most sold garments by sales...</td>\n",
       "      <td>CREATE TABLE garments (id INT, name VARCHAR(10...</td>\n",
       "      <td>SELECT garments.name, garments_sales.total_sol...</td>\n",
       "      <td>llama-3.1-8b-instant</td>\n",
       "      <td>SELECT garments.name, garments_sales.total_sol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6939</th>\n",
       "      <td>List all the bus stops in the city of Santiago...</td>\n",
       "      <td>bus_stops (id, name, city, country, issues)</td>\n",
       "      <td>SELECT bus_stops.* FROM bus_stops WHERE bus_st...</td>\n",
       "      <td>llama-3.1-8b-instant</td>\n",
       "      <td>SELECT bus_stops.* FROM bus_stops WHERE bus_st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6940</th>\n",
       "      <td>How many players from each continent play Non-...</td>\n",
       "      <td>CREATE TABLE countries (id INT, name VARCHAR(2...</td>\n",
       "      <td>SELECT c.continent, COUNT(DISTINCT p.id) as nu...</td>\n",
       "      <td>llama-3.1-8b-instant</td>\n",
       "      <td>INSERT INTO countries ('3', 'Canada', 'North A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6941</th>\n",
       "      <td>What is the total mass of space debris in the ...</td>\n",
       "      <td>CREATE TABLE space_debris (debris_id INT, name...</td>\n",
       "      <td>SELECT SUM(mass) FROM space_debris WHERE origi...</td>\n",
       "      <td>llama-3.1-8b-instant</td>\n",
       "      <td>SELECT SUM(mass) FROM space_debris WHERE origi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6942</th>\n",
       "      <td>What is the average mental health score for st...</td>\n",
       "      <td>CREATE TABLE mental_health_scores (score_id IN...</td>\n",
       "      <td>SELECT s.grade_level, sd.district_name, AVG(mh...</td>\n",
       "      <td>llama-3.1-8b-instant</td>\n",
       "      <td>SELECT s.grade_level, sd.district_name, AVG(mh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6943 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sql_prompt  \\\n",
       "0     What is the average moisture level for each cr...   \n",
       "1     Add a new job title called 'Data Science Manag...   \n",
       "2     What is the total number of military equipment...   \n",
       "3     Insert a new record into the 'community_educat...   \n",
       "4     How many users signed up daily in the 'games' ...   \n",
       "...                                                 ...   \n",
       "6938  What are the top 5 most sold garments by sales...   \n",
       "6939  List all the bus stops in the city of Santiago...   \n",
       "6940  How many players from each continent play Non-...   \n",
       "6941  What is the total mass of space debris in the ...   \n",
       "6942  What is the average mental health score for st...   \n",
       "\n",
       "                                            sql_context  \\\n",
       "0     CREATE TABLE crop_moisture (id INT, crop_id IN...   \n",
       "1     CREATE TABLE JobTitle (JobTitleID INT PRIMARY ...   \n",
       "2     CREATE TABLE MaintenanceRequests (RequestID IN...   \n",
       "3     CREATE TABLE community_education (id INT, prog...   \n",
       "4     CREATE TABLE signups (user_id INT, category TE...   \n",
       "...                                                 ...   \n",
       "6938  CREATE TABLE garments (id INT, name VARCHAR(10...   \n",
       "6939        bus_stops (id, name, city, country, issues)   \n",
       "6940  CREATE TABLE countries (id INT, name VARCHAR(2...   \n",
       "6941  CREATE TABLE space_debris (debris_id INT, name...   \n",
       "6942  CREATE TABLE mental_health_scores (score_id IN...   \n",
       "\n",
       "                                                    sql  \\\n",
       "0     SELECT type, AVG(moisture) as avg_moisture FRO...   \n",
       "1     INSERT INTO JobTitle (JobTitleID, JobTitleName...   \n",
       "2     SELECT COUNT(*) FROM MaintenanceRequests WHERE...   \n",
       "3     INSERT INTO community_education (id, program, ...   \n",
       "4     SELECT DATE(timestamp) as signup_date, COUNT(D...   \n",
       "...                                                 ...   \n",
       "6938  SELECT garments.name, garments_sales.total_sol...   \n",
       "6939  SELECT bus_stops.* FROM bus_stops WHERE bus_st...   \n",
       "6940  SELECT c.continent, COUNT(DISTINCT p.id) as nu...   \n",
       "6941  SELECT SUM(mass) FROM space_debris WHERE origi...   \n",
       "6942  SELECT s.grade_level, sd.district_name, AVG(mh...   \n",
       "\n",
       "                                         model_used  \\\n",
       "0     meta-llama/llama-4-maverick-17b-128e-instruct   \n",
       "1     meta-llama/llama-4-maverick-17b-128e-instruct   \n",
       "2         meta-llama/llama-4-scout-17b-16e-instruct   \n",
       "3                       moonshotai/kimi-k2-instruct   \n",
       "4                       moonshotai/kimi-k2-instruct   \n",
       "...                                             ...   \n",
       "6938                           llama-3.1-8b-instant   \n",
       "6939                           llama-3.1-8b-instant   \n",
       "6940                           llama-3.1-8b-instant   \n",
       "6941                           llama-3.1-8b-instant   \n",
       "6942                           llama-3.1-8b-instant   \n",
       "\n",
       "                                             completion  \n",
       "0     SELECT type, moisture as avg_moisture FROM cro...  \n",
       "1     INSERT INTO JobTitel (JobTitleID, JobTitleName...  \n",
       "2     SELECT COUNT(*) FROM MaintenanceRequests WHERE...  \n",
       "3     \"INSERT INTO community_education (id, program,...  \n",
       "4     SELECT DATE(timestamp) as signup_date, COUNT(u...  \n",
       "...                                                 ...  \n",
       "6938  SELECT garments.name, garments_sales.total_sol...  \n",
       "6939  SELECT bus_stops.* FROM bus_stops WHERE bus_st...  \n",
       "6940  INSERT INTO countries ('3', 'Canada', 'North A...  \n",
       "6941  SELECT SUM(mass) FROM space_debris WHERE origi...  \n",
       "6942  SELECT s.grade_level, sd.district_name, AVG(mh...  \n",
       "\n",
       "[6943 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'sql_prompt': [result['sql_prompt'] for result in results],\n",
    "    'sql_context': [result['sql_context'] for result in results],\n",
    "    'sql': [result['sql'] for result in results],\n",
    "    'model_used': [result['model_used'] for result in results],\n",
    "    'completion': [result['completion'] for result in results]\n",
    "})\n",
    "print(results_df.shape)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "874e1249-dce2-4418-b7c6-e7a4572e6b87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_used\n",
       "llama-3.1-8b-instant                             1700\n",
       "moonshotai/kimi-k2-instruct                      1000\n",
       "meta-llama/llama-4-scout-17b-16e-instruct        1000\n",
       "meta-llama/llama-4-maverick-17b-128e-instruct     999\n",
       "moonshotai/kimi-k2-instruct-0905                  999\n",
       "openai/gpt-oss-120b                               347\n",
       "qwen/qwen3-32b                                    337\n",
       "llama-3.3-70b-versatile                           312\n",
       "openai/gpt-oss-20b                                249\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.value_counts('model_used')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b739bef-b9f5-444b-ac74-29b015cce7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df.to_csv('rm_data.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3652ab-33b9-4580-9998-f58d8db446b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python T4",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
