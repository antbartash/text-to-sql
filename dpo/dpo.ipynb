{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfe20728-8bcc-47fc-8148-a2144c58e259",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49d56f23-d923-4899-85c1-b91adf512c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import datasets\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from trl import DPOConfig, DPOTrainer\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import evaluate\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import sqlite3\n",
    "import sqlparse\n",
    "import _config\n",
    "\n",
    "import os\n",
    "import psutil\n",
    "import GPUtil\n",
    "import gc\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"WANDB_API_KEY\"] = _config.WANDB_API_KEY\n",
    "os.environ[\"WANDB_PROJECT\"] = _config.WANDB_PROJECT\n",
    "\n",
    "ENABLE_THINKING = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de55c654-917a-406c-bc81-201d4397e62f",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "525fe697-fbe6-4939-a379-b320f0bf0ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "CPU 0 load: 0.00\n",
      "CPU 1 load: 3.00\n",
      "CPU 2 load: 2.00\n",
      "CPU 3 load: 0.00\n",
      "RAM Total: 27.40 GB, Used: 1.74 GB\n",
      "GPU 0 (Tesla T4) load: 0.0%\n",
      "GPU 0 (Tesla T4) VRAM Total: 16384.0 MB, Used 3.0 MB\n",
      "Disk Total: 60.95 GB, Used: 57.04 GB\n"
     ]
    }
   ],
   "source": [
    "def get_vm_usage_metrics():\n",
    "    # CPU usage\n",
    "    cpu_load = psutil.cpu_percent(interval=1, percpu=True)\n",
    "    for id, load in enumerate(cpu_load):\n",
    "        print(f\"CPU {id} load: {load:.2f}\")\n",
    "    # RAM usage\n",
    "    ram = psutil.virtual_memory()\n",
    "    print(f\"RAM Total: {ram.total/(1024**3):.2f} GB, Used: {(ram.used)/(1024**3):.2f} GB\")\n",
    "    # GPU\n",
    "    if torch.cuda.is_available():\n",
    "        gpus = GPUtil.getGPUs()\n",
    "        for gpu in gpus:\n",
    "            print(f\"GPU {gpu.id} ({gpu.name}) load: {gpu.load*100}%\")\n",
    "            print(f\"GPU {gpu.id} ({gpu.name}) VRAM Total: {gpu.memoryTotal} MB, Used {gpu.memoryUsed} MB\")\n",
    "    # Disk \n",
    "    disk = psutil.disk_usage('/')\n",
    "    print(f\"Disk Total: {disk.total/(1024**3):.2f} GB, Used: {(disk.used)/(1024**3):.2f} GB\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f'Device: {device}')\n",
    "get_vm_usage_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f2fc14-5482-4a8c-b9cb-41410dbc89ec",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95c19a40-edd8-4972-9a7e-b3ee05c35df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6943, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sql_prompt</th>\n",
       "      <th>sql_context</th>\n",
       "      <th>sql</th>\n",
       "      <th>model_used</th>\n",
       "      <th>completion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the average moisture level for each cr...</td>\n",
       "      <td>CREATE TABLE crop_moisture (id INT, crop_id IN...</td>\n",
       "      <td>SELECT type, AVG(moisture) as avg_moisture FRO...</td>\n",
       "      <td>meta-llama/llama-4-maverick-17b-128e-instruct</td>\n",
       "      <td>SELECT type, moisture as avg_moisture FROM cro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Add a new job title called 'Data Science Manag...</td>\n",
       "      <td>CREATE TABLE JobTitle (JobTitleID INT PRIMARY ...</td>\n",
       "      <td>INSERT INTO JobTitle (JobTitleID, JobTitleName...</td>\n",
       "      <td>meta-llama/llama-4-maverick-17b-128e-instruct</td>\n",
       "      <td>INSERT INTO JobTitel (JobTitleID, JobTitleName...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the total number of military equipment...</td>\n",
       "      <td>CREATE TABLE MaintenanceRequests (RequestID IN...</td>\n",
       "      <td>SELECT COUNT(*) FROM MaintenanceRequests WHERE...</td>\n",
       "      <td>meta-llama/llama-4-scout-17b-16e-instruct</td>\n",
       "      <td>SELECT COUNT(*) FROM MaintenanceRequests WHERE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Insert a new record into the 'community_educat...</td>\n",
       "      <td>CREATE TABLE community_education (id INT, prog...</td>\n",
       "      <td>INSERT INTO community_education (id, program, ...</td>\n",
       "      <td>moonshotai/kimi-k2-instruct</td>\n",
       "      <td>\"INSERT INTO community_education (id, program,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How many users signed up daily in the 'games' ...</td>\n",
       "      <td>CREATE TABLE signups (user_id INT, category TE...</td>\n",
       "      <td>SELECT DATE(timestamp) as signup_date, COUNT(D...</td>\n",
       "      <td>moonshotai/kimi-k2-instruct</td>\n",
       "      <td>SELECT DATE(timestamp) as signup_date, COUNT(u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          sql_prompt  \\\n",
       "0  What is the average moisture level for each cr...   \n",
       "1  Add a new job title called 'Data Science Manag...   \n",
       "2  What is the total number of military equipment...   \n",
       "3  Insert a new record into the 'community_educat...   \n",
       "4  How many users signed up daily in the 'games' ...   \n",
       "\n",
       "                                         sql_context  \\\n",
       "0  CREATE TABLE crop_moisture (id INT, crop_id IN...   \n",
       "1  CREATE TABLE JobTitle (JobTitleID INT PRIMARY ...   \n",
       "2  CREATE TABLE MaintenanceRequests (RequestID IN...   \n",
       "3  CREATE TABLE community_education (id INT, prog...   \n",
       "4  CREATE TABLE signups (user_id INT, category TE...   \n",
       "\n",
       "                                                 sql  \\\n",
       "0  SELECT type, AVG(moisture) as avg_moisture FRO...   \n",
       "1  INSERT INTO JobTitle (JobTitleID, JobTitleName...   \n",
       "2  SELECT COUNT(*) FROM MaintenanceRequests WHERE...   \n",
       "3  INSERT INTO community_education (id, program, ...   \n",
       "4  SELECT DATE(timestamp) as signup_date, COUNT(D...   \n",
       "\n",
       "                                      model_used  \\\n",
       "0  meta-llama/llama-4-maverick-17b-128e-instruct   \n",
       "1  meta-llama/llama-4-maverick-17b-128e-instruct   \n",
       "2      meta-llama/llama-4-scout-17b-16e-instruct   \n",
       "3                    moonshotai/kimi-k2-instruct   \n",
       "4                    moonshotai/kimi-k2-instruct   \n",
       "\n",
       "                                          completion  \n",
       "0  SELECT type, moisture as avg_moisture FROM cro...  \n",
       "1  INSERT INTO JobTitel (JobTitleID, JobTitleName...  \n",
       "2  SELECT COUNT(*) FROM MaintenanceRequests WHERE...  \n",
       "3  \"INSERT INTO community_education (id, program,...  \n",
       "4  SELECT DATE(timestamp) as signup_date, COUNT(u...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('preference_data.xlsx')\n",
    "\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f28e13cb-941e-4b68-a894-37d6db255666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'chosen', 'rejected'],\n",
       "    num_rows: 6248\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = []\n",
    "for id in range(data.shape[0]):\n",
    "    dataset.append({\n",
    "        'prompt': [{'role': 'user', 'content': data.loc[id, 'sql_prompt']}],\n",
    "        'chosen': [{'role': 'assistant', 'content': data.loc[id, 'sql']}],\n",
    "        'rejected': [{'role': 'assistant', 'content': data.loc[id, 'completion']}]\n",
    "    })\n",
    "dataset = datasets.Dataset.from_list(dataset)\n",
    "\n",
    "split = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "ds_train = split['train']\n",
    "ds_valid = split['test']\n",
    "\n",
    "ds_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd8f046-2cc9-4c40-b9a5-2ee9d98e19c1",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "859dac18-d643-4903-ba52-5848afc209b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU 0 load: 0.00\n",
      "CPU 1 load: 2.00\n",
      "CPU 2 load: 0.00\n",
      "CPU 3 load: 1.00\n",
      "RAM Total: 27.40 GB, Used: 2.39 GB\n",
      "GPU 0 (Tesla T4) load: 0.0%\n",
      "GPU 0 (Tesla T4) VRAM Total: 16384.0 MB, Used 1713.0 MB\n",
      "Disk Total: 60.95 GB, Used: 55.15 GB\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"Qwen/Qwen3-0.6B\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    checkpoint,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "get_vm_usage_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21da278c-ac63-4dc4-903a-f65a3e1de9dc",
   "metadata": {},
   "source": [
    "# DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d81214b5-35fd-4109-b4b7-4a36c258b177",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33molialeshka\u001b[0m (\u001b[33molialeshka-none\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/olialeshka_1/wandb/run-20260112_195930-hlnn8k0m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/olialeshka-none/text-to-sql/runs/hlnn8k0m' target=\"_blank\">dpo-qlora-lr1e5-epochs1-2026-01-12_19-59-28</a></strong> to <a href='https://wandb.ai/olialeshka-none/text-to-sql' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/olialeshka-none/text-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/olialeshka-none/text-to-sql/runs/hlnn8k0m' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql/runs/hlnn8k0m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6347aaf312a49009edc44ff06986bda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting prompt in train dataset:   0%|          | 0/6248 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82fc8a9a2e6e49b392182ae527ce7884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/6248 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ea189b705b34ff8b8d13230ec665244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/6248 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7df745d3c934f108a4c59fb1e30a1c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting prompt in eval dataset:   0%|          | 0/695 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c0351e73aa2473489efc40d5ba3861d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset:   0%|          | 0/695 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "499282c6e60a4a4894b409b9ad7b1ab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/695 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Training Setup Summary =====\n",
      "Num epochs:            1\n",
      "Effective batch size:  16\n",
      "Per-device batch size: 2\n",
      "Gradient accumulation: 8\n",
      "Dataset size:          6248\n",
      "Steps per epoch:       390\n",
      "Total training steps:  390\n",
      "Warmup steps:          39\n",
      "Logging steps:         40\n",
      "===================================\n",
      "Start time: 2026-01-12_19-59-41\n",
      "Starting fresh training run\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='391' max='391' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [391/391 2:44:23, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rewards/chosen</th>\n",
       "      <th>Rewards/rejected</th>\n",
       "      <th>Rewards/accuracies</th>\n",
       "      <th>Rewards/margins</th>\n",
       "      <th>Logps/chosen</th>\n",
       "      <th>Logps/rejected</th>\n",
       "      <th>Logits/chosen</th>\n",
       "      <th>Logits/rejected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.453700</td>\n",
       "      <td>0.279676</td>\n",
       "      <td>0.781279</td>\n",
       "      <td>-3.567111</td>\n",
       "      <td>0.908046</td>\n",
       "      <td>4.348390</td>\n",
       "      <td>-112.456573</td>\n",
       "      <td>-246.840836</td>\n",
       "      <td>-2.600768</td>\n",
       "      <td>-2.717176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.236300</td>\n",
       "      <td>0.214792</td>\n",
       "      <td>-0.276054</td>\n",
       "      <td>-6.058134</td>\n",
       "      <td>0.918103</td>\n",
       "      <td>5.782079</td>\n",
       "      <td>-123.029922</td>\n",
       "      <td>-271.751068</td>\n",
       "      <td>-2.971108</td>\n",
       "      <td>-3.106626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.169568</td>\n",
       "      <td>-0.260063</td>\n",
       "      <td>-6.494625</td>\n",
       "      <td>0.935345</td>\n",
       "      <td>6.234561</td>\n",
       "      <td>-122.869995</td>\n",
       "      <td>-276.115967</td>\n",
       "      <td>-3.006020</td>\n",
       "      <td>-3.170763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.166100</td>\n",
       "      <td>0.148824</td>\n",
       "      <td>-4.003447</td>\n",
       "      <td>-11.723321</td>\n",
       "      <td>0.949713</td>\n",
       "      <td>7.719874</td>\n",
       "      <td>-160.303848</td>\n",
       "      <td>-328.402924</td>\n",
       "      <td>-3.662523</td>\n",
       "      <td>-3.795769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.127400</td>\n",
       "      <td>0.148165</td>\n",
       "      <td>-3.111150</td>\n",
       "      <td>-11.753782</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>8.642632</td>\n",
       "      <td>-151.380859</td>\n",
       "      <td>-328.707581</td>\n",
       "      <td>-3.697229</td>\n",
       "      <td>-3.890591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.132900</td>\n",
       "      <td>0.135818</td>\n",
       "      <td>-3.057075</td>\n",
       "      <td>-12.640137</td>\n",
       "      <td>0.946839</td>\n",
       "      <td>9.583062</td>\n",
       "      <td>-150.840118</td>\n",
       "      <td>-337.571106</td>\n",
       "      <td>-3.713598</td>\n",
       "      <td>-3.964784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.172800</td>\n",
       "      <td>0.124555</td>\n",
       "      <td>-5.424104</td>\n",
       "      <td>-14.227682</td>\n",
       "      <td>0.954023</td>\n",
       "      <td>8.803579</td>\n",
       "      <td>-174.510422</td>\n",
       "      <td>-353.446503</td>\n",
       "      <td>-3.981601</td>\n",
       "      <td>-4.156662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.115600</td>\n",
       "      <td>0.123575</td>\n",
       "      <td>-3.426789</td>\n",
       "      <td>-13.605733</td>\n",
       "      <td>0.956897</td>\n",
       "      <td>10.178943</td>\n",
       "      <td>-154.537262</td>\n",
       "      <td>-347.227051</td>\n",
       "      <td>-3.835569</td>\n",
       "      <td>-4.110652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.112400</td>\n",
       "      <td>0.116178</td>\n",
       "      <td>-3.996172</td>\n",
       "      <td>-13.999291</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>10.003119</td>\n",
       "      <td>-160.231079</td>\n",
       "      <td>-351.162628</td>\n",
       "      <td>-3.926410</td>\n",
       "      <td>-4.173460</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End time: 2026-01-12_22-44-23\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "RUN_NAME = f'dpo-qlora-lr1e5-epochs1-{timestamp}'\n",
    "OUTPUT_DIR = './dpo-output'\n",
    "RESUME_TRAINING = False\n",
    "\n",
    "PER_DEVICE_BATCH_SIZE = 2\n",
    "effective_batch_size = 16\n",
    "epochs=1\n",
    "learning_rate = 1e-5\n",
    "warmup_ratio = 0.1\n",
    "lora_r = 16*4\n",
    "lora_alpha = 64*4\n",
    "lora_dropout = 0.01\n",
    "\n",
    "gradient_accumulation_steps = int(effective_batch_size / PER_DEVICE_BATCH_SIZE)\n",
    "\n",
    "wandb.init(\n",
    "    project=os.environ[\"WANDB_PROJECT\"],\n",
    "    name=RUN_NAME,\n",
    "    # id=run_id ,         # resume previous run if available\n",
    "    # resume=\"allow\",    # allows resuming crashed run\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "training_args = DPOConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    \n",
    "    num_train_epochs=epochs,\n",
    "    beta=0.1,\n",
    "    \n",
    "    per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=gradient_accumulation_steps*5,\n",
    "    save_total_limit=2,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=gradient_accumulation_steps*5,\n",
    "    per_device_eval_batch_size=PER_DEVICE_BATCH_SIZE*2,\n",
    "    eval_accumulation_steps=4,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=gradient_accumulation_steps*5,\n",
    "    report_to=['wandb'],\n",
    "    run_name=RUN_NAME,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    max_grad_norm=1,\n",
    "    load_best_model_at_end=True,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    # generate_during_eval=True\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules='all-linear'\n",
    ")\n",
    "# model.requires_grad_(False)                     # freeze base weights (precautionary)\n",
    "model_peft = get_peft_model(model, peft_config) # inject a LoRA adapter\n",
    "\n",
    "trainer = DPOTrainer(\n",
    "    processing_class=tokenizer,\n",
    "    model=model_peft,\n",
    "    args=training_args,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_valid,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "\n",
    "# Training setup summary\n",
    "dataset_size = len(ds_train)\n",
    "steps_per_epoch = dataset_size // (PER_DEVICE_BATCH_SIZE * gradient_accumulation_steps)\n",
    "total_steps = steps_per_epoch * epochs\n",
    "warmup_steps = int(total_steps * warmup_ratio)\n",
    "\n",
    "print(\"===== Training Setup Summary =====\")\n",
    "print(f\"Num epochs:            {epochs}\")\n",
    "print(f\"Effective batch size:  {effective_batch_size}\")\n",
    "print(f\"Per-device batch size: {PER_DEVICE_BATCH_SIZE}\")\n",
    "print(f\"Gradient accumulation: {gradient_accumulation_steps}\")\n",
    "print(f\"Dataset size:          {dataset_size}\")\n",
    "print(f\"Steps per epoch:       {steps_per_epoch}\")\n",
    "print(f\"Total training steps:  {total_steps}\")\n",
    "print(f\"Warmup steps:          {warmup_steps}\")\n",
    "print(f\"Logging steps:         {training_args.logging_steps}\")\n",
    "print(\"===================================\")\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\")\n",
    "\n",
    "\n",
    "# Training\n",
    "last_checkpoint = None\n",
    "if RESUME_TRAINING and os.path.isdir(OUTPUT_DIR):\n",
    "    last_checkpoint = get_last_checkpoint(OUTPUT_DIR)\n",
    "\n",
    "if last_checkpoint is not None:\n",
    "    print(f\"Resuming training from checkpoint: {last_checkpoint}\")\n",
    "    trainer.train(resume_from_checkpoint=last_checkpoint)\n",
    "else:\n",
    "    print(\"Starting fresh training run\")\n",
    "    trainer.train()\n",
    "\n",
    "print(f\"End time: {datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8978293a-2f92-4370-b6f1-e4bd7bb959dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(f\"{OUTPUT_DIR}/best_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7495893-c8fb-4d81-a16a-09346fe6aa9b",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc97fe8b-51ec-4679-b391-0fe44ae89737",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = './dpo-output'\n",
    "checkpoint = f\"{OUTPUT_DIR}/checkpoint-391/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, dtype=torch.float16).to(device)\n",
    "model.eval()\n",
    "\n",
    "ds = datasets.load_dataset('gretelai/synthetic_text_to_sql', streaming=False)\n",
    "ds_train, ds_test = ds['train'], ds['test']\n",
    "\n",
    "\n",
    "def construct_message(prompt, context):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": f\"The user asks a question. Your task is to generate the SQL query to answer that question. Return SQL query only. The context of the question is the following: '{context}'\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "def generate_model_response_batch(messages_list, enable_thinking=True, max_new_tokens=512):\n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=enable_thinking\n",
    "        )\n",
    "        for messages in messages_list\n",
    "    ]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        padding_side='left'\n",
    "    ).to(model.device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=max_new_tokens\n",
    "    )\n",
    "\n",
    "    responses = []\n",
    "    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids):\n",
    "        # Slice to get only generated part\n",
    "        output_only_ids = output_ids[len(input_ids):].tolist()\n",
    "\n",
    "        # Try to find `</think>` (id 151668)\n",
    "        try:\n",
    "            index = len(output_only_ids) - output_only_ids[::-1].index(151668)\n",
    "        except ValueError:\n",
    "            index = 0\n",
    "\n",
    "        if enable_thinking:\n",
    "            thinking_content = tokenizer.decode(\n",
    "                output_only_ids[:index],\n",
    "                skip_special_tokens=True\n",
    "            ).strip(\"\\n\")\n",
    "            content = tokenizer.decode(\n",
    "                output_only_ids[index:],\n",
    "                skip_special_tokens=True\n",
    "            ).strip(\"\\n\")\n",
    "        else:\n",
    "            thinking_content = None\n",
    "            content = tokenizer.decode(\n",
    "                output_only_ids,\n",
    "                skip_special_tokens=True\n",
    "            ).strip(\"\\n\")\n",
    "\n",
    "        responses.append({\n",
    "            'thinking_content': thinking_content,\n",
    "            'content': content\n",
    "        })\n",
    "\n",
    "    return responses\n",
    "\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def normalize_sql(sql):\n",
    "    return sqlparse.format(sql, reindent=True, keyword_case='upper').strip()\n",
    "\n",
    "def compute_rouge(reference, prediction):\n",
    "    result = rouge.compute(predictions=[prediction], references=[reference])\n",
    "    return result['rougeL']\n",
    "\n",
    "def evaluate_sql_response(reference, prediction, sql_context):\n",
    "    # ROUGE-L\n",
    "    rouge_score = compute_rouge(reference, prediction)\n",
    "    \n",
    "    # execution check\n",
    "    try:\n",
    "        conn = sqlite3.connect(\":memory:\")\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.executescript(sql_context)\n",
    "        cursor.execute(reference)\n",
    "        ref_result = cursor.fetchall()\n",
    "        \n",
    "        cursor.execute(prediction)\n",
    "        model_result = cursor.fetchall()\n",
    "        \n",
    "        execution_match = ref_result == model_result\n",
    "    except Exception:\n",
    "        execution_match = False\n",
    "    finally:\n",
    "        conn.close()\n",
    "    \n",
    "    # final score\n",
    "    if execution_match:\n",
    "        final_score = 1.0\n",
    "    else:\n",
    "        final_score = 0.7 * rouge_score\n",
    "\n",
    "    return {\n",
    "        \"rougeL\": round(rouge_score, 4),\n",
    "        \"execution_match\": execution_match,\n",
    "        \"final_score\": final_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "081b17b1-4e0b-4df0-8377-21bb25aa93b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: Tue Jan 13 07:50:06 2026\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3217ab9cf0cf4f44a25a2989ba441c44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End time: Tue Jan 13 09:37:58 2026\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "ENABLE_THINKING = False\n",
    "MAX_NEW_TOKENS = 512\n",
    "\n",
    "\n",
    "prompts = [ds_test[id]['sql_prompt'] for id in range(len(ds_test))]\n",
    "contexts = [ds_test[id]['sql_context'] for id in range(len(ds_test))]\n",
    "\n",
    "responses = []\n",
    "print(f\"Start time: {time.ctime(time.time())}\")\n",
    "for i in tqdm(range(0, len(prompts), BATCH_SIZE)):\n",
    "    batch_prompts = prompts[i : i + BATCH_SIZE]\n",
    "    batch_contexts = contexts[i : i + BATCH_SIZE]\n",
    "\n",
    "    messages_list = [\n",
    "        construct_message(prompt=p, context=c)\n",
    "        for p, c in zip(batch_prompts, batch_contexts)\n",
    "    ]\n",
    "\n",
    "    batch_responses = generate_model_response_batch(messages_list, enable_thinking=ENABLE_THINKING, max_new_tokens=MAX_NEW_TOKENS)\n",
    "\n",
    "    responses.extend(batch_responses)\n",
    "\n",
    "print(f\"End time: {time.ctime(time.time())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "941358bb-1995-4b19-abf1-3b8b48201366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d10625daed74bf4a769e6ec8c9cde4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5851 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "references = [ds_test[id]['sql'] for id in range(len(ds_test))]\n",
    "predictions = [responses[id]['content'] for id in range(len(ds_test))]\n",
    "\n",
    "scores = [\n",
    "    evaluate_sql_response(\n",
    "        reference=reference,\n",
    "        prediction=prediction,\n",
    "        sql_context=context\n",
    "    )\n",
    "    for reference, prediction, context in tqdm(zip(references, predictions, contexts), total=len(ds_test))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9495348-a4b1-42fd-a86e-94b49095729b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean test set score: 0.612\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean test set score: {np.mean([score['final_score'] for score in scores]):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bc5fb4-0fcd-422a-b6af-4e950febf344",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python T4",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
