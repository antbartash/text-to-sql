{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e84451c-1ae2-4446-a896-1ab971c9612e",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81de34e8-a5c3-4638-af77-78c396cea599",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import datasets\n",
    "import evaluate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, EarlyStoppingCallback\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from trl import SFTTrainer\n",
    "from peft import PromptTuningConfig, PromptTuningInit, get_peft_model\n",
    "from accelerate import cpu_offload\n",
    "import sqlite3\n",
    "import sqlparse\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import wandb\n",
    "import psutil\n",
    "import GPUtil\n",
    "import os\n",
    "import gc\n",
    "import math\n",
    "\n",
    "import _config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fdad24c-9f36-4778-8ff8-54608e5883fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENABLE_THINKING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "182dcb56-ff15-4e23-8b42-e75aeac4cf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = _config.WANDB_API_KEY\n",
    "os.environ[\"WANDB_PROJECT\"] = _config.WANDB_PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5339c1d6-98a8-49c1-a69c-0b83820dc9be",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e2b2599-ab5a-4542-9e96-e4315eddc80b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "CPU 0 load: 0.00\n",
      "CPU 1 load: 0.00\n",
      "CPU 2 load: 1.00\n",
      "CPU 3 load: 0.00\n",
      "RAM Total: 27.41 GB, Used: 1.62 GB\n",
      "GPU 0 (Tesla T4) load: 0.0%\n",
      "GPU 0 (Tesla T4) VRAM Total: 16384.0 MB, Used 3.0 MB\n",
      "Disk Total: 60.95 GB, Used: 35.30 GB\n"
     ]
    }
   ],
   "source": [
    "def get_vm_usage_metrics():\n",
    "    # CPU usage\n",
    "    cpu_load = psutil.cpu_percent(interval=1, percpu=True)\n",
    "    for id, load in enumerate(cpu_load):\n",
    "        print(f\"CPU {id} load: {load:.2f}\")\n",
    "    # RAM usage\n",
    "    ram = psutil.virtual_memory()\n",
    "    print(f\"RAM Total: {ram.total/(1024**3):.2f} GB, Used: {(ram.used)/(1024**3):.2f} GB\")\n",
    "    # GPU\n",
    "    if torch.cuda.is_available():\n",
    "        gpus = GPUtil.getGPUs()\n",
    "        for gpu in gpus:\n",
    "            print(f\"GPU {gpu.id} ({gpu.name}) load: {gpu.load*100}%\")\n",
    "            print(f\"GPU {gpu.id} ({gpu.name}) VRAM Total: {gpu.memoryTotal} MB, Used {gpu.memoryUsed} MB\")\n",
    "    # Disk \n",
    "    disk = psutil.disk_usage('/')\n",
    "    print(f\"Disk Total: {disk.total/(1024**3):.2f} GB, Used: {(disk.used)/(1024**3):.2f} GB\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f'Device: {device}')\n",
    "get_vm_usage_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41c61735-9bfc-48c5-9b8c-c9b7941dcf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"Trainable params: {trainable_params} || All params: {all_param} || Trainable %: {100 * trainable_params / all_param:.2f}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def return_num_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Returns the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    return trainable_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c64ffa-c729-43f4-b32f-6be96e4cfe0c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cce15154-ce9c-42ea-aad3-6cb9a75ce806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'domain', 'domain_description', 'sql_complexity', 'sql_complexity_description', 'sql_task_type', 'sql_task_type_description', 'sql_prompt', 'sql_context', 'sql', 'sql_explanation'],\n",
       "    num_rows: 97500\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = datasets.load_dataset('gretelai/synthetic_text_to_sql', streaming=False)\n",
    "ds_train, ds_test = ds['train'], ds['test']\n",
    "\n",
    "split = ds_train.train_test_split(test_size=0.025, seed=42)\n",
    "ds_train = split['train']\n",
    "ds_valid = split['test']\n",
    "\n",
    "ds_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc899f6-b89f-455d-8033-f3b39ebc1886",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e8c8ee0-69cf-4c00-a542-7e4479c03431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 596049920 || All params: 596049920 || Trainable %: 100.00\n",
      "CPU 0 load: 1.00\n",
      "CPU 1 load: 0.00\n",
      "CPU 2 load: 2.90\n",
      "CPU 3 load: 0.00\n",
      "RAM Total: 27.41 GB, Used: 1.93 GB\n",
      "GPU 0 (Tesla T4) load: 0.0%\n",
      "GPU 0 (Tesla T4) VRAM Total: 16384.0 MB, Used 2987.0 MB\n",
      "Disk Total: 60.95 GB, Used: 35.30 GB\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"Qwen/Qwen3-0.6B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    checkpoint,\n",
    "    device_map=\"cuda\",\n",
    ")\n",
    "# model = cpu_offload(model)\n",
    "\n",
    "print_trainable_parameters(model)\n",
    "get_vm_usage_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0106aedd-3c47-4705-b3b9-3379107df6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_message(prompt, context):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": f\"The user asks a question. Your task is to generate the SQL query to answer that question. Return SQL query only. The context of the question is the following: '{context}'\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "912ded5e-a7cb-4a4f-9fce-5254e8dee39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_response_batch(model, tokenizer, messages_list, enable_thinking=True, max_new_tokens=512):\n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=enable_thinking\n",
    "        )\n",
    "        for messages in messages_list\n",
    "    ]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        padding_side='left'\n",
    "    ).to(model.device)\n",
    "\n",
    "    model.eval()\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=max_new_tokens\n",
    "    )\n",
    "\n",
    "    responses = []\n",
    "    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids):\n",
    "        # Slice to get only generated part\n",
    "        output_only_ids = output_ids[len(input_ids):].tolist()\n",
    "\n",
    "        # Try to find `</think>` (id 151668)\n",
    "        try:\n",
    "            index = len(output_only_ids) - output_only_ids[::-1].index(151668)\n",
    "        except ValueError:\n",
    "            index = 0\n",
    "\n",
    "        if enable_thinking:\n",
    "            thinking_content = tokenizer.decode(\n",
    "                output_only_ids[:index],\n",
    "                skip_special_tokens=True\n",
    "            ).strip(\"\\n\")\n",
    "            content = tokenizer.decode(\n",
    "                output_only_ids[index:],\n",
    "                skip_special_tokens=True\n",
    "            ).strip(\"\\n\")\n",
    "        else:\n",
    "            thinking_content = None\n",
    "            content = tokenizer.decode(\n",
    "                output_only_ids,\n",
    "                skip_special_tokens=True\n",
    "            ).strip(\"\\n\")\n",
    "\n",
    "        responses.append({\n",
    "            'thinking_content': thinking_content,\n",
    "            'content': content\n",
    "        })\n",
    "\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b957410-6efa-4926-a3c9-e5fdc63513ea",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "932573b4-2ede-4724-a6a8-14428dd6125a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def normalize_sql(sql):\n",
    "    return sqlparse.format(sql, reindent=True, keyword_case='upper').strip()\n",
    "\n",
    "def compute_rouge(reference, prediction):\n",
    "    result = rouge.compute(predictions=[prediction], references=[reference])\n",
    "    return result['rougeL']\n",
    "\n",
    "def evaluate_sql_response(reference, prediction, sql_context):\n",
    "    # ROUGE-L\n",
    "    rouge_score = compute_rouge(reference, prediction)\n",
    "    \n",
    "    # execution check\n",
    "    try:\n",
    "        conn = sqlite3.connect(\":memory:\")\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.executescript(sql_context)\n",
    "        cursor.execute(reference)\n",
    "        ref_result = cursor.fetchall()\n",
    "        \n",
    "        cursor.execute(prediction)\n",
    "        model_result = cursor.fetchall()\n",
    "        \n",
    "        execution_match = ref_result == model_result\n",
    "    except Exception:\n",
    "        execution_match = False\n",
    "    finally:\n",
    "        conn.close()\n",
    "    \n",
    "    # final score\n",
    "    if execution_match:\n",
    "        final_score = 1.0\n",
    "    else:\n",
    "        final_score = 0.7 * rouge_score\n",
    "\n",
    "    return {\n",
    "        \"rougeL\": round(rouge_score, 4),\n",
    "        \"execution_match\": execution_match,\n",
    "        \"final_score\": final_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd975079-9037-4745-a8d4-d9d828ee2257",
   "metadata": {},
   "source": [
    "# Formatting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7aa8c985-2ebd-4f95-a5d3-a804abcce942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for training\n",
    "def construct_message_with_assistant_content(example):\n",
    "    messages = construct_message(example['sql_prompt'], example['sql_context'])\n",
    "    messages.append({\n",
    "        'role': 'assistant',\n",
    "        'content': example['sql']\n",
    "    })\n",
    "    return {'messages': messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21145d07-3501-44ff-acaa-ca323091539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(example, enable_thinking=ENABLE_THINKING):\n",
    "    return tokenizer.apply_chat_template(\n",
    "        example[\"messages\"],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False, # no generation prompt during training\n",
    "        enable_thinking=ENABLE_THINKING \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcdb61f-44c3-449a-a7a7-632136c58c87",
   "metadata": {},
   "source": [
    "# Prompt tuning - step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94d19298-6ded-4234-82fe-a651910680df",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 4096\n",
    "VALID_SIZE = 1024\n",
    "\n",
    "ds_train_sample = ds_train.take(TRAIN_SIZE)\n",
    "ds_valid_sample = ds_valid.take(VALID_SIZE)\n",
    "\n",
    "len(ds_train_sample), len(ds_valid_sample), len(ds_test)\n",
    "\n",
    "ds_train_with_assistant_content = ds_train_sample.map(construct_message_with_assistant_content)\n",
    "ds_valid_with_assistant_content = ds_valid_sample.map(construct_message_with_assistant_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc3045e2-5576-41ad-9278-67c8dc80a01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU 0 load: 2.00\n",
      "CPU 1 load: 0.00\n",
      "CPU 2 load: 2.00\n",
      "CPU 3 load: 4.00\n",
      "RAM Total: 27.41 GB, Used: 2.02 GB\n",
      "GPU 0 (Tesla T4) load: 0.0%\n",
      "GPU 0 (Tesla T4) VRAM Total: 16384.0 MB, Used 2987.0 MB\n",
      "Disk Total: 60.95 GB, Used: 35.22 GB\n"
     ]
    }
   ],
   "source": [
    "get_vm_usage_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7e80681-a80e-40e1-a891-2bb16149587e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474c1bf1-b8a5-4d13-a946-2dffc2c2cd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: lc4wbh65\n",
      "Sweep URL: https://wandb.ai/olialeshka-none/text-to-sql/sweeps/lc4wbh65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mxb2punu with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbetas: [0.9, 0.999]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teffective_batch_size: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_virtual_tokens: 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33molialeshka\u001b[0m (\u001b[33molialeshka-none\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/olialeshka_1/wandb/run-20251018_115820-mxb2punu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/olialeshka-none/text-to-sql/runs/mxb2punu' target=\"_blank\">silver-sweep-1</a></strong> to <a href='https://wandb.ai/olialeshka-none/text-to-sql' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/olialeshka-none/text-to-sql/sweeps/lc4wbh65' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql/sweeps/lc4wbh65</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/olialeshka-none/text-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/olialeshka-none/text-to-sql/sweeps/lc4wbh65' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql/sweeps/lc4wbh65</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/olialeshka-none/text-to-sql/runs/mxb2punu' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql/runs/mxb2punu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 51200 || All params: 596101120 || Trainable %: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 06:42, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8.111500</td>\n",
       "      <td>8.258953</td>\n",
       "      <td>3.174531</td>\n",
       "      <td>799399.000000</td>\n",
       "      <td>0.174135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>▁</td></tr><tr><td>eval/entropy</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/mean_token_accuracy</td><td>▁</td></tr><tr><td>eval/num_tokens</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval_loss</td><td>▁</td></tr><tr><td>eval_perplexity</td><td>▁</td></tr><tr><td>+13</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>512</td></tr><tr><td>eval/entropy</td><td>3.17453</td></tr><tr><td>eval/loss</td><td>8.25895</td></tr><tr><td>eval/mean_token_accuracy</td><td>0.17414</td></tr><tr><td>eval/num_tokens</td><td>799399</td></tr><tr><td>eval/runtime</td><td>51.7755</td></tr><tr><td>eval/samples_per_second</td><td>19.778</td></tr><tr><td>eval/steps_per_second</td><td>2.472</td></tr><tr><td>eval_loss</td><td>8.25895</td></tr><tr><td>eval_perplexity</td><td>3862.04878</td></tr><tr><td>+19</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">silver-sweep-1</strong> at: <a href='https://wandb.ai/olialeshka-none/text-to-sql/runs/mxb2punu' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql/runs/mxb2punu</a><br> View project at: <a href='https://wandb.ai/olialeshka-none/text-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251018_115820-mxb2punu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4yt3x7uo with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbetas: [0.9, 0.9999]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teffective_batch_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-07\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_virtual_tokens: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.1\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/olialeshka_1/wandb/run-20251018_120607-4yt3x7uo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/olialeshka-none/text-to-sql/runs/4yt3x7uo' target=\"_blank\">sandy-sweep-2</a></strong> to <a href='https://wandb.ai/olialeshka-none/text-to-sql' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/olialeshka-none/text-to-sql/sweeps/lc4wbh65' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql/sweeps/lc4wbh65</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/olialeshka-none/text-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/olialeshka-none/text-to-sql/sweeps/lc4wbh65' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql/sweeps/lc4wbh65</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/olialeshka-none/text-to-sql/runs/4yt3x7uo' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql/runs/4yt3x7uo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 20480 || All params: 596070400 || Trainable %: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14/16 04:32 < 00:45, 0.04 it/s, Epoch 0.81/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "sweep_config = {\n",
    "    'name': f'sweep-prompt-tuning-step1-epochs1-samples{TRAIN_SIZE}-{timestamp}',\n",
    "    'method': 'bayes',\n",
    "    'metric': {\n",
    "        'name': 'eval_loss',\n",
    "        'goal': 'minimize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'optimizer': {'values': ['adam', 'adamw', 'nadam', 'adamax']},\n",
    "        'effective_batch_size': {'values': [16, 32, 64, 128, 256, 512]},\n",
    "        'learning_rate': {'values': [5e-6, 1e-6, 1e-7, 5e-7, 1e-8]}, # lower learning rates\n",
    "        'weight_decay': {'values': [0.0, 0.01, 0.1]},\n",
    "        'betas': {'values': [(0.9, 0.999), (0.95, 0.999), (0.9, 0.9999)]},\n",
    "        'warmup_ratio': {'values': [0.05, 0.1, 0.2]},\n",
    "        'epochs': {'values': [1]},\n",
    "        'num_virtual_tokens': {'values': [10, 20, 50, 100]},\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config)\n",
    "# sweep_id = '9a4oj3so' # continue the crashed sweep\n",
    "\n",
    "optimizer_map = {\n",
    "    \"adam\": torch.optim.Adam,\n",
    "    \"adamw\": torch.optim.AdamW,\n",
    "    \"nadam\": torch.optim.NAdam,\n",
    "    \"adamax\": torch.optim.Adamax\n",
    "}\n",
    "\n",
    "def sweep_train():\n",
    "    with wandb.init() as run:\n",
    "        model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "        config = wandb.config  \n",
    "        PER_DEVICE_BATCH_SIZE = 2  # higher values --> OOM\n",
    "        gradient_accumulation_steps = int(config.effective_batch_size / PER_DEVICE_BATCH_SIZE)\n",
    "        \n",
    "        training_args = TrainingArguments(\n",
    "            per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            learning_rate=config.learning_rate,\n",
    "            num_train_epochs=config.epochs,\n",
    "            weight_decay=config.weight_decay,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            warmup_ratio=config.warmup_ratio,\n",
    "            save_strategy=\"no\",\n",
    "            eval_strategy=\"epoch\",\n",
    "            logging_strategy=\"steps\",\n",
    "            logging_steps=1,\n",
    "            report_to=['wandb'],\n",
    "            fp16=True,\n",
    "            fp16_full_eval=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False,\n",
    "            max_grad_norm=1,\n",
    "            # load_best_model_at_end=True\n",
    "        )\n",
    "        \n",
    "        def build_optimizer(model):\n",
    "            optimizer_class = optimizer_map[config.optimizer]\n",
    "            return optimizer_class(\n",
    "                model.parameters(),\n",
    "                lr=config.learning_rate,\n",
    "                weight_decay=config.weight_decay,\n",
    "                betas=config.betas\n",
    "            )\n",
    "\n",
    "        peft_config = PromptTuningConfig(\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "            num_virtual_tokens=config.num_virtual_tokens,\n",
    "            prompt_tuning_init=PromptTuningInit.RANDOM,\n",
    "            tokenizer_name_or_path=checkpoint\n",
    "        )\n",
    "        model.requires_grad_(False)                     # freeze base weights (precautionary)\n",
    "        model_peft = get_peft_model(model, peft_config) # inject prompt tuning parameters\n",
    "\n",
    "        print_trainable_parameters(model_peft)\n",
    "        num_trainable_parameters = return_num_trainable_parameters(model_peft)\n",
    "        \n",
    "        trainer = SFTTrainer(\n",
    "            model=model_peft,\n",
    "            train_dataset=ds_train_with_assistant_content,\n",
    "            eval_dataset=ds_valid_with_assistant_content,\n",
    "            formatting_func=formatting_func,\n",
    "            args=training_args,\n",
    "            optimizers=(build_optimizer(model_peft), None),  # (optimizer, scheduler)\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        for log in trainer.state.log_history:\n",
    "            if 'eval_loss' in log:\n",
    "                wandb.log({\n",
    "                    \"eval_loss\": log['eval_loss'],\n",
    "                    \"eval_perplexity\": math.exp(log['eval_loss']),\n",
    "                    \"step\": log['step'],\n",
    "                    \"learning_rate\": config.learning_rate,\n",
    "                    \"weight_decay\": config.weight_decay,\n",
    "                    \"betas\": config.betas,\n",
    "                    \"warmup_ratio\": config.warmup_ratio,\n",
    "                    \"effective_batch_size\": config.effective_batch_size,\n",
    "                    \"optimizer\": config.optimizer,\n",
    "                    \"num_trainable_parameters\": num_trainable_parameters\n",
    "                })\n",
    "        wandb.finish(); # finish the run\n",
    "        del trainer\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "wandb.agent(sweep_id, function=sweep_train, count=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23bb48a-3244-40ea-b5fa-6b4bc71a0066",
   "metadata": {},
   "source": [
    "# Prompt tuning - step 2 - the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e5b932a-7198-425c-a060-634c1ca40c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU 0 load: 2.00\n",
      "CPU 1 load: 2.00\n",
      "CPU 2 load: 3.00\n",
      "CPU 3 load: 3.00\n",
      "RAM Total: 27.41 GB, Used: 1.99 GB\n",
      "GPU 0 (Tesla T4) load: 0.0%\n",
      "GPU 0 (Tesla T4) VRAM Total: 16384.0 MB, Used 2407.0 MB\n",
      "Disk Total: 60.95 GB, Used: 35.22 GB\n",
      "97500 2500 5851\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "get_vm_usage_metrics()\n",
    "\n",
    "print(len(ds_train), len(ds_valid), len(ds_test))\n",
    "\n",
    "ds_train_with_assistant_content = ds_train.map(construct_message_with_assistant_content)\n",
    "ds_valid_with_assistant_content = ds_valid.map(construct_message_with_assistant_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd44b5e5-d0a5-4139-8a99-a8c954262d07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">prompt-tuning-final-model-2025-10-18_16-26-53</strong> at: <a href='https://wandb.ai/olialeshka-none/text-to-sql/runs/mmhf2ph8' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql/runs/mmhf2ph8</a><br> View project at: <a href='https://wandb.ai/olialeshka-none/text-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251018_162655-mmhf2ph8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/olialeshka_1/wandb/run-20251018_163042-22k3z62r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/olialeshka-none/text-to-sql/runs/22k3z62r' target=\"_blank\">prompt-tuning-final-model-2025-10-18_16-30-42</a></strong> to <a href='https://wandb.ai/olialeshka-none/text-to-sql' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/olialeshka-none/text-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/olialeshka-none/text-to-sql/runs/22k3z62r' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql/runs/22k3z62r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 10240 || All params: 596060160 || Trainable %: 0.00\n",
      "===== Training Setup Summary =====\n",
      "Num epochs:            1\n",
      "Effective batch size:  32\n",
      "Per-device batch size: 2\n",
      "Gradient accumulation: 16\n",
      "Dataset size:          97500\n",
      "Steps per epoch:       3046\n",
      "Total training steps:  3046\n",
      "Warmup steps:          609\n",
      "Logging steps:         80\n",
      "===================================\n",
      "Start time: 2025-10-18_16-30-45\n",
      "Starting fresh training run\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='321' max='3047' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 321/3047 24:40 < 3:30:53, 0.22 it/s, Epoch 0.11/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>7.209700</td>\n",
       "      <td>7.221970</td>\n",
       "      <td>2.813579</td>\n",
       "      <td>496224.000000</td>\n",
       "      <td>0.230796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>7.161000</td>\n",
       "      <td>7.220797</td>\n",
       "      <td>2.813698</td>\n",
       "      <td>999432.000000</td>\n",
       "      <td>0.230785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>7.211100</td>\n",
       "      <td>7.224030</td>\n",
       "      <td>2.814249</td>\n",
       "      <td>1494927.000000</td>\n",
       "      <td>0.230727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='49' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 49/313 00:17 < 01:33, 2.81 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "# resuming the prev run\n",
    "# timestamp = '2025-08-19_08-33-30'\n",
    "RUN_NAME = f'prompt-tuning-final-model-{timestamp}'\n",
    "# run_id = 'imoh6jtd'\n",
    "wandb.init(\n",
    "    project=os.environ[\"WANDB_PROJECT\"],\n",
    "    name=RUN_NAME,\n",
    "    # id=run_id,         # resume previous run if available\n",
    "    resume=\"allow\",    # allows resuming crashed run\n",
    ")\n",
    "\n",
    "\n",
    "RESUME_TRAINING = True\n",
    "OUTPUT_DIR = \"./prompt-tuning-final_model-output\"\n",
    "PER_DEVICE_BATCH_SIZE = 2  # higher values --> OOM\n",
    "\n",
    "optimizer = 'nadam'\n",
    "effective_batch_size = 32\n",
    "learning_rate = 1e-8\n",
    "weight_decay = 0.01\n",
    "betas = (0.95, 0.999)\n",
    "warmup_ratio = 0.2\n",
    "epochs = 1\n",
    "gradient_accumulation_steps = int(effective_batch_size / PER_DEVICE_BATCH_SIZE)\n",
    "num_virtual_tokens = 10\n",
    "\n",
    "optimizer_map = {\n",
    "    \"adam\": torch.optim.Adam,\n",
    "    \"adamw\": torch.optim.AdamW,\n",
    "    \"nadam\": torch.optim.NAdam,\n",
    "    \"adamax\": torch.optim.Adamax\n",
    "}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    num_train_epochs=epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=gradient_accumulation_steps*5,\n",
    "    save_total_limit=2,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=gradient_accumulation_steps*5,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=gradient_accumulation_steps*5,\n",
    "    report_to=['wandb'],\n",
    "    run_name=RUN_NAME,\n",
    "    fp16=True,\n",
    "    fp16_full_eval=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    max_grad_norm=1,\n",
    "    load_best_model_at_end=True,\n",
    "    gradient_checkpointing=True\n",
    ")\n",
    "\n",
    "def build_optimizer(model):\n",
    "    optimizer_class = optimizer_map[optimizer]\n",
    "    return optimizer_class(\n",
    "        model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        betas=betas\n",
    "    )\n",
    "\n",
    "\n",
    "peft_config = PromptTuningConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    num_virtual_tokens=num_virtual_tokens,\n",
    "    prompt_tuning_init=PromptTuningInit.RANDOM,\n",
    "    tokenizer_name_or_path=checkpoint\n",
    ")\n",
    "model.requires_grad_(False)                     # freeze base weights (precautionary)\n",
    "model_peft = get_peft_model(model, peft_config) # inject prompt tuning parameters\n",
    "\n",
    "print_trainable_parameters(model_peft)\n",
    "num_trainable_parameters = return_num_trainable_parameters(model_peft)\n",
    "        \n",
    "model.requires_grad_(False)                     # freeze base weights (precautionary)\n",
    "model_peft = get_peft_model(model, peft_config) # inject a LoRA adapter\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model_peft,\n",
    "    train_dataset=ds_train_with_assistant_content,\n",
    "    eval_dataset=ds_valid_with_assistant_content,\n",
    "    formatting_func=formatting_func,\n",
    "    args=training_args,\n",
    "    optimizers=(build_optimizer(model_peft), None),  # (optimizer, scheduler)\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=25)]\n",
    ")\n",
    "\n",
    "\n",
    "# Training setup summary\n",
    "dataset_size = len(ds_train_with_assistant_content)\n",
    "steps_per_epoch = dataset_size // (PER_DEVICE_BATCH_SIZE * gradient_accumulation_steps)\n",
    "total_steps = steps_per_epoch * epochs\n",
    "warmup_steps = int(total_steps * warmup_ratio)\n",
    "\n",
    "print(\"===== Training Setup Summary =====\")\n",
    "print(f\"Num epochs:            {epochs}\")\n",
    "print(f\"Effective batch size:  {effective_batch_size}\")\n",
    "print(f\"Per-device batch size: {PER_DEVICE_BATCH_SIZE}\")\n",
    "print(f\"Gradient accumulation: {gradient_accumulation_steps}\")\n",
    "print(f\"Dataset size:          {dataset_size}\")\n",
    "print(f\"Steps per epoch:       {steps_per_epoch}\")\n",
    "print(f\"Total training steps:  {total_steps}\")\n",
    "print(f\"Warmup steps:          {warmup_steps}\")\n",
    "print(f\"Logging steps:         {training_args.logging_steps}\")\n",
    "print(\"===================================\")\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\")\n",
    "\n",
    "\n",
    "# Training\n",
    "last_checkpoint = None\n",
    "if RESUME_TRAINING and os.path.isdir(OUTPUT_DIR):\n",
    "    last_checkpoint = get_last_checkpoint(OUTPUT_DIR)\n",
    "\n",
    "if last_checkpoint is not None:\n",
    "    print(f\"Resuming training from checkpoint: {last_checkpoint}\")\n",
    "    trainer.train(resume_from_checkpoint=last_checkpoint)\n",
    "else:\n",
    "    print(\"Starting fresh training run\")\n",
    "    trainer.train()\n",
    "\n",
    "print(f\"End time: {datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\")\n",
    "\n",
    "\n",
    "# WandB logging of eval metrics\n",
    "for log in trainer.state.log_history:\n",
    "    if 'eval_loss' in log:\n",
    "        wandb.log({\n",
    "            \"eval_loss\": log['eval_loss'],\n",
    "            \"eval_perplexity\": math.exp(log['eval_loss']),\n",
    "            \"step\": log['step'],\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"betas\": betas,\n",
    "            \"warmup_ratio\": warmup_ratio,\n",
    "            \"effective_batch_size\": effective_batch_size,\n",
    "            \"optimizer\": optimizer,\n",
    "            \"num_trainable_parameters\": num_trainable_parameters\n",
    "        })\n",
    "\n",
    "wandb.finish()  # finish the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a85071-939a-4139-a48e-acaf10eeb482",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(OUTPUT_DIR, 'final')\n",
    "trainer.save_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3646cbc6-22fe-425d-944b-becf67217666",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79e4fc47-001c-40a5-a6fb-edc174736baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU 0 load: 0.00\n",
      "CPU 1 load: 0.00\n",
      "CPU 2 load: 2.00\n",
      "CPU 3 load: 0.00\n",
      "RAM Total: 27.41 GB, Used: 1.93 GB\n",
      "GPU 0 (Tesla T4) load: 0.0%\n",
      "GPU 0 (Tesla T4) VRAM Total: 16384.0 MB, Used 2407.0 MB\n",
      "Disk Total: 60.95 GB, Used: 35.30 GB\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "get_vm_usage_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07ee1b39-1d7f-489f-ab42-eb365da720ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): Qwen3ForCausalLM(\n",
       "    (model): Qwen3Model(\n",
       "      (embed_tokens): Embedding(151936, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-27): 28 x Qwen3DecoderLayer(\n",
       "          (self_attn): Qwen3Attention(\n",
       "            (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "            (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "            (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          )\n",
       "          (mlp): Qwen3MLP(\n",
       "            (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "          (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "      (rotary_emb): Qwen3RotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       "  )\n",
       "  (prompt_encoder): ModuleDict(\n",
       "    (default): PromptEmbedding(\n",
       "      (embedding): Embedding(10, 1024)\n",
       "    )\n",
       "  )\n",
       "  (word_embeddings): Embedding(151936, 1024)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_path = './prompt-tuning-final_model-output/checkpoint-1680'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(model_path).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f13328-2275-416b-b75e-3252b39f6f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 2025-10-19_07-12-59\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cff81e5744c402bafc1218bf80938cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olialeshka_1/myenv/lib/python3.12/site-packages/peft/peft_model.py:2066: UserWarning: Position ids are not supported for parameter efficient tuning. Ignoring position ids.\n",
      "  warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "ENABLE_THINKING = False\n",
    "MAX_NEW_TOKENS = 512\n",
    "\n",
    "\n",
    "prompts = [ds_test[id]['sql_prompt'] for id in range(len(ds_test))]\n",
    "contexts = [ds_test[id]['sql_context'] for id in range(len(ds_test))]\n",
    "\n",
    "responses = []\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\")\n",
    "for i in tqdm(range(0, len(prompts), BATCH_SIZE)):\n",
    "    batch_prompts = prompts[i : i + BATCH_SIZE]\n",
    "    batch_contexts = contexts[i : i + BATCH_SIZE]\n",
    "\n",
    "    messages_list = [\n",
    "        construct_message(prompt=p, context=c)\n",
    "        for p, c in zip(batch_prompts, batch_contexts)\n",
    "    ]\n",
    "\n",
    "    batch_responses = generate_model_response_batch(model, tokenizer, messages_list, enable_thinking=ENABLE_THINKING, max_new_tokens=MAX_NEW_TOKENS)\n",
    "\n",
    "    responses.extend(batch_responses)\n",
    "\n",
    "print(f\"End time: {datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd50f297-d73c-4fb2-81a4-3952da67e019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ec0427f13554eb1b4b01aec9e2e5fb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5851 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m references = [ds_test[\u001b[38;5;28mid\u001b[39m][\u001b[33m'\u001b[39m\u001b[33msql\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(ds_test))]\n\u001b[32m      2\u001b[39m predictions = [responses[\u001b[38;5;28mid\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(ds_test))]\n\u001b[32m      4\u001b[39m scores = [\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[43mevaluate_sql_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreference\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43msql_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m reference, prediction, context \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mzip\u001b[39m(references, predictions, contexts), total=\u001b[38;5;28mlen\u001b[39m(ds_test))\n\u001b[32m     11\u001b[39m ]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mevaluate_sql_response\u001b[39m\u001b[34m(reference, prediction, sql_context)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluate_sql_response\u001b[39m(reference, prediction, sql_context):\n\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# ROUGE-L\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     rouge_score = \u001b[43mcompute_rouge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreference\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m# execution check\u001b[39;00m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mcompute_rouge\u001b[39m\u001b[34m(reference, prediction)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_rouge\u001b[39m(reference, prediction):\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     result = \u001b[43mrouge\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mreference\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[33m'\u001b[39m\u001b[33mrougeL\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.12/site-packages/evaluate/module.py:467\u001b[39m, in \u001b[36mEvaluationModule.compute\u001b[39m\u001b[34m(self, predictions, references, **kwargs)\u001b[39m\n\u001b[32m    465\u001b[39m inputs = {input_name: \u001b[38;5;28mself\u001b[39m.data[input_name][:] \u001b[38;5;28;01mfor\u001b[39;00m input_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._feature_names()}\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m temp_seed(\u001b[38;5;28mself\u001b[39m.seed):\n\u001b[32m--> \u001b[39m\u001b[32m467\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcompute_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.buf_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    470\u001b[39m     \u001b[38;5;28mself\u001b[39m.buf_writer = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--rouge/b01e0accf3bd6dd24839b769a5fda24e14995071570870922c71970b3a6ed886/rouge.py:149\u001b[39m, in \u001b[36mRouge._compute\u001b[39m\u001b[34m(self, predictions, references, rouge_types, use_aggregator, use_stemmer, tokenizer)\u001b[39m\n\u001b[32m    146\u001b[39m         scores.append(score)\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_aggregator:\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     result = \u001b[43maggregator\u001b[49m\u001b[43m.\u001b[49m\u001b[43maggregate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m result:\n\u001b[32m    151\u001b[39m         result[key] = result[key].mid.fmeasure\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.12/site-packages/rouge_score/scoring.py:124\u001b[39m, in \u001b[36mBootstrapAggregator.aggregate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    122\u001b[39m score_matrix = np.vstack(\u001b[38;5;28mtuple\u001b[39m(scores))\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Percentiles are returned as (interval, measure).\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m percentiles = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bootstrap_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscore_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# Extract the three intervals (low, mid, high).\u001b[39;00m\n\u001b[32m    126\u001b[39m intervals = \u001b[38;5;28mtuple\u001b[39m(\n\u001b[32m    127\u001b[39m     (scores[\u001b[32m0\u001b[39m].\u001b[34m__class__\u001b[39m(*percentiles[j, :]) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m3\u001b[39m)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.12/site-packages/rouge_score/scoring.py:149\u001b[39m, in \u001b[36mBootstrapAggregator._bootstrap_resample\u001b[39m\u001b[34m(self, matrix)\u001b[39m\n\u001b[32m    147\u001b[39m sample_mean = np.zeros((\u001b[38;5;28mself\u001b[39m._n_samples, matrix.shape[\u001b[32m1\u001b[39m]))\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m._n_samples):\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m   sample_idx = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmatrix\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmatrix\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m   sample = matrix[sample_idx, :]\n\u001b[32m    152\u001b[39m   sample_mean[i, :] = np.mean(sample, axis=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mnumpy/random/mtrand.pyx:1001\u001b[39m, in \u001b[36mnumpy.random.mtrand.RandomState.choice\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:3445\u001b[39m, in \u001b[36mprod\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, initial, where)\u001b[39m\n\u001b[32m   3328\u001b[39m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_prod_dispatcher)\n\u001b[32m   3329\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprod\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=np._NoValue,\n\u001b[32m   3330\u001b[39m          initial=np._NoValue, where=np._NoValue):\n\u001b[32m   3331\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3332\u001b[39m \u001b[33;03m    Return the product of array elements over a given axis.\u001b[39;00m\n\u001b[32m   3333\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3443\u001b[39m \u001b[33;03m    10\u001b[39;00m\n\u001b[32m   3444\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3445\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmultiply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprod\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3446\u001b[39m \u001b[43m                          \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:86\u001b[39m, in \u001b[36m_wrapreduction\u001b[39m\u001b[34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[39m\n\u001b[32m     83\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     84\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis=axis, out=out, **passkwargs)\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "references = [ds_test[id]['sql'] for id in range(len(ds_test))]\n",
    "predictions = [responses[id]['content'] for id in range(len(ds_test))]\n",
    "\n",
    "scores = [\n",
    "    evaluate_sql_response(\n",
    "        reference=reference,\n",
    "        prediction=prediction,\n",
    "        sql_context=context\n",
    "    )\n",
    "    for reference, prediction, context in tqdm(zip(references, predictions, contexts), total=len(ds_test))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c89f771-1522-4f51-881b-4ddd41249e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean test set score: 0.052\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean test set score: {np.mean([score['final_score'] for score in scores]):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92efc8d8-1149-47b4-8dfa-3c068af3f82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('prompt_tuning_test_predictions.pkl', 'wb') as f:\n",
    "    pickle.dump(predictions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcfbd08-58f8-4bd5-8ecc-b56f09de26a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python T4",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
