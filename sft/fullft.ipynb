{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e84451c-1ae2-4446-a896-1ab971c9612e",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81de34e8-a5c3-4638-af77-78c396cea599",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import datasets\n",
    "import evaluate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, EarlyStoppingCallback\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from trl import SFTTrainer\n",
    "from accelerate import cpu_offload\n",
    "import sqlite3\n",
    "import sqlparse\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import wandb\n",
    "import psutil\n",
    "import GPUtil\n",
    "import os\n",
    "import gc\n",
    "import math\n",
    "\n",
    "import _config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fdad24c-9f36-4778-8ff8-54608e5883fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENABLE_THINKING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "182dcb56-ff15-4e23-8b42-e75aeac4cf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = _config.WANDB_API_KEY\n",
    "os.environ[\"WANDB_PROJECT\"] = _config.WANDB_PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5339c1d6-98a8-49c1-a69c-0b83820dc9be",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e2b2599-ab5a-4542-9e96-e4315eddc80b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "CPU 0 load: 0.00\n",
      "CPU 1 load: 0.00\n",
      "CPU 2 load: 0.00\n",
      "CPU 3 load: 0.00\n",
      "RAM Total: 27.41 GB, Used: 1.12 GB\n",
      "GPU 0 (Tesla T4) load: 0.0%\n",
      "GPU 0 (Tesla T4) VRAM Total: 16384.0 MB, Used 3.0 MB\n",
      "Disk Total: 60.95 GB, Used: 48.28 GB\n"
     ]
    }
   ],
   "source": [
    "def get_vm_usage_metrics():\n",
    "    # CPU usage\n",
    "    cpu_load = psutil.cpu_percent(interval=1, percpu=True)\n",
    "    for id, load in enumerate(cpu_load):\n",
    "        print(f\"CPU {id} load: {load:.2f}\")\n",
    "    # RAM usage\n",
    "    ram = psutil.virtual_memory()\n",
    "    print(f\"RAM Total: {ram.total/(1024**3):.2f} GB, Used: {(ram.used)/(1024**3):.2f} GB\")\n",
    "    # GPU\n",
    "    if torch.cuda.is_available():\n",
    "        gpus = GPUtil.getGPUs()\n",
    "        for gpu in gpus:\n",
    "            print(f\"GPU {gpu.id} ({gpu.name}) load: {gpu.load*100}%\")\n",
    "            print(f\"GPU {gpu.id} ({gpu.name}) VRAM Total: {gpu.memoryTotal} MB, Used {gpu.memoryUsed} MB\")\n",
    "    # Disk \n",
    "    disk = psutil.disk_usage('/')\n",
    "    print(f\"Disk Total: {disk.total/(1024**3):.2f} GB, Used: {(disk.used)/(1024**3):.2f} GB\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f'Device: {device}')\n",
    "get_vm_usage_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c64ffa-c729-43f4-b32f-6be96e4cfe0c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cce15154-ce9c-42ea-aad3-6cb9a75ce806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'domain', 'domain_description', 'sql_complexity', 'sql_complexity_description', 'sql_task_type', 'sql_task_type_description', 'sql_prompt', 'sql_context', 'sql', 'sql_explanation'],\n",
       "    num_rows: 97500\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = datasets.load_dataset('gretelai/synthetic_text_to_sql', streaming=False)\n",
    "ds_train, ds_test = ds['train'], ds['test']\n",
    "\n",
    "split = ds_train.train_test_split(test_size=0.025, seed=42)\n",
    "ds_train = split['train']\n",
    "ds_valid = split['test']\n",
    "\n",
    "ds_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc899f6-b89f-455d-8033-f3b39ebc1886",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e8c8ee0-69cf-4c00-a542-7e4479c03431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU 0 load: 0.00\n",
      "CPU 1 load: 2.00\n",
      "CPU 2 load: 1.00\n",
      "CPU 3 load: 0.00\n",
      "RAM Total: 27.41 GB, Used: 1.35 GB\n",
      "GPU 0 (Tesla T4) load: 0.0%\n",
      "GPU 0 (Tesla T4) VRAM Total: 16384.0 MB, Used 2985.0 MB\n",
      "Disk Total: 60.95 GB, Used: 48.28 GB\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"Qwen/Qwen3-0.6B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    checkpoint,\n",
    "    device_map=\"cuda\",\n",
    ")\n",
    "# model = cpu_offload(model)\n",
    "get_vm_usage_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "912ded5e-a7cb-4a4f-9fce-5254e8dee39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_response_batch(model, tokenizer, messages_list, enable_thinking=True, max_new_tokens=512):\n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=enable_thinking\n",
    "        )\n",
    "        for messages in messages_list\n",
    "    ]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        padding_side='left'\n",
    "    ).to(model.device)\n",
    "\n",
    "    model.eval()\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=max_new_tokens\n",
    "    )\n",
    "\n",
    "    responses = []\n",
    "    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids):\n",
    "        # Slice to get only generated part\n",
    "        output_only_ids = output_ids[len(input_ids):].tolist()\n",
    "\n",
    "        # Try to find `</think>` (id 151668)\n",
    "        try:\n",
    "            index = len(output_only_ids) - output_only_ids[::-1].index(151668)\n",
    "        except ValueError:\n",
    "            index = 0\n",
    "\n",
    "        if enable_thinking:\n",
    "            thinking_content = tokenizer.decode(\n",
    "                output_only_ids[:index],\n",
    "                skip_special_tokens=True\n",
    "            ).strip(\"\\n\")\n",
    "            content = tokenizer.decode(\n",
    "                output_only_ids[index:],\n",
    "                skip_special_tokens=True\n",
    "            ).strip(\"\\n\")\n",
    "        else:\n",
    "            thinking_content = None\n",
    "            content = tokenizer.decode(\n",
    "                output_only_ids,\n",
    "                skip_special_tokens=True\n",
    "            ).strip(\"\\n\")\n",
    "\n",
    "        responses.append({\n",
    "            'thinking_content': thinking_content,\n",
    "            'content': content\n",
    "        })\n",
    "\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b957410-6efa-4926-a3c9-e5fdc63513ea",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "932573b4-2ede-4724-a6a8-14428dd6125a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def normalize_sql(sql):\n",
    "    return sqlparse.format(sql, reindent=True, keyword_case='upper').strip()\n",
    "\n",
    "def compute_rouge(reference, prediction):\n",
    "    result = rouge.compute(predictions=[prediction], references=[reference])\n",
    "    return result['rougeL']\n",
    "\n",
    "def evaluate_sql_response(reference, prediction, sql_context):\n",
    "    # ROUGE-L\n",
    "    rouge_score = compute_rouge(reference, prediction)\n",
    "    \n",
    "    # execution check\n",
    "    try:\n",
    "        conn = sqlite3.connect(\":memory:\")\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.executescript(sql_context)\n",
    "        cursor.execute(reference)\n",
    "        ref_result = cursor.fetchall()\n",
    "        \n",
    "        cursor.execute(prediction)\n",
    "        model_result = cursor.fetchall()\n",
    "        \n",
    "        execution_match = ref_result == model_result\n",
    "    except Exception:\n",
    "        execution_match = False\n",
    "    finally:\n",
    "        conn.close()\n",
    "    \n",
    "    # final score\n",
    "    if execution_match:\n",
    "        final_score = 1.0\n",
    "    else:\n",
    "        final_score = 0.7 * rouge_score\n",
    "\n",
    "    return {\n",
    "        \"rougeL\": round(rouge_score, 4),\n",
    "        \"execution_match\": execution_match,\n",
    "        \"final_score\": final_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd975079-9037-4745-a8d4-d9d828ee2257",
   "metadata": {},
   "source": [
    "# Formatting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7aa8c985-2ebd-4f95-a5d3-a804abcce942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for training\n",
    "def construct_message_with_assistant_content(example):\n",
    "    messages = construct_message(example['sql_prompt'], example['sql_context'])\n",
    "    messages.append({\n",
    "        'role': 'assistant',\n",
    "        'content': example['sql']\n",
    "    })\n",
    "    return {'messages': messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21145d07-3501-44ff-acaa-ca323091539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(example, enable_thinking=ENABLE_THINKING):\n",
    "    return tokenizer.apply_chat_template(\n",
    "        example[\"messages\"],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False, # no generation prompt during training\n",
    "        enable_thinking=ENABLE_THINKING \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcdb61f-44c3-449a-a7a7-632136c58c87",
   "metadata": {},
   "source": [
    "# SFT - step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94d19298-6ded-4234-82fe-a651910680df",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 4096\n",
    "VALID_SIZE = 1024\n",
    "\n",
    "ds_train_sample = ds_train.take(TRAIN_SIZE)\n",
    "ds_valid_sample = ds_valid.take(VALID_SIZE)\n",
    "\n",
    "len(ds_train_sample), len(ds_valid_sample), len(ds_test)\n",
    "\n",
    "ds_train_with_assistant_content = ds_train_sample.map(construct_message_with_assistant_content)\n",
    "ds_valid_with_assistant_content = ds_valid_sample.map(construct_message_with_assistant_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc3045e2-5576-41ad-9278-67c8dc80a01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU 0 load: 0.00\n",
      "CPU 1 load: 3.00\n",
      "CPU 2 load: 0.00\n",
      "CPU 3 load: 0.00\n",
      "RAM Total: 27.41 GB, Used: 3.07 GB\n",
      "GPU 0 (Tesla T4) load: 0.0%\n",
      "GPU 0 (Tesla T4) VRAM Total: 16384.0 MB, Used 2385.0 MB\n",
      "Disk Total: 60.95 GB, Used: 31.83 GB\n"
     ]
    }
   ],
   "source": [
    "get_vm_usage_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7e80681-a80e-40e1-a891-2bb16149587e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474c1bf1-b8a5-4d13-a946-2dffc2c2cd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: br443b1o with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbetas: [0.9, 0.9999]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teffective_batch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamw\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33molialeshka\u001b[0m (\u001b[33molialeshka-none\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/olialeshka_1/code_to_sql/wandb/run-20250817_160922-br443b1o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/br443b1o' target=\"_blank\">breezy-sweep-40</a></strong> to <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/br443b1o' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/runs/br443b1o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 08:53, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.510800</td>\n",
       "      <td>0.537351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/mean_token_accuracy</td><td>▁</td></tr><tr><td>eval/num_tokens</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval_loss</td><td>▁</td></tr><tr><td>eval_perplexity</td><td>▁</td></tr><tr><td>learning_rate</td><td>▁</td></tr><tr><td>step</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>█▇▆▄▃▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>train/learning_rate</td><td>▁▃▅█████▇▇▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▃▃▃▃▃▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>██▇▆▅▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/mean_token_accuracy</td><td>▁▁▂▂▄▆▇▇▇▇▇▇▇▇█▇▇▇████▇█████████████████</td></tr><tr><td>train/num_tokens</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>warmup_ratio</td><td>▁</td></tr><tr><td>weight_decay</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>64</td></tr><tr><td>eval/loss</td><td>0.53735</td></tr><tr><td>eval/mean_token_accuracy</td><td>0.86883</td></tr><tr><td>eval/num_tokens</td><td>799399</td></tr><tr><td>eval/runtime</td><td>41.8312</td></tr><tr><td>eval/samples_per_second</td><td>24.479</td></tr><tr><td>eval/steps_per_second</td><td>3.06</td></tr><tr><td>eval_loss</td><td>0.53735</td></tr><tr><td>eval_perplexity</td><td>1.71147</td></tr><tr><td>learning_rate</td><td>1e-05</td></tr><tr><td>optimizer</td><td>adamw</td></tr><tr><td>step</td><td>64</td></tr><tr><td>total_flos</td><td>2482064066347008.0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>64</td></tr><tr><td>train/grad_norm</td><td>7.22515</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.5108</td></tr><tr><td>train/mean_token_accuracy</td><td>0.87189</td></tr><tr><td>train/num_tokens</td><td>799399</td></tr><tr><td>train_loss</td><td>0.70861</td></tr><tr><td>train_runtime</td><td>545.1412</td></tr><tr><td>train_samples_per_second</td><td>7.514</td></tr><tr><td>train_steps_per_second</td><td>0.117</td></tr><tr><td>warmup_ratio</td><td>0.05</td></tr><tr><td>weight_decay</td><td>0.01</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">breezy-sweep-40</strong> at: <a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/br443b1o' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/runs/br443b1o</a><br> View project at: <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250817_160922-br443b1o/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3z5ja3hb with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbetas: [0.95, 0.999]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teffective_batch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/olialeshka_1/code_to_sql/wandb/run-20250817_161836-3z5ja3hb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/3z5ja3hb' target=\"_blank\">unique-sweep-41</a></strong> to <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/3z5ja3hb' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/runs/3z5ja3hb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 09:06, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.432100</td>\n",
       "      <td>0.465656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/mean_token_accuracy</td><td>▁</td></tr><tr><td>eval/num_tokens</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval_loss</td><td>▁</td></tr><tr><td>eval_perplexity</td><td>▁</td></tr><tr><td>learning_rate</td><td>▁</td></tr><tr><td>step</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▁▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train/grad_norm</td><td>▂▂▇▃▃▃▂█▄▂▁▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▁▃▅▆████▇▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▄▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>▂▂▄▄▆▂█▃▄▄▃▃▂▂▂▃▂▂▃▂▂▁▂▂▂▁▂▂▂▁▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train/mean_token_accuracy</td><td>▇▅▃▃▁▂▅▄▅▄▆▆▇▇▅▆▆▆▆▆▇█▇▆▇▇▇▇█▇▇█▇███▇█▇█</td></tr><tr><td>train/num_tokens</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>warmup_ratio</td><td>▁</td></tr><tr><td>weight_decay</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>64</td></tr><tr><td>eval/loss</td><td>0.46566</td></tr><tr><td>eval/mean_token_accuracy</td><td>0.87883</td></tr><tr><td>eval/num_tokens</td><td>799399</td></tr><tr><td>eval/runtime</td><td>41.7945</td></tr><tr><td>eval/samples_per_second</td><td>24.501</td></tr><tr><td>eval/steps_per_second</td><td>3.063</td></tr><tr><td>eval_loss</td><td>0.46566</td></tr><tr><td>eval_perplexity</td><td>1.59306</td></tr><tr><td>learning_rate</td><td>5e-05</td></tr><tr><td>optimizer</td><td>nadam</td></tr><tr><td>step</td><td>64</td></tr><tr><td>total_flos</td><td>2482064066347008.0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>64</td></tr><tr><td>train/grad_norm</td><td>1.17144</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.4321</td></tr><tr><td>train/mean_token_accuracy</td><td>0.88421</td></tr><tr><td>train/num_tokens</td><td>799399</td></tr><tr><td>train_loss</td><td>0.53084</td></tr><tr><td>train_runtime</td><td>554.2143</td></tr><tr><td>train_samples_per_second</td><td>7.391</td></tr><tr><td>train_steps_per_second</td><td>0.115</td></tr><tr><td>warmup_ratio</td><td>0.05</td></tr><tr><td>weight_decay</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">unique-sweep-41</strong> at: <a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/3z5ja3hb' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/runs/3z5ja3hb</a><br> View project at: <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250817_161836-3z5ja3hb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: k7whhbck with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbetas: [0.95, 0.999]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teffective_batch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamax\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/olialeshka_1/code_to_sql/wandb/run-20250817_162755-k7whhbck</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/k7whhbck' target=\"_blank\">charmed-sweep-42</a></strong> to <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/k7whhbck' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/runs/k7whhbck</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 08:49, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.447800</td>\n",
       "      <td>0.472673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/mean_token_accuracy</td><td>▁</td></tr><tr><td>eval/num_tokens</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval_loss</td><td>▁</td></tr><tr><td>eval_perplexity</td><td>▁</td></tr><tr><td>learning_rate</td><td>▁</td></tr><tr><td>step</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇█████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇██████</td></tr><tr><td>train/grad_norm</td><td>▁▁▃▅▅█▆▆▆▆▅█▆▅▇▅▅▃▅▄▄▄▄▄▃▃▄▃▃▄▃▃</td></tr><tr><td>train/learning_rate</td><td>▁▂▃▄▅▆▇█████▇▇▇▆▆▆▅▅▄▄▃▃▃▂▂▂▁▁▁▁</td></tr><tr><td>train/loss</td><td>▂▅▂▃▁▅▃▃▃▃▁▃▂▁▃▃▁▂▅▄▃▅▆▆▆▆▇▇▇█▇▇</td></tr><tr><td>train/mean_token_accuracy</td><td>█▃▇▅▇▄▆▅▅▅▇▅▇▇▆▆█▇▄▅▆▃▃▃▁▄▂▃▁▂▁▂</td></tr><tr><td>train/num_tokens</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>warmup_ratio</td><td>▁</td></tr><tr><td>weight_decay</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>128</td></tr><tr><td>eval/loss</td><td>0.47267</td></tr><tr><td>eval/mean_token_accuracy</td><td>0.87743</td></tr><tr><td>eval/num_tokens</td><td>799399</td></tr><tr><td>eval/runtime</td><td>41.7536</td></tr><tr><td>eval/samples_per_second</td><td>24.525</td></tr><tr><td>eval/steps_per_second</td><td>3.066</td></tr><tr><td>eval_loss</td><td>0.47267</td></tr><tr><td>eval_perplexity</td><td>1.60428</td></tr><tr><td>learning_rate</td><td>0.0001</td></tr><tr><td>optimizer</td><td>adamax</td></tr><tr><td>step</td><td>32</td></tr><tr><td>total_flos</td><td>2482064066347008.0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>32</td></tr><tr><td>train/grad_norm</td><td>1.00801</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.4478</td></tr><tr><td>train/mean_token_accuracy</td><td>0.88042</td></tr><tr><td>train/num_tokens</td><td>799399</td></tr><tr><td>train_loss</td><td>0.41693</td></tr><tr><td>train_runtime</td><td>545.6605</td></tr><tr><td>train_samples_per_second</td><td>7.506</td></tr><tr><td>train_steps_per_second</td><td>0.059</td></tr><tr><td>warmup_ratio</td><td>0.2</td></tr><tr><td>weight_decay</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">charmed-sweep-42</strong> at: <a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/k7whhbck' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/runs/k7whhbck</a><br> View project at: <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250817_162755-k7whhbck/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vhwxy94h with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbetas: [0.9, 0.999]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teffective_batch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/olialeshka_1/code_to_sql/wandb/run-20250817_163709-vhwxy94h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/vhwxy94h' target=\"_blank\">leafy-sweep-43</a></strong> to <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/vhwxy94h' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/runs/vhwxy94h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='128' max='128' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [128/128 09:25, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.450800</td>\n",
       "      <td>0.477449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/mean_token_accuracy</td><td>▁</td></tr><tr><td>eval/num_tokens</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval_loss</td><td>▁</td></tr><tr><td>eval_perplexity</td><td>▁</td></tr><tr><td>learning_rate</td><td>▁</td></tr><tr><td>step</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>▂▂▂▃▂▂▃▃▂▂▂▁▁▁▁▁▁▂▁▁▂▂▂▂▂▃▂▄▄▄▆▆▄▄▄▆█▅▆▇</td></tr><tr><td>train/learning_rate</td><td>▁▂▄▅███████▇▇▇▇▇▆▆▆▆▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>▁▁▂▂▁▃▂▃▂▂▂▂▃▂▂▃▂▂▂▃▃▃▃▅▄▄▅▅▆▇▇▇▆▇█▇▇█▇▇</td></tr><tr><td>train/mean_token_accuracy</td><td>█▇▇███▆█▆▇▆▇▆▆▇▇▆▇▅▆▄▄▅▄▂▂▃▁▂▃▄▁▂▂▂▂▂▁▁▁</td></tr><tr><td>train/num_tokens</td><td>▁▁▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>warmup_ratio</td><td>▁</td></tr><tr><td>weight_decay</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>32</td></tr><tr><td>eval/loss</td><td>0.47745</td></tr><tr><td>eval/mean_token_accuracy</td><td>0.87827</td></tr><tr><td>eval/num_tokens</td><td>799399</td></tr><tr><td>eval/runtime</td><td>41.7705</td></tr><tr><td>eval/samples_per_second</td><td>24.515</td></tr><tr><td>eval/steps_per_second</td><td>3.064</td></tr><tr><td>eval_loss</td><td>0.47745</td></tr><tr><td>eval_perplexity</td><td>1.61196</td></tr><tr><td>learning_rate</td><td>1e-05</td></tr><tr><td>optimizer</td><td>nadam</td></tr><tr><td>step</td><td>128</td></tr><tr><td>total_flos</td><td>2482064066347008.0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>128</td></tr><tr><td>train/grad_norm</td><td>2.6037</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.4508</td></tr><tr><td>train/mean_token_accuracy</td><td>0.87916</td></tr><tr><td>train/num_tokens</td><td>799399</td></tr><tr><td>train_loss</td><td>0.34588</td></tr><tr><td>train_runtime</td><td>569.3789</td></tr><tr><td>train_samples_per_second</td><td>7.194</td></tr><tr><td>train_steps_per_second</td><td>0.225</td></tr><tr><td>warmup_ratio</td><td>0.1</td></tr><tr><td>weight_decay</td><td>0.1</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">leafy-sweep-43</strong> at: <a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/vhwxy94h' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/runs/vhwxy94h</a><br> View project at: <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250817_163709-vhwxy94h/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: iquhvgo2 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbetas: [0.9, 0.999]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teffective_batch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamax\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/olialeshka_1/code_to_sql/wandb/run-20250817_164643-iquhvgo2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/iquhvgo2' target=\"_blank\">peach-sweep-44</a></strong> to <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/iquhvgo2' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/runs/iquhvgo2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='256' max='256' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [256/256 09:57, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.515800</td>\n",
       "      <td>0.481466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/mean_token_accuracy</td><td>▁</td></tr><tr><td>eval/num_tokens</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval_loss</td><td>▁</td></tr><tr><td>eval_perplexity</td><td>▁</td></tr><tr><td>learning_rate</td><td>▁</td></tr><tr><td>step</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇█</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▁▁▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇█████</td></tr><tr><td>train/grad_norm</td><td>▂▁▂▁▃▂▇▂▃▂▂▂▄▃▂▃▃▃▂▂▅▄▄▄▄▄▆▄▅▅▆▅▆▆▇▇▆▇▅█</td></tr><tr><td>train/learning_rate</td><td>▆▇███████▇▇▇▇▇▇▆▆▆▅▅▅▄▄▄▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>▂▂▂▂▁▃▃▂▂▃▂▃▃▃▃▃▃▄▃▂▅▄▅▅▄▅▇▆▅▆▆▇▇▆▆█▇▇▇▇</td></tr><tr><td>train/mean_token_accuracy</td><td>▇▇██▇▇▆▇▇▇▆▆▆▇▆▄▄▄▅▆▄▄▄▅▄▃▃▄▃▃▄▄▃▄▃▁▃▂▃▄</td></tr><tr><td>train/num_tokens</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>warmup_ratio</td><td>▁</td></tr><tr><td>weight_decay</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>16</td></tr><tr><td>eval/loss</td><td>0.48147</td></tr><tr><td>eval/mean_token_accuracy</td><td>0.87842</td></tr><tr><td>eval/num_tokens</td><td>799399</td></tr><tr><td>eval/runtime</td><td>41.7977</td></tr><tr><td>eval/samples_per_second</td><td>24.499</td></tr><tr><td>eval/steps_per_second</td><td>3.062</td></tr><tr><td>eval_loss</td><td>0.48147</td></tr><tr><td>eval_perplexity</td><td>1.61845</td></tr><tr><td>learning_rate</td><td>1e-05</td></tr><tr><td>optimizer</td><td>adamax</td></tr><tr><td>step</td><td>256</td></tr><tr><td>total_flos</td><td>2482064066347008.0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>256</td></tr><tr><td>train/grad_norm</td><td>4.13265</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.5158</td></tr><tr><td>train/mean_token_accuracy</td><td>0.86725</td></tr><tr><td>train/num_tokens</td><td>799399</td></tr><tr><td>train_loss</td><td>0.34032</td></tr><tr><td>train_runtime</td><td>599.909</td></tr><tr><td>train_samples_per_second</td><td>6.828</td></tr><tr><td>train_steps_per_second</td><td>0.427</td></tr><tr><td>warmup_ratio</td><td>0.05</td></tr><tr><td>weight_decay</td><td>0.01</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">peach-sweep-44</strong> at: <a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/iquhvgo2' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/runs/iquhvgo2</a><br> View project at: <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250817_164643-iquhvgo2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2kdrqip1 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbetas: [0.95, 0.999]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teffective_batch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/olialeshka_1/code_to_sql/wandb/run-20250817_165648-2kdrqip1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/2kdrqip1' target=\"_blank\">scarlet-sweep-45</a></strong> to <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/2kdrqip1' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/runs/2kdrqip1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 08:48, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.460200</td>\n",
       "      <td>0.483716</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/mean_token_accuracy</td><td>▁</td></tr><tr><td>eval/num_tokens</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval_loss</td><td>▁</td></tr><tr><td>eval_perplexity</td><td>▁</td></tr><tr><td>learning_rate</td><td>▁</td></tr><tr><td>step</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇█████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇██████</td></tr><tr><td>train/grad_norm</td><td>▁▁▂▂▁▃▆▇▇▄▃▂▂▂▃▃▂▂▄▄▄▅▇▆▆▆▇▆▆█▆▇</td></tr><tr><td>train/learning_rate</td><td>▁▂▃▄▅▆▇█████▇▇▇▆▆▆▅▅▄▄▃▃▃▂▂▂▁▁▁▁</td></tr><tr><td>train/loss</td><td>▂▂▁▂▁▂▂▂▂▂▂▃▂▂▃▃▃▃▄▄▅▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/mean_token_accuracy</td><td>▇▇█▇█▇▇▇▆▆▇▆▇▆▆▆▆▅▄▄▄▃▂▃▁▂▁▂▁▁▁▁</td></tr><tr><td>train/num_tokens</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>warmup_ratio</td><td>▁</td></tr><tr><td>weight_decay</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>128</td></tr><tr><td>eval/loss</td><td>0.48372</td></tr><tr><td>eval/mean_token_accuracy</td><td>0.87699</td></tr><tr><td>eval/num_tokens</td><td>799399</td></tr><tr><td>eval/runtime</td><td>41.5545</td></tr><tr><td>eval/samples_per_second</td><td>24.642</td></tr><tr><td>eval/steps_per_second</td><td>3.08</td></tr><tr><td>eval_loss</td><td>0.48372</td></tr><tr><td>eval_perplexity</td><td>1.62209</td></tr><tr><td>learning_rate</td><td>0.0001</td></tr><tr><td>optimizer</td><td>adam</td></tr><tr><td>step</td><td>32</td></tr><tr><td>total_flos</td><td>2482064066347008.0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>32</td></tr><tr><td>train/grad_norm</td><td>1.55571</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.4602</td></tr><tr><td>train/mean_token_accuracy</td><td>0.87962</td></tr><tr><td>train/num_tokens</td><td>799399</td></tr><tr><td>train_loss</td><td>0.34668</td></tr><tr><td>train_runtime</td><td>544.125</td></tr><tr><td>train_samples_per_second</td><td>7.528</td></tr><tr><td>train_steps_per_second</td><td>0.059</td></tr><tr><td>warmup_ratio</td><td>0.2</td></tr><tr><td>weight_decay</td><td>0.1</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">scarlet-sweep-45</strong> at: <a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/2kdrqip1' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/runs/2kdrqip1</a><br> View project at: <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250817_165648-2kdrqip1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7vxe2iku with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbetas: [0.95, 0.999]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teffective_batch_size: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamax\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/olialeshka_1/code_to_sql/wandb/run-20250817_170557-7vxe2iku</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/7vxe2iku' target=\"_blank\">summer-sweep-46</a></strong> to <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/7vxe2iku' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/runs/7vxe2iku</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 07:54, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.497400</td>\n",
       "      <td>0.515504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/mean_token_accuracy</td><td>▁</td></tr><tr><td>eval/num_tokens</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval_loss</td><td>▁</td></tr><tr><td>eval_perplexity</td><td>▁</td></tr><tr><td>learning_rate</td><td>▁</td></tr><tr><td>step</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▂▃▄▅▆▇███</td></tr><tr><td>train/global_step</td><td>▁▂▃▄▅▆▇████</td></tr><tr><td>train/grad_norm</td><td>▁▂▁▄▅▇▇█</td></tr><tr><td>train/learning_rate</td><td>▁██▇▅▄▂▁</td></tr><tr><td>train/loss</td><td>▁▁▁▂▃▆▇█</td></tr><tr><td>train/mean_token_accuracy</td><td>███▇▅▃▂▁</td></tr><tr><td>train/num_tokens</td><td>▁▂▃▄▅▆▇█</td></tr><tr><td>warmup_ratio</td><td>▁</td></tr><tr><td>weight_decay</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>512</td></tr><tr><td>eval/loss</td><td>0.5155</td></tr><tr><td>eval/mean_token_accuracy</td><td>0.87527</td></tr><tr><td>eval/num_tokens</td><td>799399</td></tr><tr><td>eval/runtime</td><td>41.6101</td></tr><tr><td>eval/samples_per_second</td><td>24.609</td></tr><tr><td>eval/steps_per_second</td><td>3.076</td></tr><tr><td>eval_loss</td><td>0.5155</td></tr><tr><td>eval_perplexity</td><td>1.67448</td></tr><tr><td>learning_rate</td><td>0.0001</td></tr><tr><td>optimizer</td><td>adamax</td></tr><tr><td>step</td><td>8</td></tr><tr><td>total_flos</td><td>2482064066347008.0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>8</td></tr><tr><td>train/grad_norm</td><td>2.33372</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.4974</td></tr><tr><td>train/mean_token_accuracy</td><td>0.8762</td></tr><tr><td>train/num_tokens</td><td>799399</td></tr><tr><td>train_loss</td><td>0.36044</td></tr><tr><td>train_runtime</td><td>536.2697</td></tr><tr><td>train_samples_per_second</td><td>7.638</td></tr><tr><td>train_steps_per_second</td><td>0.015</td></tr><tr><td>warmup_ratio</td><td>0.1</td></tr><tr><td>weight_decay</td><td>0.1</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">summer-sweep-46</strong> at: <a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/7vxe2iku' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/runs/7vxe2iku</a><br> View project at: <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250817_170557-7vxe2iku/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wqtvwgms with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbetas: [0.9, 0.9999]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teffective_batch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamax\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/olialeshka_1/code_to_sql/wandb/run-20250817_171500-wqtvwgms</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/wqtvwgms' target=\"_blank\">gentle-sweep-47</a></strong> to <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/wqtvwgms' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/runs/wqtvwgms</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='128' max='128' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [128/128 09:14, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.452600</td>\n",
       "      <td>0.482452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/mean_token_accuracy</td><td>▁</td></tr><tr><td>eval/num_tokens</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval_loss</td><td>▁</td></tr><tr><td>eval_perplexity</td><td>▁</td></tr><tr><td>learning_rate</td><td>▁</td></tr><tr><td>step</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▂▁▁▁▂▃▂▂▂▁▂▃▃▄▃▃▃▃▂▄▃▃▃▃▅▄▅█▃▄▃▅▄▃▃▄▅▆▄▄</td></tr><tr><td>train/learning_rate</td><td>▁▂▃▄▅▇██████▇▇▇▇▆▆▅▅▅▅▄▄▄▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>▂▁▁▂▂▂▂▂▃▃▃▃▃▂▃▃▄▄▃▃▅▄▆▅█▇▇▆▆▇█▆▇▇███▇▇▇</td></tr><tr><td>train/mean_token_accuracy</td><td>▆██▇█▇▇█▇▆▅▆▇▅▆▇▆▅▅▅▅▅▄▅▄▃▄▃▄▂▂▃▃▂▃▁▂▂▁▂</td></tr><tr><td>train/num_tokens</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇██</td></tr><tr><td>warmup_ratio</td><td>▁</td></tr><tr><td>weight_decay</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>32</td></tr><tr><td>eval/loss</td><td>0.48245</td></tr><tr><td>eval/mean_token_accuracy</td><td>0.87738</td></tr><tr><td>eval/num_tokens</td><td>799399</td></tr><tr><td>eval/runtime</td><td>41.5981</td></tr><tr><td>eval/samples_per_second</td><td>24.616</td></tr><tr><td>eval/steps_per_second</td><td>3.077</td></tr><tr><td>eval_loss</td><td>0.48245</td></tr><tr><td>eval_perplexity</td><td>1.62004</td></tr><tr><td>learning_rate</td><td>5e-05</td></tr><tr><td>optimizer</td><td>adamax</td></tr><tr><td>step</td><td>128</td></tr><tr><td>total_flos</td><td>2482064066347008.0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>128</td></tr><tr><td>train/grad_norm</td><td>2.51183</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.4526</td></tr><tr><td>train/mean_token_accuracy</td><td>0.87885</td></tr><tr><td>train/num_tokens</td><td>799399</td></tr><tr><td>train_loss</td><td>0.35607</td></tr><tr><td>train_runtime</td><td>558.7885</td></tr><tr><td>train_samples_per_second</td><td>7.33</td></tr><tr><td>train_steps_per_second</td><td>0.229</td></tr><tr><td>warmup_ratio</td><td>0.1</td></tr><tr><td>weight_decay</td><td>0.01</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gentle-sweep-47</strong> at: <a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/wqtvwgms' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/runs/wqtvwgms</a><br> View project at: <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250817_171500-wqtvwgms/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: tni7vibm with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbetas: [0.9, 0.999]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teffective_batch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/olialeshka_1/code_to_sql/wandb/run-20250817_172424-tni7vibm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/tni7vibm' target=\"_blank\">young-sweep-48</a></strong> to <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/tni7vibm' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/runs/tni7vibm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 09:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.469100</td>\n",
       "      <td>0.503751</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/mean_token_accuracy</td><td>▁</td></tr><tr><td>eval/num_tokens</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval_loss</td><td>▁</td></tr><tr><td>eval_perplexity</td><td>▁</td></tr><tr><td>learning_rate</td><td>▁</td></tr><tr><td>step</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>▁▂▁▂▁▂▁▁▂▂▄▃▄▄▃▂▃▂▂▃▃▂▂▃▃▄▄▄▄▅▆▅▆▆▇▇▆█▆█</td></tr><tr><td>train/learning_rate</td><td>▁▃▄▅▆████▇▇▇▇▇▇▆▆▆▆▅▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>▄▃▄▄▂▂▂▁▂▂▂▂▂▂▁▁▂▂▂▂▃▃▂▂▃▄▅▅▆▆▆▇▆█▇▇██▇▇</td></tr><tr><td>train/mean_token_accuracy</td><td>▅▆▆▇▇▇▇█▇▇▇▇▇▇▇▇▇▇█▇▇▇▆▆▅▅▄▄▄▃▃▂▃▃▃▁▂▂▂▂</td></tr><tr><td>train/num_tokens</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>warmup_ratio</td><td>▁</td></tr><tr><td>weight_decay</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>64</td></tr><tr><td>eval/loss</td><td>0.50375</td></tr><tr><td>eval/mean_token_accuracy</td><td>0.87577</td></tr><tr><td>eval/num_tokens</td><td>799399</td></tr><tr><td>eval/runtime</td><td>41.46</td></tr><tr><td>eval/samples_per_second</td><td>24.699</td></tr><tr><td>eval/steps_per_second</td><td>3.087</td></tr><tr><td>eval_loss</td><td>0.50375</td></tr><tr><td>eval_perplexity</td><td>1.65492</td></tr><tr><td>learning_rate</td><td>1e-05</td></tr><tr><td>optimizer</td><td>adam</td></tr><tr><td>step</td><td>64</td></tr><tr><td>total_flos</td><td>2482064066347008.0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>64</td></tr><tr><td>train/grad_norm</td><td>3.01256</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.4691</td></tr><tr><td>train/mean_token_accuracy</td><td>0.87933</td></tr><tr><td>train/num_tokens</td><td>799399</td></tr><tr><td>train_loss</td><td>0.34286</td></tr><tr><td>train_runtime</td><td>548.4345</td></tr><tr><td>train_samples_per_second</td><td>7.469</td></tr><tr><td>train_steps_per_second</td><td>0.117</td></tr><tr><td>warmup_ratio</td><td>0.1</td></tr><tr><td>weight_decay</td><td>0.01</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">young-sweep-48</strong> at: <a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/tni7vibm' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/runs/tni7vibm</a><br> View project at: <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250817_172424-tni7vibm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: lbipx3vm with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbetas: [0.9, 0.9999]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teffective_batch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamax\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/olialeshka_1/code_to_sql/wandb/run-20250817_173337-lbipx3vm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/lbipx3vm' target=\"_blank\">stoic-sweep-49</a></strong> to <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/lbipx3vm' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/runs/lbipx3vm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 08:46, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.491900</td>\n",
       "      <td>0.515153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/mean_token_accuracy</td><td>▁</td></tr><tr><td>eval/num_tokens</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval_loss</td><td>▁</td></tr><tr><td>eval_perplexity</td><td>▁</td></tr><tr><td>learning_rate</td><td>▁</td></tr><tr><td>step</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇█████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇██████</td></tr><tr><td>train/grad_norm</td><td>▂▁▁▃▃▅▅▆▆▅▅▆▅▅▅▅▅▅▆▅▅▅▇▆▆▅▆▅▆█▅▆</td></tr><tr><td>train/learning_rate</td><td>▁▅██████▇▇▇▇▆▆▆▅▅▅▄▄▃▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>▃▃▁▁▁▂▂▂▂▂▂▃▂▃▃▃▃▄▅▄▅▆▇▆▇▇▇▇███▇</td></tr><tr><td>train/mean_token_accuracy</td><td>▆▆█▇█▇▇▆▇▆▇▆▆▆▅▅▅▅▄▄▄▃▂▂▁▂▁▂▁▁▁▁</td></tr><tr><td>train/num_tokens</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>warmup_ratio</td><td>▁</td></tr><tr><td>weight_decay</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>128</td></tr><tr><td>eval/loss</td><td>0.51515</td></tr><tr><td>eval/mean_token_accuracy</td><td>0.87156</td></tr><tr><td>eval/num_tokens</td><td>799399</td></tr><tr><td>eval/runtime</td><td>41.4722</td></tr><tr><td>eval/samples_per_second</td><td>24.691</td></tr><tr><td>eval/steps_per_second</td><td>3.086</td></tr><tr><td>eval_loss</td><td>0.51515</td></tr><tr><td>eval_perplexity</td><td>1.67389</td></tr><tr><td>learning_rate</td><td>0.0001</td></tr><tr><td>optimizer</td><td>adamax</td></tr><tr><td>step</td><td>32</td></tr><tr><td>total_flos</td><td>2482064066347008.0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>32</td></tr><tr><td>train/grad_norm</td><td>1.92151</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.4919</td></tr><tr><td>train/mean_token_accuracy</td><td>0.8733</td></tr><tr><td>train/num_tokens</td><td>799399</td></tr><tr><td>train_loss</td><td>0.36728</td></tr><tr><td>train_runtime</td><td>541.7416</td></tr><tr><td>train_samples_per_second</td><td>7.561</td></tr><tr><td>train_steps_per_second</td><td>0.059</td></tr><tr><td>warmup_ratio</td><td>0.05</td></tr><tr><td>weight_decay</td><td>0.1</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stoic-sweep-49</strong> at: <a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/lbipx3vm' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/runs/lbipx3vm</a><br> View project at: <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250817_173337-lbipx3vm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 49x1p1yo with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbetas: [0.9, 0.9999]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teffective_batch_size: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/olialeshka_1/code_to_sql/wandb/run-20250817_174246-49x1p1yo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/49x1p1yo' target=\"_blank\">ethereal-sweep-50</a></strong> to <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/49x1p1yo' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/runs/49x1p1yo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 07:57, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.498200</td>\n",
       "      <td>0.515965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/mean_token_accuracy</td><td>▁</td></tr><tr><td>eval/num_tokens</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval_loss</td><td>▁</td></tr><tr><td>eval_perplexity</td><td>▁</td></tr><tr><td>learning_rate</td><td>▁</td></tr><tr><td>step</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▂▃▄▅▆▇███</td></tr><tr><td>train/global_step</td><td>▁▂▃▄▅▆▇████</td></tr><tr><td>train/grad_norm</td><td>▂▇██▅▁▂▅</td></tr><tr><td>train/learning_rate</td><td>▁██▇▅▄▂▁</td></tr><tr><td>train/loss</td><td>▄▁▁▁▃▅▇█</td></tr><tr><td>train/mean_token_accuracy</td><td>▆███▆▄▂▁</td></tr><tr><td>train/num_tokens</td><td>▁▂▃▄▅▆▇█</td></tr><tr><td>warmup_ratio</td><td>▁</td></tr><tr><td>weight_decay</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>512</td></tr><tr><td>eval/loss</td><td>0.51596</td></tr><tr><td>eval/mean_token_accuracy</td><td>0.87167</td></tr><tr><td>eval/num_tokens</td><td>799399</td></tr><tr><td>eval/runtime</td><td>41.6978</td></tr><tr><td>eval/samples_per_second</td><td>24.558</td></tr><tr><td>eval/steps_per_second</td><td>3.07</td></tr><tr><td>eval_loss</td><td>0.51596</td></tr><tr><td>eval_perplexity</td><td>1.67525</td></tr><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>optimizer</td><td>nadam</td></tr><tr><td>step</td><td>8</td></tr><tr><td>total_flos</td><td>2482064066347008.0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>8</td></tr><tr><td>train/grad_norm</td><td>1.47604</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.4982</td></tr><tr><td>train/mean_token_accuracy</td><td>0.87259</td></tr><tr><td>train/num_tokens</td><td>799399</td></tr><tr><td>train_loss</td><td>0.37637</td></tr><tr><td>train_runtime</td><td>539.181</td></tr><tr><td>train_samples_per_second</td><td>7.597</td></tr><tr><td>train_steps_per_second</td><td>0.015</td></tr><tr><td>warmup_ratio</td><td>0.05</td></tr><tr><td>weight_decay</td><td>0.1</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ethereal-sweep-50</strong> at: <a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/49x1p1yo' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/runs/49x1p1yo</a><br> View project at: <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250817_174246-49x1p1yo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9xnwc3f2 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbetas: [0.9, 0.999]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teffective_batch_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamax\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/olialeshka_1/code_to_sql/wandb/run-20250817_175152-9xnwc3f2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/9xnwc3f2' target=\"_blank\">solar-sweep-51</a></strong> to <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/9xnwc3f2' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/runs/9xnwc3f2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 08:29, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.493900</td>\n",
       "      <td>0.515855</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/mean_token_accuracy</td><td>▁</td></tr><tr><td>eval/num_tokens</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval_loss</td><td>▁</td></tr><tr><td>eval_perplexity</td><td>▁</td></tr><tr><td>learning_rate</td><td>▁</td></tr><tr><td>step</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇█████</td></tr><tr><td>train/grad_norm</td><td>▁▁█▃▄▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▁███▇▇▆▆▅▄▃▃▂▂▁▁</td></tr><tr><td>train/loss</td><td>▂▁█▆▅▄▃▃▃▃▃▃▃▃▃▃</td></tr><tr><td>train/mean_token_accuracy</td><td>▆█▁▂▁▃▄▄▅▅▅▅▅▅▄▄</td></tr><tr><td>train/num_tokens</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇██</td></tr><tr><td>warmup_ratio</td><td>▁</td></tr><tr><td>weight_decay</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>256</td></tr><tr><td>eval/loss</td><td>0.51585</td></tr><tr><td>eval/mean_token_accuracy</td><td>0.87012</td></tr><tr><td>eval/num_tokens</td><td>799399</td></tr><tr><td>eval/runtime</td><td>41.7764</td></tr><tr><td>eval/samples_per_second</td><td>24.511</td></tr><tr><td>eval/steps_per_second</td><td>3.064</td></tr><tr><td>eval_loss</td><td>0.51585</td></tr><tr><td>eval_perplexity</td><td>1.67507</td></tr><tr><td>learning_rate</td><td>0.0001</td></tr><tr><td>optimizer</td><td>adamax</td></tr><tr><td>step</td><td>16</td></tr><tr><td>total_flos</td><td>2482064066347008.0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>16</td></tr><tr><td>train/grad_norm</td><td>0.91086</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.4939</td></tr><tr><td>train/mean_token_accuracy</td><td>0.8719</td></tr><tr><td>train/num_tokens</td><td>799399</td></tr><tr><td>train_loss</td><td>0.5215</td></tr><tr><td>train_runtime</td><td>541.3033</td></tr><tr><td>train_samples_per_second</td><td>7.567</td></tr><tr><td>train_steps_per_second</td><td>0.03</td></tr><tr><td>warmup_ratio</td><td>0.05</td></tr><tr><td>weight_decay</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">solar-sweep-51</strong> at: <a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/9xnwc3f2' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/runs/9xnwc3f2</a><br> View project at: <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250817_175152-9xnwc3f2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: u8s9fh66 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbetas: [0.95, 0.999]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teffective_batch_size: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/olialeshka_1/code_to_sql/wandb/run-20250817_180101-u8s9fh66</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/u8s9fh66' target=\"_blank\">unique-sweep-52</a></strong> to <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/u8s9fh66' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/runs/u8s9fh66</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 08:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.494200</td>\n",
       "      <td>0.515453</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/mean_token_accuracy</td><td>▁</td></tr><tr><td>eval/num_tokens</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval_loss</td><td>▁</td></tr><tr><td>eval_perplexity</td><td>▁</td></tr><tr><td>learning_rate</td><td>▁</td></tr><tr><td>step</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▂▃▄▅▆▇███</td></tr><tr><td>train/global_step</td><td>▁▂▃▄▅▆▇████</td></tr><tr><td>train/grad_norm</td><td>█▅▄▅▄▁▁▂</td></tr><tr><td>train/learning_rate</td><td>▁██▇▅▄▂▁</td></tr><tr><td>train/loss</td><td>▁▄▄▄▄▆▇█</td></tr><tr><td>train/mean_token_accuracy</td><td>█▅▄▅▅▃▂▁</td></tr><tr><td>train/num_tokens</td><td>▁▂▃▄▅▆▇█</td></tr><tr><td>warmup_ratio</td><td>▁</td></tr><tr><td>weight_decay</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>512</td></tr><tr><td>eval/loss</td><td>0.51545</td></tr><tr><td>eval/mean_token_accuracy</td><td>0.87024</td></tr><tr><td>eval/num_tokens</td><td>799399</td></tr><tr><td>eval/runtime</td><td>41.7769</td></tr><tr><td>eval/samples_per_second</td><td>24.511</td></tr><tr><td>eval/steps_per_second</td><td>3.064</td></tr><tr><td>eval_loss</td><td>0.51545</td></tr><tr><td>eval_perplexity</td><td>1.6744</td></tr><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>optimizer</td><td>adam</td></tr><tr><td>step</td><td>8</td></tr><tr><td>total_flos</td><td>2482064066347008.0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>8</td></tr><tr><td>train/grad_norm</td><td>0.73353</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.4942</td></tr><tr><td>train/mean_token_accuracy</td><td>0.87194</td></tr><tr><td>train/num_tokens</td><td>799399</td></tr><tr><td>train_loss</td><td>0.3956</td></tr><tr><td>train_runtime</td><td>542.7284</td></tr><tr><td>train_samples_per_second</td><td>7.547</td></tr><tr><td>train_steps_per_second</td><td>0.015</td></tr><tr><td>warmup_ratio</td><td>0.05</td></tr><tr><td>weight_decay</td><td>0.1</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">unique-sweep-52</strong> at: <a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/u8s9fh66' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/runs/u8s9fh66</a><br> View project at: <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250817_180101-u8s9fh66/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: o86ugvtp with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbetas: [0.9, 0.9999]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teffective_batch_size: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/olialeshka_1/code_to_sql/wandb/run-20250817_181010-o86ugvtp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/o86ugvtp' target=\"_blank\">crisp-sweep-53</a></strong> to <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/o86ugvtp' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/runs/o86ugvtp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 07:59, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.491800</td>\n",
       "      <td>0.512805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/mean_token_accuracy</td><td>▁</td></tr><tr><td>eval/num_tokens</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval_loss</td><td>▁</td></tr><tr><td>eval_perplexity</td><td>▁</td></tr><tr><td>learning_rate</td><td>▁</td></tr><tr><td>step</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▂▃▄▅▆▇███</td></tr><tr><td>train/global_step</td><td>▁▂▃▄▅▆▇████</td></tr><tr><td>train/grad_norm</td><td>█▅▃▄▂▁▂▄</td></tr><tr><td>train/learning_rate</td><td>▁██▇▅▄▂▁</td></tr><tr><td>train/loss</td><td>▁▄▄▄▄▆▇█</td></tr><tr><td>train/mean_token_accuracy</td><td>█▅▄▅▅▃▂▁</td></tr><tr><td>train/num_tokens</td><td>▁▂▃▄▅▆▇█</td></tr><tr><td>warmup_ratio</td><td>▁</td></tr><tr><td>weight_decay</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>512</td></tr><tr><td>eval/loss</td><td>0.5128</td></tr><tr><td>eval/mean_token_accuracy</td><td>0.87107</td></tr><tr><td>eval/num_tokens</td><td>799399</td></tr><tr><td>eval/runtime</td><td>41.7681</td></tr><tr><td>eval/samples_per_second</td><td>24.516</td></tr><tr><td>eval/steps_per_second</td><td>3.065</td></tr><tr><td>eval_loss</td><td>0.5128</td></tr><tr><td>eval_perplexity</td><td>1.66997</td></tr><tr><td>learning_rate</td><td>1e-05</td></tr><tr><td>optimizer</td><td>nadam</td></tr><tr><td>step</td><td>8</td></tr><tr><td>total_flos</td><td>2482064066347008.0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>8</td></tr><tr><td>train/grad_norm</td><td>0.835</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.4918</td></tr><tr><td>train/mean_token_accuracy</td><td>0.87268</td></tr><tr><td>train/num_tokens</td><td>799399</td></tr><tr><td>train_loss</td><td>0.39188</td></tr><tr><td>train_runtime</td><td>540.9463</td></tr><tr><td>train_samples_per_second</td><td>7.572</td></tr><tr><td>train_steps_per_second</td><td>0.015</td></tr><tr><td>warmup_ratio</td><td>0.1</td></tr><tr><td>weight_decay</td><td>0.01</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">crisp-sweep-53</strong> at: <a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/o86ugvtp' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/runs/o86ugvtp</a><br> View project at: <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250817_181010-o86ugvtp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: bo4whbdv with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbetas: [0.9, 0.9999]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teffective_batch_size: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/olialeshka_1/code_to_sql/wandb/run-20250817_181919-bo4whbdv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/bo4whbdv' target=\"_blank\">ethereal-sweep-54</a></strong> to <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/bo4whbdv' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/runs/bo4whbdv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 07:58, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.573600</td>\n",
       "      <td>0.581259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/mean_token_accuracy</td><td>▁</td></tr><tr><td>eval/num_tokens</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval_loss</td><td>▁</td></tr><tr><td>eval_perplexity</td><td>▁</td></tr><tr><td>learning_rate</td><td>▁</td></tr><tr><td>step</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▂▃▄▅▆▇███</td></tr><tr><td>train/global_step</td><td>▁▂▃▄▅▆▇████</td></tr><tr><td>train/grad_norm</td><td>▁▁▄█▆▄▂▁</td></tr><tr><td>train/learning_rate</td><td>▁▅██▆▅▃▁</td></tr><tr><td>train/loss</td><td>▁▂▄█▇▇▆▆</td></tr><tr><td>train/mean_token_accuracy</td><td>█▆▄▁▁▁▂▂</td></tr><tr><td>train/num_tokens</td><td>▁▂▃▄▅▆▇█</td></tr><tr><td>warmup_ratio</td><td>▁</td></tr><tr><td>weight_decay</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>512</td></tr><tr><td>eval/loss</td><td>0.58126</td></tr><tr><td>eval/mean_token_accuracy</td><td>0.86136</td></tr><tr><td>eval/num_tokens</td><td>799399</td></tr><tr><td>eval/runtime</td><td>41.8214</td></tr><tr><td>eval/samples_per_second</td><td>24.485</td></tr><tr><td>eval/steps_per_second</td><td>3.061</td></tr><tr><td>eval_loss</td><td>0.58126</td></tr><tr><td>eval_perplexity</td><td>1.78829</td></tr><tr><td>learning_rate</td><td>0.0001</td></tr><tr><td>optimizer</td><td>adam</td></tr><tr><td>step</td><td>8</td></tr><tr><td>total_flos</td><td>2482064066347008.0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>8</td></tr><tr><td>train/grad_norm</td><td>1.25285</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>0.5736</td></tr><tr><td>train/mean_token_accuracy</td><td>0.86103</td></tr><tr><td>train/num_tokens</td><td>799399</td></tr><tr><td>train_loss</td><td>0.5373</td></tr><tr><td>train_runtime</td><td>540.6308</td></tr><tr><td>train_samples_per_second</td><td>7.576</td></tr><tr><td>train_steps_per_second</td><td>0.015</td></tr><tr><td>warmup_ratio</td><td>0.2</td></tr><tr><td>weight_decay</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ethereal-sweep-54</strong> at: <a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/bo4whbdv' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/runs/bo4whbdv</a><br> View project at: <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250817_181919-bo4whbdv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: qpnyytus with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbetas: [0.9, 0.9999]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teffective_batch_size: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/olialeshka_1/code_to_sql/wandb/run-20250817_182828-qpnyytus</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/qpnyytus' target=\"_blank\">rural-sweep-55</a></strong> to <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/qpnyytus' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/runs/qpnyytus</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 07:58, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.531900</td>\n",
       "      <td>0.555546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/mean_token_accuracy</td><td>▁</td></tr><tr><td>eval/num_tokens</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval_loss</td><td>▁</td></tr><tr><td>eval_perplexity</td><td>▁</td></tr><tr><td>learning_rate</td><td>▁</td></tr><tr><td>step</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▂▃▄▅▆▇███</td></tr><tr><td>train/global_step</td><td>▁▂▃▄▅▆▇████</td></tr><tr><td>train/grad_norm</td><td>██▄▂▁▁▁▂</td></tr><tr><td>train/learning_rate</td><td>▁▅██▆▅▃▁</td></tr><tr><td>train/loss</td><td>▁▂▃▄▅▆▇█</td></tr><tr><td>train/mean_token_accuracy</td><td>█▇▅▄▄▃▂▁</td></tr><tr><td>train/num_tokens</td><td>▁▂▃▄▅▆▇█</td></tr><tr><td>warmup_ratio</td><td>▁</td></tr><tr><td>weight_decay</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>512</td></tr><tr><td>eval/loss</td><td>0.55555</td></tr><tr><td>eval/mean_token_accuracy</td><td>0.8662</td></tr><tr><td>eval/num_tokens</td><td>799399</td></tr><tr><td>eval/runtime</td><td>41.8418</td></tr><tr><td>eval/samples_per_second</td><td>24.473</td></tr><tr><td>eval/steps_per_second</td><td>3.059</td></tr><tr><td>eval_loss</td><td>0.55555</td></tr><tr><td>eval_perplexity</td><td>1.74289</td></tr><tr><td>learning_rate</td><td>1e-05</td></tr><tr><td>optimizer</td><td>nadam</td></tr><tr><td>step</td><td>8</td></tr><tr><td>total_flos</td><td>2482064066347008.0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>8</td></tr><tr><td>train/grad_norm</td><td>1.13832</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.5319</td></tr><tr><td>train/mean_token_accuracy</td><td>0.86821</td></tr><tr><td>train/num_tokens</td><td>799399</td></tr><tr><td>train_loss</td><td>0.41396</td></tr><tr><td>train_runtime</td><td>551.1007</td></tr><tr><td>train_samples_per_second</td><td>7.432</td></tr><tr><td>train_steps_per_second</td><td>0.015</td></tr><tr><td>warmup_ratio</td><td>0.2</td></tr><tr><td>weight_decay</td><td>0.01</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rural-sweep-55</strong> at: <a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/qpnyytus' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/runs/qpnyytus</a><br> View project at: <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250817_182828-qpnyytus/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2kmokyfk with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbetas: [0.9, 0.9999]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teffective_batch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamw\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/olialeshka_1/code_to_sql/wandb/run-20250817_183746-2kmokyfk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/2kmokyfk' target=\"_blank\">clean-sweep-56</a></strong> to <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/2kmokyfk' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/runs/2kmokyfk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 09:03, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.468400</td>\n",
       "      <td>0.513289</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/mean_token_accuracy</td><td>▁</td></tr><tr><td>eval/num_tokens</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval_loss</td><td>▁</td></tr><tr><td>eval_perplexity</td><td>▁</td></tr><tr><td>learning_rate</td><td>▁</td></tr><tr><td>step</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇█████</td></tr><tr><td>train/grad_norm</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▂█▅▂▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▁▂▂▄▅▆▇▇███████▇▇▆▆▆▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>▂▂▂▁▁▂▂▂▂▂▄▃▃▃█▆▅▅▄▅▄▄▅▄▄▄▄▄▄▄▄▄▄▄▅▄▄▄▄▄</td></tr><tr><td>train/mean_token_accuracy</td><td>▇▇▇▇███▇▇▆▆▄▄▄▄▁▁▂▂▃▃▃▃▃▃▄▄▄▃▃▄▃▃▃▄▃▄▄▄▄</td></tr><tr><td>train/num_tokens</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>warmup_ratio</td><td>▁</td></tr><tr><td>weight_decay</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>64</td></tr><tr><td>eval/loss</td><td>0.51329</td></tr><tr><td>eval/mean_token_accuracy</td><td>0.87167</td></tr><tr><td>eval/num_tokens</td><td>799399</td></tr><tr><td>eval/runtime</td><td>41.8303</td></tr><tr><td>eval/samples_per_second</td><td>24.48</td></tr><tr><td>eval/steps_per_second</td><td>3.06</td></tr><tr><td>eval_loss</td><td>0.51329</td></tr><tr><td>eval_perplexity</td><td>1.67078</td></tr><tr><td>learning_rate</td><td>0.0001</td></tr><tr><td>optimizer</td><td>adamw</td></tr><tr><td>step</td><td>64</td></tr><tr><td>total_flos</td><td>2482064066347008.0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>64</td></tr><tr><td>train/grad_norm</td><td>1.27935</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.4684</td></tr><tr><td>train/mean_token_accuracy</td><td>0.87671</td></tr><tr><td>train/num_tokens</td><td>799399</td></tr><tr><td>train_loss</td><td>0.4517</td></tr><tr><td>train_runtime</td><td>551.6464</td></tr><tr><td>train_samples_per_second</td><td>7.425</td></tr><tr><td>train_steps_per_second</td><td>0.116</td></tr><tr><td>warmup_ratio</td><td>0.2</td></tr><tr><td>weight_decay</td><td>0.01</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">clean-sweep-56</strong> at: <a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/2kmokyfk' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/runs/2kmokyfk</a><br> View project at: <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250817_183746-2kmokyfk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: h5fs3xss with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbetas: [0.9, 0.999]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teffective_batch_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/olialeshka_1/code_to_sql/wandb/run-20250817_184703-h5fs3xss</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/h5fs3xss' target=\"_blank\">faithful-sweep-57</a></strong> to <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/h5fs3xss' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/runs/h5fs3xss</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 08:30, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.520200</td>\n",
       "      <td>0.547764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/mean_token_accuracy</td><td>▁</td></tr><tr><td>eval/num_tokens</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval_loss</td><td>▁</td></tr><tr><td>eval_perplexity</td><td>▁</td></tr><tr><td>learning_rate</td><td>▁</td></tr><tr><td>step</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇█████</td></tr><tr><td>train/grad_norm</td><td>▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▁▃▅▆███▇▆▅▅▄▃▂▁▁</td></tr><tr><td>train/loss</td><td>▂▂▂▁▂▃▆▅▅▅▅▆▆▇██</td></tr><tr><td>train/mean_token_accuracy</td><td>▆▇▇█▇▅▂▃▃▃▄▃▂▂▁▁</td></tr><tr><td>train/num_tokens</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇██</td></tr><tr><td>warmup_ratio</td><td>▁</td></tr><tr><td>weight_decay</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>256</td></tr><tr><td>eval/loss</td><td>0.54776</td></tr><tr><td>eval/mean_token_accuracy</td><td>0.86988</td></tr><tr><td>eval/num_tokens</td><td>799399</td></tr><tr><td>eval/runtime</td><td>41.8296</td></tr><tr><td>eval/samples_per_second</td><td>24.48</td></tr><tr><td>eval/steps_per_second</td><td>3.06</td></tr><tr><td>eval_loss</td><td>0.54776</td></tr><tr><td>eval_perplexity</td><td>1.72938</td></tr><tr><td>learning_rate</td><td>5e-05</td></tr><tr><td>optimizer</td><td>adam</td></tr><tr><td>step</td><td>16</td></tr><tr><td>total_flos</td><td>2482064066347008.0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>16</td></tr><tr><td>train/grad_norm</td><td>1.08255</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.5202</td></tr><tr><td>train/mean_token_accuracy</td><td>0.87208</td></tr><tr><td>train/num_tokens</td><td>799399</td></tr><tr><td>train_loss</td><td>0.38285</td></tr><tr><td>train_runtime</td><td>542.0952</td></tr><tr><td>train_samples_per_second</td><td>7.556</td></tr><tr><td>train_steps_per_second</td><td>0.03</td></tr><tr><td>warmup_ratio</td><td>0.2</td></tr><tr><td>weight_decay</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">faithful-sweep-57</strong> at: <a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/h5fs3xss' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/runs/h5fs3xss</a><br> View project at: <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250817_184703-h5fs3xss/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: q18i30w8 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbetas: [0.9, 0.999]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teffective_batch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamw\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/olialeshka_1/code_to_sql/wandb/run-20250817_185611-q18i30w8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/q18i30w8' target=\"_blank\">happy-sweep-58</a></strong> to <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/q18i30w8' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/runs/q18i30w8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='128' max='128' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [128/128 09:19, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.513200</td>\n",
       "      <td>0.555168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/mean_token_accuracy</td><td>▁</td></tr><tr><td>eval/num_tokens</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval_loss</td><td>▁</td></tr><tr><td>eval_perplexity</td><td>▁</td></tr><tr><td>learning_rate</td><td>▁</td></tr><tr><td>step</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇█████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇███████</td></tr><tr><td>train/grad_norm</td><td>▂▂▂▂▂▂▂▂▁▂▁▁▁▂▂▁▁▃▃▃▄▅▄▇▃▄▃▃▃▃▃▄▄▅▅▆▅▇▇█</td></tr><tr><td>train/learning_rate</td><td>▁▂▃▅▇█████▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>▂▂▂▂▁▁▁▁▁▁▁▁▁▁▂▃▃▃▃▃▃▄▃▃▃▄▄▄▄▅▆▆▅▆▆█▇███</td></tr><tr><td>train/mean_token_accuracy</td><td>▇▇▇▇▇██▇█▇▇▇▆▇▇▆▆▃▄▄▅▄▅▅▅▅▄▄▄▄▄▄▃▂▃▂▂▁▁▁</td></tr><tr><td>train/num_tokens</td><td>▁▁▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇█</td></tr><tr><td>warmup_ratio</td><td>▁</td></tr><tr><td>weight_decay</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>32</td></tr><tr><td>eval/loss</td><td>0.55517</td></tr><tr><td>eval/mean_token_accuracy</td><td>0.87162</td></tr><tr><td>eval/num_tokens</td><td>799399</td></tr><tr><td>eval/runtime</td><td>41.9199</td></tr><tr><td>eval/samples_per_second</td><td>24.428</td></tr><tr><td>eval/steps_per_second</td><td>3.053</td></tr><tr><td>eval_loss</td><td>0.55517</td></tr><tr><td>eval_perplexity</td><td>1.74223</td></tr><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>optimizer</td><td>adamw</td></tr><tr><td>step</td><td>128</td></tr><tr><td>total_flos</td><td>2482064066347008.0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>128</td></tr><tr><td>train/grad_norm</td><td>3.14924</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.5132</td></tr><tr><td>train/mean_token_accuracy</td><td>0.87468</td></tr><tr><td>train/num_tokens</td><td>799399</td></tr><tr><td>train_loss</td><td>0.30514</td></tr><tr><td>train_runtime</td><td>563.8805</td></tr><tr><td>train_samples_per_second</td><td>7.264</td></tr><tr><td>train_steps_per_second</td><td>0.227</td></tr><tr><td>warmup_ratio</td><td>0.05</td></tr><tr><td>weight_decay</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">happy-sweep-58</strong> at: <a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/q18i30w8' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/runs/q18i30w8</a><br> View project at: <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250817_185611-q18i30w8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vtp7tczz with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbetas: [0.95, 0.999]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teffective_batch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamw\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/olialeshka_1/code_to_sql/wandb/run-20250817_190549-vtp7tczz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/vtp7tczz' target=\"_blank\">northern-sweep-59</a></strong> to <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/olialeshka-none/code-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/sweeps/je9m6in0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/olialeshka-none/code-to-sql/runs/vtp7tczz' target=\"_blank\">https://wandb.ai/olialeshka-none/code-to-sql/runs/vtp7tczz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='58' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [58/64 07:24 < 00:47, 0.13 it/s, Epoch 0.89/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "sweep_config = {\n",
    "    'name': f'sweep-sft-step1-epochs1-samples{TRAIN_SIZE}-{timestamp}',\n",
    "    'method': 'bayes',\n",
    "    'metric': {\n",
    "        'name': 'eval_loss',\n",
    "        'goal': 'minimize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'optimizer': {'values': ['adam', 'adamw', 'nadam', 'adamax']},\n",
    "        'effective_batch_size': {'values': [16, 32, 64, 128, 256, 512]},\n",
    "        'learning_rate': {'values': [1e-4, 5e-5, 1e-5, 5e-6, 1e-6]},\n",
    "        'weight_decay': {'values': [0.0, 0.01, 0.1]},\n",
    "        'betas': {'values': [(0.9, 0.999), (0.95, 0.999), (0.9, 0.9999)]},\n",
    "        'warmup_ratio': {'values': [0.05, 0.1, 0.2]},\n",
    "        'epochs': {'values': [1]}\n",
    "    }\n",
    "}\n",
    "\n",
    "# sweep_id = wandb.sweep(sweep_config)\n",
    "sweep_id = 'je9m6in0' # continue the crashed sweep\n",
    "\n",
    "optimizer_map = {\n",
    "    \"adam\": torch.optim.Adam,\n",
    "    \"adamw\": torch.optim.AdamW,\n",
    "    \"nadam\": torch.optim.NAdam,\n",
    "    \"adamax\": torch.optim.Adamax\n",
    "}\n",
    "\n",
    "def sweep_train():\n",
    "    with wandb.init() as run:\n",
    "        config = wandb.config  \n",
    "        PER_DEVICE_BATCH_SIZE = 2  # higher values --> OOM\n",
    "        gradient_accumulation_steps = int(config.effective_batch_size / PER_DEVICE_BATCH_SIZE)\n",
    "        \n",
    "        training_args = TrainingArguments(\n",
    "            per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            learning_rate=config.learning_rate,\n",
    "            num_train_epochs=config.epochs,\n",
    "            weight_decay=config.weight_decay,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            warmup_ratio=config.warmup_ratio,\n",
    "            save_strategy=\"no\",\n",
    "            eval_strategy=\"epoch\",\n",
    "            logging_strategy=\"steps\",\n",
    "            logging_steps=1,\n",
    "            report_to=['wandb'],\n",
    "            fp16=True,\n",
    "            fp16_full_eval=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False,\n",
    "            max_grad_norm=1,\n",
    "            # load_best_model_at_end=True\n",
    "        )\n",
    "        \n",
    "        def build_optimizer(model):\n",
    "            optimizer_class = optimizer_map[config.optimizer]\n",
    "            return optimizer_class(\n",
    "                model.parameters(),\n",
    "                lr=config.learning_rate,\n",
    "                weight_decay=config.weight_decay,\n",
    "                betas=config.betas\n",
    "            )\n",
    "            \n",
    "        trainer = SFTTrainer(\n",
    "            model=model,\n",
    "            train_dataset=ds_train_with_assistant_content,\n",
    "            eval_dataset=ds_valid_with_assistant_content,\n",
    "            formatting_func=formatting_func,\n",
    "            args=training_args,\n",
    "            optimizers=(build_optimizer(model), None),  # (optimizer, scheduler)\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        for log in trainer.state.log_history:\n",
    "            if 'eval_loss' in log:\n",
    "                wandb.log({\n",
    "                    \"eval_loss\": log['eval_loss'],\n",
    "                    \"eval_perplexity\": math.exp(log['eval_loss']),\n",
    "                    \"step\": log['step'],\n",
    "                    \"learning_rate\": config.learning_rate,\n",
    "                    \"weight_decay\": config.weight_decay,\n",
    "                    \"betas\": config.betas,\n",
    "                    \"warmup_ratio\": config.warmup_ratio,\n",
    "                    \"effective_batch_size\": config.effective_batch_size,\n",
    "                    \"optimizer\": config.optimizer\n",
    "                })\n",
    "        wandb.finish(); # finish the run\n",
    "        del trainer\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "wandb.agent(sweep_id, function=sweep_train, count=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aeaf78d-1e8d-4820-967c-cd442cb31f10",
   "metadata": {},
   "source": [
    "# SFT - step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fe64009-33a0-4d0e-b986-b2bfabe92528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU 0 load: 0.00\n",
      "CPU 1 load: 2.00\n",
      "CPU 2 load: 2.00\n",
      "CPU 3 load: 0.00\n",
      "RAM Total: 27.41 GB, Used: 2.98 GB\n",
      "GPU 0 (Tesla T4) load: 0.0%\n",
      "GPU 0 (Tesla T4) VRAM Total: 16384.0 MB, Used 2385.0 MB\n",
      "Disk Total: 60.95 GB, Used: 31.66 GB\n",
      "8192 2048 5851\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "get_vm_usage_metrics()\n",
    "\n",
    "\n",
    "TRAIN_SIZE = 8192\n",
    "VALID_SIZE = 2048\n",
    "\n",
    "ds_train_sample = ds_train.take(TRAIN_SIZE)\n",
    "ds_valid_sample = ds_valid.take(VALID_SIZE)\n",
    "\n",
    "print(len(ds_train_sample), len(ds_valid_sample), len(ds_test))\n",
    "\n",
    "ds_train_with_assistant_content = ds_train_sample.map(construct_message_with_assistant_content)\n",
    "ds_valid_with_assistant_content = ds_valid_sample.map(construct_message_with_assistant_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27bb387f-b3a5-46fa-b529-791a6df614a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ingi197a with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbetas: [0.95, 0.999]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teffective_batch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33molialeshka\u001b[0m (\u001b[33molialeshka-none\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/olialeshka_1/text_to_sql/wandb/run-20250818_105538-ingi197a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/olialeshka-none/text-to-sql/runs/ingi197a' target=\"_blank\">treasured-sweep-7</a></strong> to <a href='https://wandb.ai/olialeshka-none/text-to-sql' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/olialeshka-none/text-to-sql/sweeps/r7pwu78y' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql/sweeps/r7pwu78y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/olialeshka-none/text-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/olialeshka-none/text-to-sql/sweeps/r7pwu78y' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql/sweeps/r7pwu78y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/olialeshka-none/text-to-sql/runs/ingi197a' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql/runs/ingi197a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 09:19, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.438300</td>\n",
       "      <td>0.468934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/mean_token_accuracy</td><td>▁</td></tr><tr><td>eval/num_tokens</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval_loss</td><td>▁</td></tr><tr><td>eval_perplexity</td><td>▁</td></tr><tr><td>learning_rate</td><td>▁</td></tr><tr><td>step</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▇▇█████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▃▂▂▂▂▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▁▃▅▆██████▇▇▇▇▇▆▆▆▆▆▅▅▅▅▄▄▄▄▃▃▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▅▃▃▃▂▂▂▂▂▂▂▁▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/mean_token_accuracy</td><td>▁▃▅▅▅▆▇▇▇▇▇▇▇▆▇▇█▇▇▇████████████████████</td></tr><tr><td>train/num_tokens</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇██</td></tr><tr><td>warmup_ratio</td><td>▁</td></tr><tr><td>weight_decay</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>64</td></tr><tr><td>eval/loss</td><td>0.46893</td></tr><tr><td>eval/mean_token_accuracy</td><td>0.87795</td></tr><tr><td>eval/num_tokens</td><td>799399</td></tr><tr><td>eval/runtime</td><td>47.8871</td></tr><tr><td>eval/samples_per_second</td><td>21.384</td></tr><tr><td>eval/steps_per_second</td><td>2.673</td></tr><tr><td>eval_loss</td><td>0.46893</td></tr><tr><td>eval_perplexity</td><td>1.59829</td></tr><tr><td>learning_rate</td><td>5e-05</td></tr><tr><td>optimizer</td><td>nadam</td></tr><tr><td>step</td><td>64</td></tr><tr><td>total_flos</td><td>2482064066347008.0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>64</td></tr><tr><td>train/grad_norm</td><td>1.19685</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.4383</td></tr><tr><td>train/mean_token_accuracy</td><td>0.88389</td></tr><tr><td>train/num_tokens</td><td>799399</td></tr><tr><td>train_loss</td><td>0.62101</td></tr><tr><td>train_runtime</td><td>567.7211</td></tr><tr><td>train_samples_per_second</td><td>7.215</td></tr><tr><td>train_steps_per_second</td><td>0.113</td></tr><tr><td>warmup_ratio</td><td>0.05</td></tr><tr><td>weight_decay</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">treasured-sweep-7</strong> at: <a href='https://wandb.ai/olialeshka-none/text-to-sql/runs/ingi197a' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql/runs/ingi197a</a><br> View project at: <a href='https://wandb.ai/olialeshka-none/text-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250818_105538-ingi197a/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6j0f8896 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbetas: [0.95, 0.999]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teffective_batch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/olialeshka_1/text_to_sql/wandb/run-20250818_110512-6j0f8896</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/olialeshka-none/text-to-sql/runs/6j0f8896' target=\"_blank\">olive-sweep-8</a></strong> to <a href='https://wandb.ai/olialeshka-none/text-to-sql' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/olialeshka-none/text-to-sql/sweeps/r7pwu78y' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql/sweeps/r7pwu78y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/olialeshka-none/text-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/olialeshka-none/text-to-sql/sweeps/r7pwu78y' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql/sweeps/r7pwu78y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/olialeshka-none/text-to-sql/runs/6j0f8896' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql/runs/6j0f8896</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 09:32, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.420600</td>\n",
       "      <td>0.457404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/mean_token_accuracy</td><td>▁</td></tr><tr><td>eval/num_tokens</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval_loss</td><td>▁</td></tr><tr><td>eval_perplexity</td><td>▁</td></tr><tr><td>learning_rate</td><td>▁</td></tr><tr><td>step</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train/grad_norm</td><td>▁▁▂▄▆▇▅█▆▅▅▅▃▃▄▃▄▂▄▃▂▂▂▁▁▂▂▃▁▂▂▂▃▂▂▁▂▂▃▂</td></tr><tr><td>train/learning_rate</td><td>▃▅▆▇███████▇▇▇▇▇▇▆▆▆▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>train/loss</td><td>▆▅▆▄▄▄█▆▆▃▅▂▄▂▃▅▅▄▃▂▂▃▁▁▂▃▂▁▄▄▄▄▅▅▄▆▆▆▆▅</td></tr><tr><td>train/mean_token_accuracy</td><td>▅▄▃▃▁▄▃▆▄▆▆▄▃▄▆▇▇███▄▆▇▇▆▅▅▅▃▄▅▃▅▄▃▅▃▃▃▄</td></tr><tr><td>train/num_tokens</td><td>▁▁▁▁▁▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>warmup_ratio</td><td>▁</td></tr><tr><td>weight_decay</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>64</td></tr><tr><td>eval/loss</td><td>0.4574</td></tr><tr><td>eval/mean_token_accuracy</td><td>0.88083</td></tr><tr><td>eval/num_tokens</td><td>799399</td></tr><tr><td>eval/runtime</td><td>47.4192</td></tr><tr><td>eval/samples_per_second</td><td>21.595</td></tr><tr><td>eval/steps_per_second</td><td>2.699</td></tr><tr><td>eval_loss</td><td>0.4574</td></tr><tr><td>eval_perplexity</td><td>1.57997</td></tr><tr><td>learning_rate</td><td>5e-05</td></tr><tr><td>optimizer</td><td>nadam</td></tr><tr><td>step</td><td>64</td></tr><tr><td>total_flos</td><td>2482064066347008.0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>64</td></tr><tr><td>train/grad_norm</td><td>1.22134</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.4206</td></tr><tr><td>train/mean_token_accuracy</td><td>0.88462</td></tr><tr><td>train/num_tokens</td><td>799399</td></tr><tr><td>train_loss</td><td>0.41559</td></tr><tr><td>train_runtime</td><td>580.9598</td></tr><tr><td>train_samples_per_second</td><td>7.05</td></tr><tr><td>train_steps_per_second</td><td>0.11</td></tr><tr><td>warmup_ratio</td><td>0.1</td></tr><tr><td>weight_decay</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">olive-sweep-8</strong> at: <a href='https://wandb.ai/olialeshka-none/text-to-sql/runs/6j0f8896' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql/runs/6j0f8896</a><br> View project at: <a href='https://wandb.ai/olialeshka-none/text-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250818_110512-6j0f8896/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5h8mck2v with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbetas: [0.95, 0.999]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teffective_batch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/olialeshka_1/text_to_sql/wandb/run-20250818_111457-5h8mck2v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/olialeshka-none/text-to-sql/runs/5h8mck2v' target=\"_blank\">copper-sweep-9</a></strong> to <a href='https://wandb.ai/olialeshka-none/text-to-sql' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/olialeshka-none/text-to-sql/sweeps/r7pwu78y' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql/sweeps/r7pwu78y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/olialeshka-none/text-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/olialeshka-none/text-to-sql/sweeps/r7pwu78y' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql/sweeps/r7pwu78y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/olialeshka-none/text-to-sql/runs/5h8mck2v' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql/runs/5h8mck2v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 09:16, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.436400</td>\n",
       "      <td>0.467642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/mean_token_accuracy</td><td>▁</td></tr><tr><td>eval/num_tokens</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval_loss</td><td>▁</td></tr><tr><td>eval_perplexity</td><td>▁</td></tr><tr><td>learning_rate</td><td>▁</td></tr><tr><td>step</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇█████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇██████</td></tr><tr><td>train/grad_norm</td><td>▁▁▃▅▅▅▇▆▃█▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▁▅██████▇▇▇▇▆▆▆▅▅▅▄▄▃▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>▂▁▁█▄▇▅▇▂█▄▃▃▃▂▂▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▅</td></tr><tr><td>train/mean_token_accuracy</td><td>██▇▁▄▅▄▃▇▂▄▄▅▆▇▆██▇▇▇▆▆▆▅▅▅▅▄▄▃▅</td></tr><tr><td>train/num_tokens</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>warmup_ratio</td><td>▁</td></tr><tr><td>weight_decay</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>128</td></tr><tr><td>eval/loss</td><td>0.46764</td></tr><tr><td>eval/mean_token_accuracy</td><td>0.87933</td></tr><tr><td>eval/num_tokens</td><td>799399</td></tr><tr><td>eval/runtime</td><td>47.2382</td></tr><tr><td>eval/samples_per_second</td><td>21.677</td></tr><tr><td>eval/steps_per_second</td><td>2.71</td></tr><tr><td>eval_loss</td><td>0.46764</td></tr><tr><td>eval_perplexity</td><td>1.59623</td></tr><tr><td>learning_rate</td><td>0.0001</td></tr><tr><td>optimizer</td><td>nadam</td></tr><tr><td>step</td><td>32</td></tr><tr><td>total_flos</td><td>2482064066347008.0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>32</td></tr><tr><td>train/grad_norm</td><td>1.01308</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.4364</td></tr><tr><td>train/mean_token_accuracy</td><td>0.88539</td></tr><tr><td>train/num_tokens</td><td>799399</td></tr><tr><td>train_loss</td><td>0.39723</td></tr><tr><td>train_runtime</td><td>573.3698</td></tr><tr><td>train_samples_per_second</td><td>7.144</td></tr><tr><td>train_steps_per_second</td><td>0.056</td></tr><tr><td>warmup_ratio</td><td>0.05</td></tr><tr><td>weight_decay</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">copper-sweep-9</strong> at: <a href='https://wandb.ai/olialeshka-none/text-to-sql/runs/5h8mck2v' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql/runs/5h8mck2v</a><br> View project at: <a href='https://wandb.ai/olialeshka-none/text-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250818_111457-5h8mck2v/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ri9qjadh with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbetas: [0.95, 0.999]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teffective_batch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/olialeshka_1/text_to_sql/wandb/run-20250818_112435-ri9qjadh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/olialeshka-none/text-to-sql/runs/ri9qjadh' target=\"_blank\">dashing-sweep-10</a></strong> to <a href='https://wandb.ai/olialeshka-none/text-to-sql' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/olialeshka-none/text-to-sql/sweeps/r7pwu78y' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql/sweeps/r7pwu78y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/olialeshka-none/text-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/olialeshka-none/text-to-sql/sweeps/r7pwu78y' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql/sweeps/r7pwu78y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/olialeshka-none/text-to-sql/runs/ri9qjadh' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql/runs/ri9qjadh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 09:18, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.450600</td>\n",
       "      <td>0.481426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/mean_token_accuracy</td><td>▁</td></tr><tr><td>eval/num_tokens</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval_loss</td><td>▁</td></tr><tr><td>eval_perplexity</td><td>▁</td></tr><tr><td>learning_rate</td><td>▁</td></tr><tr><td>step</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇█████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇██████</td></tr><tr><td>train/grad_norm</td><td>▁▁▁▂▁▃▂▂▆█▂▄▃█▅▅▅▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▁▃▅▆█████▇▇▇▇▆▆▆▅▅▅▄▄▃▃▃▂▂▂▂▁▁▁▁</td></tr><tr><td>train/loss</td><td>▂▁▁▃▂▄▃▃▅█▃▃▂▆▂▄▂▃▂▂▃▃▃▃▄▄▄▄▅▅▅▅</td></tr><tr><td>train/mean_token_accuracy</td><td>▆██▅▆▃▄▄▄▁▅▅▆▄▆▆▆▅▆▆▆▅▅▅▄▄▄▃▃▃▃▃</td></tr><tr><td>train/num_tokens</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>warmup_ratio</td><td>▁</td></tr><tr><td>weight_decay</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>128</td></tr><tr><td>eval/loss</td><td>0.48143</td></tr><tr><td>eval/mean_token_accuracy</td><td>0.8786</td></tr><tr><td>eval/num_tokens</td><td>799399</td></tr><tr><td>eval/runtime</td><td>47.649</td></tr><tr><td>eval/samples_per_second</td><td>21.49</td></tr><tr><td>eval/steps_per_second</td><td>2.686</td></tr><tr><td>eval_loss</td><td>0.48143</td></tr><tr><td>eval_perplexity</td><td>1.61838</td></tr><tr><td>learning_rate</td><td>0.0001</td></tr><tr><td>optimizer</td><td>nadam</td></tr><tr><td>step</td><td>32</td></tr><tr><td>total_flos</td><td>2482064066347008.0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>32</td></tr><tr><td>train/grad_norm</td><td>1.33997</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.4506</td></tr><tr><td>train/mean_token_accuracy</td><td>0.88336</td></tr><tr><td>train/num_tokens</td><td>799399</td></tr><tr><td>train_loss</td><td>0.36796</td></tr><tr><td>train_runtime</td><td>575.0634</td></tr><tr><td>train_samples_per_second</td><td>7.123</td></tr><tr><td>train_steps_per_second</td><td>0.056</td></tr><tr><td>warmup_ratio</td><td>0.1</td></tr><tr><td>weight_decay</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dashing-sweep-10</strong> at: <a href='https://wandb.ai/olialeshka-none/text-to-sql/runs/ri9qjadh' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql/runs/ri9qjadh</a><br> View project at: <a href='https://wandb.ai/olialeshka-none/text-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250818_112435-ri9qjadh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4b99komv with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbetas: [0.95, 0.999]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teffective_batch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/olialeshka_1/text_to_sql/wandb/run-20250818_113419-4b99komv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/olialeshka-none/text-to-sql/runs/4b99komv' target=\"_blank\">hearty-sweep-11</a></strong> to <a href='https://wandb.ai/olialeshka-none/text-to-sql' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/olialeshka-none/text-to-sql/sweeps/r7pwu78y' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql/sweeps/r7pwu78y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/olialeshka-none/text-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/olialeshka-none/text-to-sql/sweeps/r7pwu78y' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql/sweeps/r7pwu78y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/olialeshka-none/text-to-sql/runs/4b99komv' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql/runs/4b99komv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 09:16, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.496300</td>\n",
       "      <td>0.532538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/mean_token_accuracy</td><td>▁</td></tr><tr><td>eval/num_tokens</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval_loss</td><td>▁</td></tr><tr><td>eval_perplexity</td><td>▁</td></tr><tr><td>learning_rate</td><td>▁</td></tr><tr><td>step</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇█████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇██████</td></tr><tr><td>train/grad_norm</td><td>▁▂▂▄▅▄▃▃▂▁▁▂▁▂▂▁▁▂▁▁▂▂▂▂▃▃▄▅▆▇██</td></tr><tr><td>train/learning_rate</td><td>▁▅██████▇▇▇▇▆▆▆▅▅▅▄▄▃▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>▃▂▁▂▂▃▃▃▃▃▂▂▂▂▂▂▂▂▁▂▂▃▃▃▄▅▅▆▇███</td></tr><tr><td>train/mean_token_accuracy</td><td>▆▇█▆▆▅▆▅▅▅▆▆▇▆▆▆▆▆▇▇▆▆▅▅▄▄▃▂▂▁▁▁</td></tr><tr><td>train/num_tokens</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>warmup_ratio</td><td>▁</td></tr><tr><td>weight_decay</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>128</td></tr><tr><td>eval/loss</td><td>0.53254</td></tr><tr><td>eval/mean_token_accuracy</td><td>0.87625</td></tr><tr><td>eval/num_tokens</td><td>799399</td></tr><tr><td>eval/runtime</td><td>47.8094</td></tr><tr><td>eval/samples_per_second</td><td>21.418</td></tr><tr><td>eval/steps_per_second</td><td>2.677</td></tr><tr><td>eval_loss</td><td>0.53254</td></tr><tr><td>eval_perplexity</td><td>1.70325</td></tr><tr><td>learning_rate</td><td>5e-05</td></tr><tr><td>optimizer</td><td>nadam</td></tr><tr><td>step</td><td>32</td></tr><tr><td>total_flos</td><td>2482064066347008.0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>32</td></tr><tr><td>train/grad_norm</td><td>2.57582</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.4963</td></tr><tr><td>train/mean_token_accuracy</td><td>0.88032</td></tr><tr><td>train/num_tokens</td><td>799399</td></tr><tr><td>train_loss</td><td>0.27246</td></tr><tr><td>train_runtime</td><td>573.2832</td></tr><tr><td>train_samples_per_second</td><td>7.145</td></tr><tr><td>train_steps_per_second</td><td>0.056</td></tr><tr><td>warmup_ratio</td><td>0.05</td></tr><tr><td>weight_decay</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">hearty-sweep-11</strong> at: <a href='https://wandb.ai/olialeshka-none/text-to-sql/runs/4b99komv' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql/runs/4b99komv</a><br> View project at: <a href='https://wandb.ai/olialeshka-none/text-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250818_113419-4b99komv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9ks072of with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbetas: [0.95, 0.999]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teffective_batch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/olialeshka_1/text_to_sql/wandb/run-20250818_114358-9ks072of</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/olialeshka-none/text-to-sql/runs/9ks072of' target=\"_blank\">silver-sweep-12</a></strong> to <a href='https://wandb.ai/olialeshka-none/text-to-sql' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/olialeshka-none/text-to-sql/sweeps/r7pwu78y' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql/sweeps/r7pwu78y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/olialeshka-none/text-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/olialeshka-none/text-to-sql/sweeps/r7pwu78y' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql/sweeps/r7pwu78y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/olialeshka-none/text-to-sql/runs/9ks072of' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql/runs/9ks072of</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 09:18, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.524900</td>\n",
       "      <td>0.565132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/mean_token_accuracy</td><td>▁</td></tr><tr><td>eval/num_tokens</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>eval_loss</td><td>▁</td></tr><tr><td>eval_perplexity</td><td>▁</td></tr><tr><td>learning_rate</td><td>▁</td></tr><tr><td>step</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇█████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇██████</td></tr><tr><td>train/grad_norm</td><td>▁▁▁▂▃▄▃▃▂▃▂▂▂▂▂▂▂▂▁▁▂▂▂▃▃▃▄▅▆▇██</td></tr><tr><td>train/learning_rate</td><td>▁▃▅▆█████▇▇▇▇▆▆▆▅▅▅▄▄▃▃▃▂▂▂▂▁▁▁▁</td></tr><tr><td>train/loss</td><td>▃▂▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▃▃▄▅▅▆▇███</td></tr><tr><td>train/mean_token_accuracy</td><td>▆▇█▇▇▆▆▆▆▆▇▆▇▆▇▇▇▆▇▇▆▆▅▅▄▄▃▂▂▁▁▁</td></tr><tr><td>train/num_tokens</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>warmup_ratio</td><td>▁</td></tr><tr><td>weight_decay</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>128</td></tr><tr><td>eval/loss</td><td>0.56513</td></tr><tr><td>eval/mean_token_accuracy</td><td>0.87427</td></tr><tr><td>eval/num_tokens</td><td>799399</td></tr><tr><td>eval/runtime</td><td>47.8987</td></tr><tr><td>eval/samples_per_second</td><td>21.378</td></tr><tr><td>eval/steps_per_second</td><td>2.672</td></tr><tr><td>eval_loss</td><td>0.56513</td></tr><tr><td>eval_perplexity</td><td>1.75968</td></tr><tr><td>learning_rate</td><td>5e-05</td></tr><tr><td>optimizer</td><td>nadam</td></tr><tr><td>step</td><td>32</td></tr><tr><td>total_flos</td><td>2482064066347008.0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>32</td></tr><tr><td>train/grad_norm</td><td>3.22406</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.5249</td></tr><tr><td>train/mean_token_accuracy</td><td>0.88008</td></tr><tr><td>train/num_tokens</td><td>799399</td></tr><tr><td>train_loss</td><td>0.23776</td></tr><tr><td>train_runtime</td><td>574.4911</td></tr><tr><td>train_samples_per_second</td><td>7.13</td></tr><tr><td>train_steps_per_second</td><td>0.056</td></tr><tr><td>warmup_ratio</td><td>0.1</td></tr><tr><td>weight_decay</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">silver-sweep-12</strong> at: <a href='https://wandb.ai/olialeshka-none/text-to-sql/runs/9ks072of' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql/runs/9ks072of</a><br> View project at: <a href='https://wandb.ai/olialeshka-none/text-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250818_114358-9ks072of/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Exiting.\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "sweep_config = {\n",
    "    'name': f'sweep-sft-step2-epochs1-samples{TRAIN_SIZE}-{timestamp}',\n",
    "    'method': 'grid',\n",
    "    'metric': {\n",
    "        'name': 'eval_loss',\n",
    "        'goal': 'minimize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'optimizer': {'values': ['nadam']},\n",
    "        'effective_batch_size': {'values': [32, 64, 128]},\n",
    "        'learning_rate': {'values': [1e-4, 5e-5]},\n",
    "        'weight_decay': {'values': [0.0]},\n",
    "        'betas': {'values': [(0.95, 0.999)]},\n",
    "        'warmup_ratio': {'values': [0.05, 0.1]},\n",
    "        'epochs': {'values': [1]}\n",
    "    }\n",
    "}\n",
    "\n",
    "# sweep_id = wandb.sweep(sweep_config)\n",
    "sweep_id = 'r7pwu78y' # continue the crashed sweep\n",
    "\n",
    "optimizer_map = {\n",
    "    \"adam\": torch.optim.Adam,\n",
    "    \"adamw\": torch.optim.AdamW,\n",
    "    \"nadam\": torch.optim.NAdam,\n",
    "    \"adamax\": torch.optim.Adamax\n",
    "}\n",
    "\n",
    "def sweep_train():\n",
    "    with wandb.init() as run:\n",
    "        config = wandb.config  \n",
    "        PER_DEVICE_BATCH_SIZE = 2  # higher values --> OOM\n",
    "        gradient_accumulation_steps = int(config.effective_batch_size / PER_DEVICE_BATCH_SIZE)\n",
    "        \n",
    "        training_args = TrainingArguments(\n",
    "            per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            learning_rate=config.learning_rate,\n",
    "            num_train_epochs=config.epochs,\n",
    "            weight_decay=config.weight_decay,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            warmup_ratio=config.warmup_ratio,\n",
    "            save_strategy=\"no\",\n",
    "            eval_strategy=\"epoch\",\n",
    "            logging_strategy=\"steps\",\n",
    "            logging_steps=1,\n",
    "            report_to=['wandb'],\n",
    "            fp16=True,\n",
    "            fp16_full_eval=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False,\n",
    "            max_grad_norm=1,\n",
    "            # load_best_model_at_end=True\n",
    "        )\n",
    "        \n",
    "        def build_optimizer(model):\n",
    "            optimizer_class = optimizer_map[config.optimizer]\n",
    "            return optimizer_class(\n",
    "                model.parameters(),\n",
    "                lr=config.learning_rate,\n",
    "                weight_decay=config.weight_decay,\n",
    "                betas=config.betas\n",
    "            )\n",
    "            \n",
    "        trainer = SFTTrainer(\n",
    "            model=model,\n",
    "            train_dataset=ds_train_with_assistant_content,\n",
    "            eval_dataset=ds_valid_with_assistant_content,\n",
    "            formatting_func=formatting_func,\n",
    "            args=training_args,\n",
    "            optimizers=(build_optimizer(model), None),  # (optimizer, scheduler)\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        for log in trainer.state.log_history:\n",
    "            if 'eval_loss' in log:\n",
    "                wandb.log({\n",
    "                    \"eval_loss\": log['eval_loss'],\n",
    "                    \"eval_perplexity\": math.exp(log['eval_loss']),\n",
    "                    \"step\": log['step'],\n",
    "                    \"learning_rate\": config.learning_rate,\n",
    "                    \"weight_decay\": config.weight_decay,\n",
    "                    \"betas\": config.betas,\n",
    "                    \"warmup_ratio\": config.warmup_ratio,\n",
    "                    \"effective_batch_size\": config.effective_batch_size,\n",
    "                    \"optimizer\": config.optimizer\n",
    "                })\n",
    "        wandb.finish(); # finish the run\n",
    "        del trainer\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "wandb.agent(sweep_id, function=sweep_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23bb48a-3244-40ea-b5fa-6b4bc71a0066",
   "metadata": {},
   "source": [
    "# SFT - step 3 - the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e5b932a-7198-425c-a060-634c1ca40c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU 0 load: 3.00\n",
      "CPU 1 load: 0.00\n",
      "CPU 2 load: 0.00\n",
      "CPU 3 load: 1.00\n",
      "RAM Total: 27.41 GB, Used: 1.34 GB\n",
      "GPU 0 (Tesla T4) load: 0.0%\n",
      "GPU 0 (Tesla T4) VRAM Total: 16384.0 MB, Used 2405.0 MB\n",
      "Disk Total: 60.95 GB, Used: 46.04 GB\n",
      "97500 2500 5851\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "get_vm_usage_metrics()\n",
    "\n",
    "print(len(ds_train), len(ds_valid), len(ds_test))\n",
    "\n",
    "ds_train_with_assistant_content = ds_train.map(construct_message_with_assistant_content)\n",
    "ds_valid_with_assistant_content = ds_valid.map(construct_message_with_assistant_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd44b5e5-d0a5-4139-8a99-a8c954262d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33molialeshka\u001b[0m (\u001b[33molialeshka-none\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/olialeshka_1/text_to_sql/wandb/run-20250820_072235-imoh6jtd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/olialeshka-none/text-to-sql/runs/imoh6jtd' target=\"_blank\">sft-final-model-2025-08-19_08-33-30</a></strong> to <a href='https://wandb.ai/olialeshka-none/text-to-sql' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/olialeshka-none/text-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/olialeshka-none/text-to-sql/runs/imoh6jtd' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql/runs/imoh6jtd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Training Setup Summary =====\n",
      "Num epochs:            1\n",
      "Effective batch size:  32\n",
      "Per-device batch size: 2\n",
      "Gradient accumulation: 16\n",
      "Dataset size:          97500\n",
      "Steps per epoch:       3046\n",
      "Total training steps:  3046\n",
      "Warmup steps:          152\n",
      "Logging steps:         80\n",
      "===================================\n",
      "Start time: 2025-08-20_07-22-45\n",
      "Resuming training from checkpoint: ./sft-final_model-output/checkpoint-2240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3046' max='3047' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3046/3047 1:30:35 < 00:06, 0.15 it/s, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2320</td>\n",
       "      <td>0.352100</td>\n",
       "      <td>0.353870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.349800</td>\n",
       "      <td>0.351453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2480</td>\n",
       "      <td>0.340300</td>\n",
       "      <td>0.349289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2560</td>\n",
       "      <td>0.343300</td>\n",
       "      <td>0.347501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2640</td>\n",
       "      <td>0.345500</td>\n",
       "      <td>0.345989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2720</td>\n",
       "      <td>0.345000</td>\n",
       "      <td>0.344931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.336700</td>\n",
       "      <td>0.344186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2880</td>\n",
       "      <td>0.336100</td>\n",
       "      <td>0.343747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2960</td>\n",
       "      <td>0.338100</td>\n",
       "      <td>0.343571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3040</td>\n",
       "      <td>0.340200</td>\n",
       "      <td>0.343535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End time: 2025-08-20_08-55-37\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/loss</td><td>█▆▅▄▃▂▁▁▁▁</td></tr><tr><td>eval/mean_token_accuracy</td><td>▁▃▄▆▆▇████</td></tr><tr><td>eval/num_tokens</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>eval/runtime</td><td>█▂▂▁▁▁▂▁▂▂</td></tr><tr><td>eval/samples_per_second</td><td>▁▇▇███▇█▇▇</td></tr><tr><td>eval/steps_per_second</td><td>▁▇████▇█▇▇</td></tr><tr><td>eval_loss</td><td>█▆▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>eval_perplexity</td><td>█▅▇▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅▅▆▇▇▇██</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███</td></tr><tr><td>train/global_step</td><td>▁▄▅▆▇███████████████████████████████████</td></tr><tr><td>train/grad_norm</td><td>█▆▄▆▁▅▃▅▄▂</td></tr><tr><td>train/learning_rate</td><td>█▇▅▄▃▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▇▃▄▅▅▁▁▂▃</td></tr><tr><td>train/mean_token_accuracy</td><td>▁▁▆▃▃▃▇▇▆▅█</td></tr><tr><td>train/num_tokens</td><td>▁▂▃▃▄▅▆▆▇██</td></tr><tr><td>warmup_ratio</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>weight_decay</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>effective_batch_size</td><td>32</td></tr><tr><td>eval/loss</td><td>0.34353</td></tr><tr><td>eval/mean_token_accuracy</td><td>0.90365</td></tr><tr><td>eval/num_tokens</td><td>4979548.0</td></tr><tr><td>eval/runtime</td><td>101.1352</td></tr><tr><td>eval/samples_per_second</td><td>24.719</td></tr><tr><td>eval/steps_per_second</td><td>3.095</td></tr><tr><td>eval_loss</td><td>0.34353</td></tr><tr><td>eval_perplexity</td><td>1.40992</td></tr><tr><td>learning_rate</td><td>0.0001</td></tr><tr><td>optimizer</td><td>nadam</td></tr><tr><td>step</td><td>3040</td></tr><tr><td>total_flos</td><td>5.896681954423603e+16</td></tr><tr><td>train/epoch</td><td>0.99971</td></tr><tr><td>train/global_step</td><td>3046</td></tr><tr><td>train/grad_norm</td><td>0.66383</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.3402</td></tr><tr><td>train/mean_token_accuracy</td><td>0.90523</td></tr><tr><td>train/num_tokens</td><td>5021970.0</td></tr><tr><td>train_loss</td><td>0.09077</td></tr><tr><td>train_runtime</td><td>5442.8192</td></tr><tr><td>train_samples_per_second</td><td>17.914</td></tr><tr><td>train_steps_per_second</td><td>0.56</td></tr><tr><td>warmup_ratio</td><td>0.05</td></tr><tr><td>weight_decay</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sft-final-model-2025-08-19_08-33-30</strong> at: <a href='https://wandb.ai/olialeshka-none/text-to-sql/runs/imoh6jtd' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql/runs/imoh6jtd</a><br> View project at: <a href='https://wandb.ai/olialeshka-none/text-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250820_072235-imoh6jtd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "# resuming the prev run\n",
    "timestamp = '2025-08-19_08-33-30'\n",
    "RUN_NAME = f'sft-final-model-{timestamp}'\n",
    "run_id = 'imoh6jtd'\n",
    "wandb.init(\n",
    "    project=os.environ[\"WANDB_PROJECT\"],\n",
    "    name=RUN_NAME,\n",
    "    id=run_id,         # resume previous run if available\n",
    "    resume=\"allow\",    # allows resuming crashed run\n",
    ")\n",
    "\n",
    "\n",
    "RESUME_TRAINING = True\n",
    "OUTPUT_DIR = \"./sft-final_model-output\"\n",
    "PER_DEVICE_BATCH_SIZE = 2  # higher values --> OOM\n",
    "\n",
    "optimizer = 'nadam'\n",
    "effective_batch_size = 32\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 0.0\n",
    "betas = (0.95, 0.999)\n",
    "warmup_ratio = 0.05\n",
    "epochs = 1\n",
    "gradient_accumulation_steps = int(effective_batch_size / PER_DEVICE_BATCH_SIZE)\n",
    "\n",
    "optimizer_map = {\n",
    "    \"adam\": torch.optim.Adam,\n",
    "    \"adamw\": torch.optim.AdamW,\n",
    "    \"nadam\": torch.optim.NAdam,\n",
    "    \"adamax\": torch.optim.Adamax\n",
    "}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    num_train_epochs=epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=gradient_accumulation_steps*5,\n",
    "    save_total_limit=2,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=gradient_accumulation_steps*5,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=gradient_accumulation_steps*5,\n",
    "    report_to=['wandb'],\n",
    "    run_name=RUN_NAME,\n",
    "    fp16=True,\n",
    "    fp16_full_eval=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    max_grad_norm=1,\n",
    "    load_best_model_at_end=True,\n",
    "    gradient_checkpointing=True\n",
    ")\n",
    "\n",
    "def build_optimizer(model):\n",
    "    optimizer_class = optimizer_map[optimizer]\n",
    "    return optimizer_class(\n",
    "        model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        betas=betas\n",
    "    )\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=ds_train_with_assistant_content,\n",
    "    eval_dataset=ds_valid_with_assistant_content,\n",
    "    formatting_func=formatting_func,\n",
    "    args=training_args,\n",
    "    optimizers=(build_optimizer(model), None),  # (optimizer, scheduler)\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=25)]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Training setup summary\n",
    "dataset_size = len(ds_train_with_assistant_content)\n",
    "steps_per_epoch = dataset_size // (PER_DEVICE_BATCH_SIZE * gradient_accumulation_steps)\n",
    "total_steps = steps_per_epoch * epochs\n",
    "warmup_steps = int(total_steps * warmup_ratio)\n",
    "\n",
    "print(\"===== Training Setup Summary =====\")\n",
    "print(f\"Num epochs:            {epochs}\")\n",
    "print(f\"Effective batch size:  {effective_batch_size}\")\n",
    "print(f\"Per-device batch size: {PER_DEVICE_BATCH_SIZE}\")\n",
    "print(f\"Gradient accumulation: {gradient_accumulation_steps}\")\n",
    "print(f\"Dataset size:          {dataset_size}\")\n",
    "print(f\"Steps per epoch:       {steps_per_epoch}\")\n",
    "print(f\"Total training steps:  {total_steps}\")\n",
    "print(f\"Warmup steps:          {warmup_steps}\")\n",
    "print(f\"Logging steps:         {training_args.logging_steps}\")\n",
    "print(\"===================================\")\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\")\n",
    "\n",
    "\n",
    "# Training\n",
    "last_checkpoint = None\n",
    "if RESUME_TRAINING and os.path.isdir(OUTPUT_DIR):\n",
    "    last_checkpoint = get_last_checkpoint(OUTPUT_DIR)\n",
    "\n",
    "if last_checkpoint is not None:\n",
    "    print(f\"Resuming training from checkpoint: {last_checkpoint}\")\n",
    "    trainer.train(resume_from_checkpoint=last_checkpoint)\n",
    "else:\n",
    "    print(\"Starting fresh training run\")\n",
    "    trainer.train()\n",
    "\n",
    "print(f\"End time: {datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\")\n",
    "\n",
    "\n",
    "# WandB logging of eval metrics\n",
    "for log in trainer.state.log_history:\n",
    "    if 'eval_loss' in log:\n",
    "        wandb.log({\n",
    "            \"eval_loss\": log['eval_loss'],\n",
    "            \"eval_perplexity\": math.exp(log['eval_loss']),\n",
    "            \"step\": log['step'],\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"betas\": betas,\n",
    "            \"warmup_ratio\": warmup_ratio,\n",
    "            \"effective_batch_size\": effective_batch_size,\n",
    "            \"optimizer\": optimizer\n",
    "        })\n",
    "\n",
    "wandb.finish()  # finish the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90a85071-939a-4139-a48e-acaf10eeb482",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(OUTPUT_DIR, 'final')\n",
    "trainer.save_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3646cbc6-22fe-425d-944b-becf67217666",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79e4fc47-001c-40a5-a6fb-edc174736baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU 0 load: 1.00\n",
      "CPU 1 load: 2.00\n",
      "CPU 2 load: 1.00\n",
      "CPU 3 load: 0.00\n",
      "RAM Total: 27.41 GB, Used: 1.40 GB\n",
      "GPU 0 (Tesla T4) load: 0.0%\n",
      "GPU 0 (Tesla T4) VRAM Total: 16384.0 MB, Used 2405.0 MB\n",
      "Disk Total: 60.95 GB, Used: 48.28 GB\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "get_vm_usage_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f673e50-7a86-4f6d-8a90-ca295c05019b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "    (rotary_emb): Qwen3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = './sft-final_model-output/final'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39f13328-2275-416b-b75e-3252b39f6f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 2025-08-20_19-03-53\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e7819b71fc145f7bef6b68acd12d0a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End time: 2025-08-20_19-50-26\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "ENABLE_THINKING = False\n",
    "MAX_NEW_TOKENS = 512\n",
    "\n",
    "\n",
    "prompts = [ds_test[id]['sql_prompt'] for id in range(len(ds_test))]\n",
    "contexts = [ds_test[id]['sql_context'] for id in range(len(ds_test))]\n",
    "\n",
    "responses = []\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\")\n",
    "for i in tqdm(range(0, len(prompts), BATCH_SIZE)):\n",
    "    batch_prompts = prompts[i : i + BATCH_SIZE]\n",
    "    batch_contexts = contexts[i : i + BATCH_SIZE]\n",
    "\n",
    "    messages_list = [\n",
    "        construct_message(prompt=p, context=c)\n",
    "        for p, c in zip(batch_prompts, batch_contexts)\n",
    "    ]\n",
    "\n",
    "    batch_responses = generate_model_response_batch(model, tokenizer, messages_list, enable_thinking=ENABLE_THINKING, max_new_tokens=MAX_NEW_TOKENS)\n",
    "\n",
    "    responses.extend(batch_responses)\n",
    "\n",
    "print(f\"End time: {datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd50f297-d73c-4fb2-81a4-3952da67e019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea7db5c5ca944ef58643701a1113521d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5851 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean test set score: 0.793\n"
     ]
    }
   ],
   "source": [
    "references = [ds_test[id]['sql'] for id in range(len(ds_test))]\n",
    "predictions = [responses[id]['content'] for id in range(len(ds_test))]\n",
    "\n",
    "scores = [\n",
    "    evaluate_sql_response(\n",
    "        reference=reference,\n",
    "        prediction=prediction,\n",
    "        sql_context=context\n",
    "    )\n",
    "    for reference, prediction, context in tqdm(zip(references, predictions, contexts), total=len(ds_test))\n",
    "]\n",
    "\n",
    "print(f\"Mean test set score: {np.mean([score['final_score'] for score in scores]):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92efc8d8-1149-47b4-8dfa-3c068af3f82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sft_test_predictions.pkl', 'wb') as f:\n",
    "    pickle.dump(predictions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd4640f-efc0-42e2-8854-56bb9f61c906",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python t4",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
