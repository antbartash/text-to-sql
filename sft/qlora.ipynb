{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e84451c-1ae2-4446-a896-1ab971c9612e",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81de34e8-a5c3-4638-af77-78c396cea599",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import datasets\n",
    "import evaluate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, BitsAndBytesConfig, EarlyStoppingCallback\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from accelerate import cpu_offload\n",
    "import sqlite3\n",
    "import sqlparse\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import wandb\n",
    "import psutil\n",
    "import GPUtil\n",
    "import os\n",
    "import gc\n",
    "import math\n",
    "\n",
    "import _config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fdad24c-9f36-4778-8ff8-54608e5883fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENABLE_THINKING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "182dcb56-ff15-4e23-8b42-e75aeac4cf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = _config.WANDB_API_KEY\n",
    "os.environ[\"WANDB_PROJECT\"] = _config.WANDB_PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5339c1d6-98a8-49c1-a69c-0b83820dc9be",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc87a80a-cae5-4435-bce2-8524c51e9cd3",
   "metadata": {},
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e2b2599-ab5a-4542-9e96-e4315eddc80b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "CPU 0 load: 0.00\n",
      "CPU 1 load: 2.00\n",
      "CPU 2 load: 0.00\n",
      "CPU 3 load: 0.00\n",
      "RAM Total: 27.41 GB, Used: 1.48 GB\n",
      "GPU 0 (Tesla T4) load: 0.0%\n",
      "GPU 0 (Tesla T4) VRAM Total: 16384.0 MB, Used 3.0 MB\n",
      "Disk Total: 60.95 GB, Used: 41.14 GB\n"
     ]
    }
   ],
   "source": [
    "def get_vm_usage_metrics():\n",
    "    # CPU usage\n",
    "    cpu_load = psutil.cpu_percent(interval=1, percpu=True)\n",
    "    for id, load in enumerate(cpu_load):\n",
    "        print(f\"CPU {id} load: {load:.2f}\")\n",
    "    # RAM usage\n",
    "    ram = psutil.virtual_memory()\n",
    "    print(f\"RAM Total: {ram.total/(1024**3):.2f} GB, Used: {(ram.used)/(1024**3):.2f} GB\")\n",
    "    # GPU\n",
    "    if torch.cuda.is_available():\n",
    "        gpus = GPUtil.getGPUs()\n",
    "        for gpu in gpus:\n",
    "            print(f\"GPU {gpu.id} ({gpu.name}) load: {gpu.load*100}%\")\n",
    "            print(f\"GPU {gpu.id} ({gpu.name}) VRAM Total: {gpu.memoryTotal} MB, Used {gpu.memoryUsed} MB\")\n",
    "    # Disk \n",
    "    disk = psutil.disk_usage('/')\n",
    "    print(f\"Disk Total: {disk.total/(1024**3):.2f} GB, Used: {(disk.used)/(1024**3):.2f} GB\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f'Device: {device}')\n",
    "get_vm_usage_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41c61735-9bfc-48c5-9b8c-c9b7941dcf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"Trainable params: {trainable_params} || All params: {all_param} || Trainable %: {100 * trainable_params / all_param:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6faa234-4650-4873-a52f-b79de6d70d6d",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0106aedd-3c47-4705-b3b9-3379107df6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_message(prompt, context):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": f\"The user asks a question. Your task is to generate the SQL query to answer that question. Return SQL query only. The context of the question is the following: '{context}'\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "912ded5e-a7cb-4a4f-9fce-5254e8dee39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_response_batch(model, tokenizer, messages_list, enable_thinking=True, max_new_tokens=512):\n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=enable_thinking\n",
    "        )\n",
    "        for messages in messages_list\n",
    "    ]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        padding_side='left'\n",
    "    ).to(model.device)\n",
    "\n",
    "    model.eval()\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=max_new_tokens\n",
    "    )\n",
    "\n",
    "    responses = []\n",
    "    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids):\n",
    "        # Slice to get only generated part\n",
    "        output_only_ids = output_ids[len(input_ids):].tolist()\n",
    "\n",
    "        # Try to find `</think>` (id 151668)\n",
    "        try:\n",
    "            index = len(output_only_ids) - output_only_ids[::-1].index(151668)\n",
    "        except ValueError:\n",
    "            index = 0\n",
    "\n",
    "        if enable_thinking:\n",
    "            thinking_content = tokenizer.decode(\n",
    "                output_only_ids[:index],\n",
    "                skip_special_tokens=True\n",
    "            ).strip(\"\\n\")\n",
    "            content = tokenizer.decode(\n",
    "                output_only_ids[index:],\n",
    "                skip_special_tokens=True\n",
    "            ).strip(\"\\n\")\n",
    "        else:\n",
    "            thinking_content = None\n",
    "            content = tokenizer.decode(\n",
    "                output_only_ids,\n",
    "                skip_special_tokens=True\n",
    "            ).strip(\"\\n\")\n",
    "\n",
    "        responses.append({\n",
    "            'thinking_content': thinking_content,\n",
    "            'content': content\n",
    "        })\n",
    "\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd975079-9037-4745-a8d4-d9d828ee2257",
   "metadata": {},
   "source": [
    "## Formatting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7aa8c985-2ebd-4f95-a5d3-a804abcce942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for training\n",
    "def construct_message_with_assistant_content(example):\n",
    "    messages = construct_message(example['sql_prompt'], example['sql_context'])\n",
    "    messages.append({\n",
    "        'role': 'assistant',\n",
    "        'content': example['sql']\n",
    "    })\n",
    "    return {'messages': messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21145d07-3501-44ff-acaa-ca323091539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(example, enable_thinking=ENABLE_THINKING):\n",
    "    return tokenizer.apply_chat_template(\n",
    "        example[\"messages\"],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False, # no generation prompt during training\n",
    "        enable_thinking=ENABLE_THINKING \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1316aa-2e36-443d-a411-3dd73793ff17",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4090520-bbc9-4624-8d43-fb69f06809b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def normalize_sql(sql):\n",
    "    return sqlparse.format(sql, reindent=True, keyword_case='upper').strip()\n",
    "\n",
    "def compute_rouge(reference, prediction):\n",
    "    result = rouge.compute(predictions=[prediction], references=[reference])\n",
    "    return result['rougeL']\n",
    "\n",
    "def evaluate_sql_response(reference, prediction, sql_context):\n",
    "    # ROUGE-L\n",
    "    rouge_score = compute_rouge(reference, prediction)\n",
    "    \n",
    "    # execution check\n",
    "    try:\n",
    "        conn = sqlite3.connect(\":memory:\")\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.executescript(sql_context)\n",
    "        cursor.execute(reference)\n",
    "        ref_result = cursor.fetchall()\n",
    "        \n",
    "        cursor.execute(prediction)\n",
    "        model_result = cursor.fetchall()\n",
    "        \n",
    "        execution_match = ref_result == model_result\n",
    "    except Exception:\n",
    "        execution_match = False\n",
    "    finally:\n",
    "        conn.close()\n",
    "    \n",
    "    # final score\n",
    "    if execution_match:\n",
    "        final_score = 1.0\n",
    "    else:\n",
    "        final_score = 0.7 * rouge_score\n",
    "\n",
    "    return {\n",
    "        \"rougeL\": round(rouge_score, 4),\n",
    "        \"execution_match\": execution_match,\n",
    "        \"final_score\": final_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86eba12-be22-4ed3-94db-6a9d0839d039",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cce15154-ce9c-42ea-aad3-6cb9a75ce806",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'domain', 'domain_description', 'sql_complexity', 'sql_complexity_description', 'sql_task_type', 'sql_task_type_description', 'sql_prompt', 'sql_context', 'sql', 'sql_explanation'],\n",
       "    num_rows: 97500\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = datasets.load_dataset('gretelai/synthetic_text_to_sql', streaming=False)\n",
    "ds_train, ds_test = ds['train'], ds['test']\n",
    "\n",
    "split = ds_train.train_test_split(test_size=0.025, seed=42)\n",
    "ds_train = split['train']\n",
    "ds_valid = split['test']\n",
    "\n",
    "ds_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3349a5-e952-4115-9e64-db1ffa74b935",
   "metadata": {},
   "source": [
    "# QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec775d7-78d9-4c90-9324-ab3f0ab77992",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97500 2500 5851\n",
      "CPU 0 load: 0.00\n",
      "CPU 1 load: 0.00\n",
      "CPU 2 load: 0.00\n",
      "CPU 3 load: 0.00\n",
      "RAM Total: 27.41 GB, Used: 1.49 GB\n",
      "GPU 0 (Tesla T4) load: 0.0%\n",
      "GPU 0 (Tesla T4) VRAM Total: 16384.0 MB, Used 3.0 MB\n",
      "Disk Total: 60.95 GB, Used: 40.97 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33molialeshka\u001b[0m (\u001b[33molialeshka-none\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/olialeshka_1/wandb/run-20251023_124800-cj9mbl11</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/olialeshka-none/text-to-sql/runs/cj9mbl11' target=\"_blank\">qlora-final-model-all-linear-r64-2025-10-23_05-59-42</a></strong> to <a href='https://wandb.ai/olialeshka-none/text-to-sql' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/olialeshka-none/text-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/olialeshka-none/text-to-sql/runs/cj9mbl11' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql/runs/cj9mbl11</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 40370176 || All params: 416219136 || Trainable %: 9.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Training Setup Summary =====\n",
      "Num epochs:            1\n",
      "Effective batch size:  16\n",
      "Per-device batch size: 2\n",
      "Gradient accumulation: 8\n",
      "Dataset size:          97500\n",
      "Steps per epoch:       6093\n",
      "Total training steps:  6093\n",
      "Warmup steps:          1218\n",
      "Logging steps:         40\n",
      "===================================\n",
      "Start time: 2025-10-23_12-48-13\n",
      "Resuming training from checkpoint: ./qlora-final_model_all_linear_r64-output/checkpoint-2680\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4232' max='6094' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4232/6094 3:24:04 < 4:05:08, 0.13 it/s, Epoch 0.69/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2720</td>\n",
       "      <td>0.426700</td>\n",
       "      <td>0.432508</td>\n",
       "      <td>0.426978</td>\n",
       "      <td>123655.000000</td>\n",
       "      <td>0.884865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2760</td>\n",
       "      <td>0.434500</td>\n",
       "      <td>0.431839</td>\n",
       "      <td>0.438531</td>\n",
       "      <td>248878.000000</td>\n",
       "      <td>0.884937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.421700</td>\n",
       "      <td>0.431473</td>\n",
       "      <td>0.430893</td>\n",
       "      <td>376696.000000</td>\n",
       "      <td>0.884978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2840</td>\n",
       "      <td>0.422700</td>\n",
       "      <td>0.431305</td>\n",
       "      <td>0.426440</td>\n",
       "      <td>502510.000000</td>\n",
       "      <td>0.884960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2880</td>\n",
       "      <td>0.426200</td>\n",
       "      <td>0.431643</td>\n",
       "      <td>0.428791</td>\n",
       "      <td>625112.000000</td>\n",
       "      <td>0.884645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2920</td>\n",
       "      <td>0.429000</td>\n",
       "      <td>0.430185</td>\n",
       "      <td>0.425335</td>\n",
       "      <td>751874.000000</td>\n",
       "      <td>0.884840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2960</td>\n",
       "      <td>0.422700</td>\n",
       "      <td>0.429823</td>\n",
       "      <td>0.430036</td>\n",
       "      <td>878979.000000</td>\n",
       "      <td>0.885062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.426500</td>\n",
       "      <td>0.429208</td>\n",
       "      <td>0.429833</td>\n",
       "      <td>1001192.000000</td>\n",
       "      <td>0.885091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3040</td>\n",
       "      <td>0.428500</td>\n",
       "      <td>0.428892</td>\n",
       "      <td>0.429349</td>\n",
       "      <td>1126073.000000</td>\n",
       "      <td>0.884924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3080</td>\n",
       "      <td>0.424900</td>\n",
       "      <td>0.428403</td>\n",
       "      <td>0.432246</td>\n",
       "      <td>1244931.000000</td>\n",
       "      <td>0.885061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3120</td>\n",
       "      <td>0.432200</td>\n",
       "      <td>0.427621</td>\n",
       "      <td>0.428065</td>\n",
       "      <td>1368057.000000</td>\n",
       "      <td>0.885403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3160</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.427670</td>\n",
       "      <td>0.427186</td>\n",
       "      <td>1496671.000000</td>\n",
       "      <td>0.885205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.420200</td>\n",
       "      <td>0.426931</td>\n",
       "      <td>0.428055</td>\n",
       "      <td>1618627.000000</td>\n",
       "      <td>0.885409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3240</td>\n",
       "      <td>0.432800</td>\n",
       "      <td>0.426369</td>\n",
       "      <td>0.424323</td>\n",
       "      <td>1742942.000000</td>\n",
       "      <td>0.885593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3280</td>\n",
       "      <td>0.423400</td>\n",
       "      <td>0.425638</td>\n",
       "      <td>0.429026</td>\n",
       "      <td>1868601.000000</td>\n",
       "      <td>0.885893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3320</td>\n",
       "      <td>0.420700</td>\n",
       "      <td>0.425233</td>\n",
       "      <td>0.428113</td>\n",
       "      <td>1993570.000000</td>\n",
       "      <td>0.885928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3360</td>\n",
       "      <td>0.425600</td>\n",
       "      <td>0.425046</td>\n",
       "      <td>0.421715</td>\n",
       "      <td>2119478.000000</td>\n",
       "      <td>0.885872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.421600</td>\n",
       "      <td>0.424384</td>\n",
       "      <td>0.425113</td>\n",
       "      <td>2246082.000000</td>\n",
       "      <td>0.886059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3440</td>\n",
       "      <td>0.417500</td>\n",
       "      <td>0.424092</td>\n",
       "      <td>0.419818</td>\n",
       "      <td>2371359.000000</td>\n",
       "      <td>0.886305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3480</td>\n",
       "      <td>0.423300</td>\n",
       "      <td>0.424100</td>\n",
       "      <td>0.422295</td>\n",
       "      <td>2495705.000000</td>\n",
       "      <td>0.885940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3520</td>\n",
       "      <td>0.421000</td>\n",
       "      <td>0.423182</td>\n",
       "      <td>0.424509</td>\n",
       "      <td>2620962.000000</td>\n",
       "      <td>0.886318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3560</td>\n",
       "      <td>0.422200</td>\n",
       "      <td>0.422803</td>\n",
       "      <td>0.425238</td>\n",
       "      <td>2748491.000000</td>\n",
       "      <td>0.886428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.421900</td>\n",
       "      <td>0.422694</td>\n",
       "      <td>0.421558</td>\n",
       "      <td>2871633.000000</td>\n",
       "      <td>0.886557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3640</td>\n",
       "      <td>0.424500</td>\n",
       "      <td>0.421834</td>\n",
       "      <td>0.424765</td>\n",
       "      <td>2994490.000000</td>\n",
       "      <td>0.886674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3680</td>\n",
       "      <td>0.421300</td>\n",
       "      <td>0.421500</td>\n",
       "      <td>0.421854</td>\n",
       "      <td>3117935.000000</td>\n",
       "      <td>0.886751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3720</td>\n",
       "      <td>0.417200</td>\n",
       "      <td>0.421052</td>\n",
       "      <td>0.417258</td>\n",
       "      <td>3243309.000000</td>\n",
       "      <td>0.886889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3760</td>\n",
       "      <td>0.425700</td>\n",
       "      <td>0.420792</td>\n",
       "      <td>0.423364</td>\n",
       "      <td>3365020.000000</td>\n",
       "      <td>0.887124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.412300</td>\n",
       "      <td>0.420962</td>\n",
       "      <td>0.413776</td>\n",
       "      <td>3490250.000000</td>\n",
       "      <td>0.886916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3840</td>\n",
       "      <td>0.408900</td>\n",
       "      <td>0.420344</td>\n",
       "      <td>0.413174</td>\n",
       "      <td>3616715.000000</td>\n",
       "      <td>0.887353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3880</td>\n",
       "      <td>0.415500</td>\n",
       "      <td>0.419968</td>\n",
       "      <td>0.420378</td>\n",
       "      <td>3740811.000000</td>\n",
       "      <td>0.887297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3920</td>\n",
       "      <td>0.416000</td>\n",
       "      <td>0.419504</td>\n",
       "      <td>0.419170</td>\n",
       "      <td>3865184.000000</td>\n",
       "      <td>0.887258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3960</td>\n",
       "      <td>0.414400</td>\n",
       "      <td>0.419130</td>\n",
       "      <td>0.412498</td>\n",
       "      <td>3988474.000000</td>\n",
       "      <td>0.887509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.401500</td>\n",
       "      <td>0.419023</td>\n",
       "      <td>0.419003</td>\n",
       "      <td>4114858.000000</td>\n",
       "      <td>0.887456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4040</td>\n",
       "      <td>0.421100</td>\n",
       "      <td>0.418464</td>\n",
       "      <td>0.419726</td>\n",
       "      <td>4239777.000000</td>\n",
       "      <td>0.887585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4080</td>\n",
       "      <td>0.420100</td>\n",
       "      <td>0.418716</td>\n",
       "      <td>0.420185</td>\n",
       "      <td>4363493.000000</td>\n",
       "      <td>0.887388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4120</td>\n",
       "      <td>0.412400</td>\n",
       "      <td>0.418519</td>\n",
       "      <td>0.415981</td>\n",
       "      <td>4486222.000000</td>\n",
       "      <td>0.887483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4160</td>\n",
       "      <td>0.417700</td>\n",
       "      <td>0.418186</td>\n",
       "      <td>0.419831</td>\n",
       "      <td>4610929.000000</td>\n",
       "      <td>0.887573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.411400</td>\n",
       "      <td>0.417912</td>\n",
       "      <td>0.409899</td>\n",
       "      <td>4737088.000000</td>\n",
       "      <td>0.887821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(ds_train), len(ds_valid), len(ds_test))\n",
    "\n",
    "ds_train_with_assistant_content = ds_train.map(construct_message_with_assistant_content)\n",
    "ds_valid_with_assistant_content = ds_valid.map(construct_message_with_assistant_content)\n",
    "\n",
    "get_vm_usage_metrics()\n",
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "checkpoint = \"Qwen/Qwen3-0.6B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    checkpoint,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "# resuming the prev run\n",
    "timestamp = '2025-10-23_05-59-42'\n",
    "RUN_NAME = f'qlora-final-model-all-linear-r64-{timestamp}'\n",
    "run_id = 'cj9mbl11'\n",
    "wandb.init(\n",
    "    project=os.environ[\"WANDB_PROJECT\"],\n",
    "    name=RUN_NAME,\n",
    "    id=run_id,         # resume previous run if available\n",
    "    resume=\"allow\",    # allows resuming crashed run\n",
    ")\n",
    "\n",
    "\n",
    "RESUME_TRAINING = True\n",
    "OUTPUT_DIR = \"./qlora-final_model_all_linear_r64-output\"\n",
    "PER_DEVICE_BATCH_SIZE = 2  # higher values --> OOM\n",
    "\n",
    "optimizer = 'paged_adamw_8bit'#'nadam'\n",
    "effective_batch_size = 16\n",
    "learning_rate = 1e-5\n",
    "weight_decay = 0.0\n",
    "betas = (0.9, 0.9999)\n",
    "warmup_ratio = 0.2\n",
    "epochs = 1\n",
    "gradient_accumulation_steps = int(effective_batch_size / PER_DEVICE_BATCH_SIZE)\n",
    "lora_r = 16*4\n",
    "lora_alpha = 64*4\n",
    "lora_dropout = 0.01\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    optim=optimizer,   # better for 4-bit models\n",
    "    num_train_epochs=epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=gradient_accumulation_steps*5,\n",
    "    save_total_limit=2,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=gradient_accumulation_steps*5,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=gradient_accumulation_steps*5,\n",
    "    report_to=['wandb'],\n",
    "    run_name=RUN_NAME,\n",
    "    # bf16=True,\n",
    "    fp16=True,\n",
    "    # fp16_full_eval=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    max_grad_norm=1,\n",
    "    load_best_model_at_end=True,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False}\n",
    ")\n",
    "\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules='all-linear'\n",
    ")\n",
    "# model.requires_grad_(False)                     # freeze base weights (precautionary)\n",
    "model_peft = get_peft_model(model, peft_config) # inject a LoRA adapter\n",
    "print_trainable_parameters(model_peft)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model_peft,\n",
    "    train_dataset=ds_train_with_assistant_content,\n",
    "    eval_dataset=ds_valid_with_assistant_content,\n",
    "    formatting_func=formatting_func,\n",
    "    args=training_args,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=25)]\n",
    ")\n",
    "\n",
    "\n",
    "# Training setup summary\n",
    "dataset_size = len(ds_train_with_assistant_content)\n",
    "steps_per_epoch = dataset_size // (PER_DEVICE_BATCH_SIZE * gradient_accumulation_steps)\n",
    "total_steps = steps_per_epoch * epochs\n",
    "warmup_steps = int(total_steps * warmup_ratio)\n",
    "\n",
    "print(\"===== Training Setup Summary =====\")\n",
    "print(f\"Num epochs:            {epochs}\")\n",
    "print(f\"Effective batch size:  {effective_batch_size}\")\n",
    "print(f\"Per-device batch size: {PER_DEVICE_BATCH_SIZE}\")\n",
    "print(f\"Gradient accumulation: {gradient_accumulation_steps}\")\n",
    "print(f\"Dataset size:          {dataset_size}\")\n",
    "print(f\"Steps per epoch:       {steps_per_epoch}\")\n",
    "print(f\"Total training steps:  {total_steps}\")\n",
    "print(f\"Warmup steps:          {warmup_steps}\")\n",
    "print(f\"Logging steps:         {training_args.logging_steps}\")\n",
    "print(\"===================================\")\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\")\n",
    "\n",
    "\n",
    "# Training\n",
    "last_checkpoint = None\n",
    "if RESUME_TRAINING and os.path.isdir(OUTPUT_DIR):\n",
    "    last_checkpoint = get_last_checkpoint(OUTPUT_DIR)\n",
    "\n",
    "if last_checkpoint is not None:\n",
    "    print(f\"Resuming training from checkpoint: {last_checkpoint}\")\n",
    "    trainer.train(resume_from_checkpoint=last_checkpoint)\n",
    "else:\n",
    "    print(\"Starting fresh training run\")\n",
    "    trainer.train()\n",
    "\n",
    "print(f\"End time: {datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\")\n",
    "\n",
    "\n",
    "# WandB logging of eval metrics\n",
    "for log in trainer.state.log_history:\n",
    "    if 'eval_loss' in log:\n",
    "        wandb.log({\n",
    "            \"eval_loss\": log['eval_loss'],\n",
    "            \"eval_perplexity\": math.exp(log['eval_loss']),\n",
    "            \"step\": log['step'],\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"betas\": betas,\n",
    "            \"warmup_ratio\": warmup_ratio,\n",
    "            \"effective_batch_size\": effective_batch_size,\n",
    "            \"optimizer\": optimizer\n",
    "        })\n",
    "\n",
    "wandb.finish()  # finish the run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ffe445a-220e-40b8-92e5-820640ba1e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(OUTPUT_DIR, 'final')\n",
    "trainer.save_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdbb6b9-7fe1-4193-b553-ab92ebe46174",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "300719a4-93f1-4131-a6b5-68d393e125dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU 0 load: 0.00\n",
      "CPU 1 load: 1.00\n",
      "CPU 2 load: 1.00\n",
      "CPU 3 load: 0.00\n",
      "RAM Total: 27.41 GB, Used: 1.51 GB\n",
      "GPU 0 (Tesla T4) load: 0.0%\n",
      "GPU 0 (Tesla T4) VRAM Total: 16384.0 MB, Used 3.0 MB\n",
      "Disk Total: 60.95 GB, Used: 41.14 GB\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "get_vm_usage_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc26f5a6-a630-4235-bdce-ad186ec7638e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 0 || All params: 636420096 || Trainable %: 0.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1024, out_features=64, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=64, out_features=2048, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (k_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1024, out_features=64, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (v_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1024, out_features=64, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (o_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1024, out_features=64, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=64, out_features=3072, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (up_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1024, out_features=64, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=64, out_features=3072, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (down_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=3072, out_features=64, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "    (rotary_emb): Qwen3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = './qlora-final_model_all_linear_r64-output/final'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(device)\n",
    "\n",
    "print_trainable_parameters(model)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6ea4a7-10b2-43f3-9ce5-5e228cb26f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 2025-10-24_10-46-59\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b0ec049c4346689958645d45bcf609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "ENABLE_THINKING = False\n",
    "MAX_NEW_TOKENS = 512\n",
    "\n",
    "\n",
    "prompts = [ds_test[id]['sql_prompt'] for id in range(len(ds_test))]\n",
    "contexts = [ds_test[id]['sql_context'] for id in range(len(ds_test))]\n",
    "\n",
    "responses = []\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\")\n",
    "for i in tqdm(range(0, len(prompts), BATCH_SIZE)):\n",
    "    batch_prompts = prompts[i : i + BATCH_SIZE]\n",
    "    batch_contexts = contexts[i : i + BATCH_SIZE]\n",
    "\n",
    "    messages_list = [\n",
    "        construct_message(prompt=p, context=c)\n",
    "        for p, c in zip(batch_prompts, batch_contexts)\n",
    "    ]\n",
    "\n",
    "    batch_responses = generate_model_response_batch(model, tokenizer, messages_list, enable_thinking=ENABLE_THINKING, max_new_tokens=MAX_NEW_TOKENS)\n",
    "\n",
    "    responses.extend(batch_responses)\n",
    "\n",
    "print(f\"End time: {datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1d4b76-ee09-474c-85a9-ed7423ec0b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "references = [ds_test[id]['sql'] for id in range(len(ds_test))]\n",
    "predictions = [responses[id]['content'] for id in range(len(ds_test))]\n",
    "\n",
    "scores = [\n",
    "    evaluate_sql_response(\n",
    "        reference=reference,\n",
    "        prediction=prediction,\n",
    "        sql_context=context\n",
    "    )\n",
    "    for reference, prediction, context in tqdm(zip(references, predictions, contexts), total=len(ds_test))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72695579-53a6-4566-a258-9ebe46317369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean test set score: 0.755\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean test set score: {np.mean([score['final_score'] for score in scores]):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python T4",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
