{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e84451c-1ae2-4446-a896-1ab971c9612e",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81de34e8-a5c3-4638-af77-78c396cea599",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import datasets\n",
    "import evaluate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, BitsAndBytesConfig, EarlyStoppingCallback\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from accelerate import cpu_offload\n",
    "import sqlite3\n",
    "import sqlparse\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import wandb\n",
    "import psutil\n",
    "import GPUtil\n",
    "import os\n",
    "import gc\n",
    "import math\n",
    "\n",
    "import _config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fdad24c-9f36-4778-8ff8-54608e5883fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENABLE_THINKING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "182dcb56-ff15-4e23-8b42-e75aeac4cf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = _config.WANDB_API_KEY\n",
    "os.environ[\"WANDB_PROJECT\"] = _config.WANDB_PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5339c1d6-98a8-49c1-a69c-0b83820dc9be",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e2b2599-ab5a-4542-9e96-e4315eddc80b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "CPU 0 load: 0.00\n",
      "CPU 1 load: 0.00\n",
      "CPU 2 load: 2.00\n",
      "CPU 3 load: 0.00\n",
      "RAM Total: 27.41 GB, Used: 1.62 GB\n",
      "GPU 0 (Tesla T4) load: 0.0%\n",
      "GPU 0 (Tesla T4) VRAM Total: 16384.0 MB, Used 3.0 MB\n",
      "Disk Total: 60.95 GB, Used: 38.36 GB\n"
     ]
    }
   ],
   "source": [
    "def get_vm_usage_metrics():\n",
    "    # CPU usage\n",
    "    cpu_load = psutil.cpu_percent(interval=1, percpu=True)\n",
    "    for id, load in enumerate(cpu_load):\n",
    "        print(f\"CPU {id} load: {load:.2f}\")\n",
    "    # RAM usage\n",
    "    ram = psutil.virtual_memory()\n",
    "    print(f\"RAM Total: {ram.total/(1024**3):.2f} GB, Used: {(ram.used)/(1024**3):.2f} GB\")\n",
    "    # GPU\n",
    "    if torch.cuda.is_available():\n",
    "        gpus = GPUtil.getGPUs()\n",
    "        for gpu in gpus:\n",
    "            print(f\"GPU {gpu.id} ({gpu.name}) load: {gpu.load*100}%\")\n",
    "            print(f\"GPU {gpu.id} ({gpu.name}) VRAM Total: {gpu.memoryTotal} MB, Used {gpu.memoryUsed} MB\")\n",
    "    # Disk \n",
    "    disk = psutil.disk_usage('/')\n",
    "    print(f\"Disk Total: {disk.total/(1024**3):.2f} GB, Used: {(disk.used)/(1024**3):.2f} GB\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f'Device: {device}')\n",
    "get_vm_usage_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41c61735-9bfc-48c5-9b8c-c9b7941dcf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"Trainable params: {trainable_params} || All params: {all_param} || Trainable %: {100 * trainable_params / all_param:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c64ffa-c729-43f4-b32f-6be96e4cfe0c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cce15154-ce9c-42ea-aad3-6cb9a75ce806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'domain', 'domain_description', 'sql_complexity', 'sql_complexity_description', 'sql_task_type', 'sql_task_type_description', 'sql_prompt', 'sql_context', 'sql', 'sql_explanation'],\n",
       "    num_rows: 97500\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = datasets.load_dataset('gretelai/synthetic_text_to_sql', streaming=False)\n",
    "ds_train, ds_test = ds['train'], ds['test']\n",
    "\n",
    "split = ds_train.train_test_split(test_size=0.025, seed=42)\n",
    "ds_train = split['train']\n",
    "ds_valid = split['test']\n",
    "\n",
    "ds_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc899f6-b89f-455d-8033-f3b39ebc1886",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8c8ee0-69cf-4c00-a542-7e4479c03431",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"Qwen/Qwen3-0.6B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    checkpoint,\n",
    "    device_map=\"cuda\",\n",
    ")\n",
    "# model = cpu_offload(model)\n",
    "\n",
    "print_trainable_parameters(model)\n",
    "get_vm_usage_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0106aedd-3c47-4705-b3b9-3379107df6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_message(prompt, context):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": f\"The user asks a question. Your task is to generate the SQL query to answer that question. Return SQL query only. The context of the question is the following: '{context}'\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "912ded5e-a7cb-4a4f-9fce-5254e8dee39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_response_batch(model, tokenizer, messages_list, enable_thinking=True, max_new_tokens=512):\n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=enable_thinking\n",
    "        )\n",
    "        for messages in messages_list\n",
    "    ]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        padding_side='left'\n",
    "    ).to(model.device)\n",
    "\n",
    "    model.eval()\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=max_new_tokens\n",
    "    )\n",
    "\n",
    "    responses = []\n",
    "    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids):\n",
    "        # Slice to get only generated part\n",
    "        output_only_ids = output_ids[len(input_ids):].tolist()\n",
    "\n",
    "        # Try to find `</think>` (id 151668)\n",
    "        try:\n",
    "            index = len(output_only_ids) - output_only_ids[::-1].index(151668)\n",
    "        except ValueError:\n",
    "            index = 0\n",
    "\n",
    "        if enable_thinking:\n",
    "            thinking_content = tokenizer.decode(\n",
    "                output_only_ids[:index],\n",
    "                skip_special_tokens=True\n",
    "            ).strip(\"\\n\")\n",
    "            content = tokenizer.decode(\n",
    "                output_only_ids[index:],\n",
    "                skip_special_tokens=True\n",
    "            ).strip(\"\\n\")\n",
    "        else:\n",
    "            thinking_content = None\n",
    "            content = tokenizer.decode(\n",
    "                output_only_ids,\n",
    "                skip_special_tokens=True\n",
    "            ).strip(\"\\n\")\n",
    "\n",
    "        responses.append({\n",
    "            'thinking_content': thinking_content,\n",
    "            'content': content\n",
    "        })\n",
    "\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b957410-6efa-4926-a3c9-e5fdc63513ea",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "932573b4-2ede-4724-a6a8-14428dd6125a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def normalize_sql(sql):\n",
    "    return sqlparse.format(sql, reindent=True, keyword_case='upper').strip()\n",
    "\n",
    "def compute_rouge(reference, prediction):\n",
    "    result = rouge.compute(predictions=[prediction], references=[reference])\n",
    "    return result['rougeL']\n",
    "\n",
    "def evaluate_sql_response(reference, prediction, sql_context):\n",
    "    # ROUGE-L\n",
    "    rouge_score = compute_rouge(reference, prediction)\n",
    "    \n",
    "    # execution check\n",
    "    try:\n",
    "        conn = sqlite3.connect(\":memory:\")\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.executescript(sql_context)\n",
    "        cursor.execute(reference)\n",
    "        ref_result = cursor.fetchall()\n",
    "        \n",
    "        cursor.execute(prediction)\n",
    "        model_result = cursor.fetchall()\n",
    "        \n",
    "        execution_match = ref_result == model_result\n",
    "    except Exception:\n",
    "        execution_match = False\n",
    "    finally:\n",
    "        conn.close()\n",
    "    \n",
    "    # final score\n",
    "    if execution_match:\n",
    "        final_score = 1.0\n",
    "    else:\n",
    "        final_score = 0.7 * rouge_score\n",
    "\n",
    "    return {\n",
    "        \"rougeL\": round(rouge_score, 4),\n",
    "        \"execution_match\": execution_match,\n",
    "        \"final_score\": final_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd975079-9037-4745-a8d4-d9d828ee2257",
   "metadata": {},
   "source": [
    "# Formatting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7aa8c985-2ebd-4f95-a5d3-a804abcce942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for training\n",
    "def construct_message_with_assistant_content(example):\n",
    "    messages = construct_message(example['sql_prompt'], example['sql_context'])\n",
    "    messages.append({\n",
    "        'role': 'assistant',\n",
    "        'content': example['sql']\n",
    "    })\n",
    "    return {'messages': messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21145d07-3501-44ff-acaa-ca323091539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(example, enable_thinking=ENABLE_THINKING):\n",
    "    return tokenizer.apply_chat_template(\n",
    "        example[\"messages\"],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False, # no generation prompt during training\n",
    "        enable_thinking=ENABLE_THINKING \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcdb61f-44c3-449a-a7a7-632136c58c87",
   "metadata": {},
   "source": [
    "# LoRA- step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94d19298-6ded-4234-82fe-a651910680df",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 4096\n",
    "VALID_SIZE = 1024\n",
    "\n",
    "ds_train_sample = ds_train.take(TRAIN_SIZE)\n",
    "ds_valid_sample = ds_valid.take(VALID_SIZE)\n",
    "\n",
    "len(ds_train_sample), len(ds_valid_sample), len(ds_test)\n",
    "\n",
    "ds_train_with_assistant_content = ds_train_sample.map(construct_message_with_assistant_content)\n",
    "ds_valid_with_assistant_content = ds_valid_sample.map(construct_message_with_assistant_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc3045e2-5576-41ad-9278-67c8dc80a01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU 0 load: 1.00\n",
      "CPU 1 load: 0.00\n",
      "CPU 2 load: 1.00\n",
      "CPU 3 load: 0.00\n",
      "RAM Total: 27.41 GB, Used: 1.79 GB\n",
      "GPU 0 (Tesla T4) load: 0.0%\n",
      "GPU 0 (Tesla T4) VRAM Total: 16384.0 MB, Used 2987.0 MB\n",
      "Disk Total: 60.95 GB, Used: 32.28 GB\n"
     ]
    }
   ],
   "source": [
    "get_vm_usage_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7e80681-a80e-40e1-a891-2bb16149587e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474c1bf1-b8a5-4d13-a946-2dffc2c2cd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: a8dgyigc with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbetas: [0.9, 0.9999]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teffective_batch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_dropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_r: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamw\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33molialeshka\u001b[0m (\u001b[33molialeshka-none\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/olialeshka_1/wandb/run-20250929_164644-a8dgyigc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/olialeshka-none/text-to-sql/runs/a8dgyigc' target=\"_blank\">robust-sweep-47</a></strong> to <a href='https://wandb.ai/olialeshka-none/text-to-sql' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/olialeshka-none/text-to-sql/sweeps/9a4oj3so' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql/sweeps/9a4oj3so</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/olialeshka-none/text-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/olialeshka-none/text-to-sql/sweeps/9a4oj3so' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql/sweeps/9a4oj3so</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/olialeshka-none/text-to-sql/runs/a8dgyigc' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql/runs/a8dgyigc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='239' max='256' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [239/256 05:58 < 00:25, 0.66 it/s, Epoch 0.93/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "sweep_config = {\n",
    "    'name': f'sweep-lora-step1-epochs1-samples{TRAIN_SIZE}-{timestamp}',\n",
    "    'method': 'bayes',\n",
    "    'metric': {\n",
    "        'name': 'eval_loss',\n",
    "        'goal': 'minimize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'optimizer': {'values': ['adam', 'adamw', 'nadam', 'adamax']},\n",
    "        'effective_batch_size': {'values': [16, 32, 64, 128, 256, 512]},\n",
    "        'learning_rate': {'values': [1e-4, 5e-5, 1e-5, 5e-6, 1e-6]},\n",
    "        'weight_decay': {'values': [0.0, 0.01, 0.1]},\n",
    "        'betas': {'values': [(0.9, 0.999), (0.95, 0.999), (0.9, 0.9999)]},\n",
    "        'warmup_ratio': {'values': [0.05, 0.1, 0.2]},\n",
    "        'epochs': {'values': [1]},\n",
    "        'lora_r': {'values': [4, 8, 16, 32]},\n",
    "        'lora_alpha': {'values': [2, 4, 8, 16, 32, 64]},\n",
    "        'lora_dropout': {'values': [0.01, 0.05, 0.1, 0.2]}\n",
    "    }\n",
    "}\n",
    "\n",
    "# sweep_id = wandb.sweep(sweep_config)\n",
    "sweep_id = '9a4oj3so' # continue the crashed sweep\n",
    "\n",
    "optimizer_map = {\n",
    "    \"adam\": torch.optim.Adam,\n",
    "    \"adamw\": torch.optim.AdamW,\n",
    "    \"nadam\": torch.optim.NAdam,\n",
    "    \"adamax\": torch.optim.Adamax\n",
    "}\n",
    "\n",
    "def sweep_train():\n",
    "    with wandb.init() as run:\n",
    "        model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "        config = wandb.config  \n",
    "        PER_DEVICE_BATCH_SIZE = 2  # higher values --> OOM\n",
    "        gradient_accumulation_steps = int(config.effective_batch_size / PER_DEVICE_BATCH_SIZE)\n",
    "        \n",
    "        training_args = TrainingArguments(\n",
    "            per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            learning_rate=config.learning_rate,\n",
    "            num_train_epochs=config.epochs,\n",
    "            weight_decay=config.weight_decay,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            warmup_ratio=config.warmup_ratio,\n",
    "            save_strategy=\"no\",\n",
    "            eval_strategy=\"epoch\",\n",
    "            logging_strategy=\"steps\",\n",
    "            logging_steps=1,\n",
    "            report_to=['wandb'],\n",
    "            fp16=True,\n",
    "            fp16_full_eval=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False,\n",
    "            max_grad_norm=1,\n",
    "            # load_best_model_at_end=True\n",
    "        )\n",
    "        \n",
    "        def build_optimizer(model):\n",
    "            optimizer_class = optimizer_map[config.optimizer]\n",
    "            return optimizer_class(\n",
    "                model.parameters(),\n",
    "                lr=config.learning_rate,\n",
    "                weight_decay=config.weight_decay,\n",
    "                betas=config.betas\n",
    "            )\n",
    "\n",
    "\n",
    "        peft_config = LoraConfig(\n",
    "            r=config.lora_r,\n",
    "            lora_alpha=config.lora_alpha,\n",
    "            lora_dropout=config.lora_dropout,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\"\n",
    "        )\n",
    "        model.requires_grad_(False)                     # freeze base weights (precautionary)\n",
    "        model_peft = get_peft_model(model, peft_config) # inject a LoRA adapter\n",
    "        \n",
    "        trainer = SFTTrainer(\n",
    "            model=model_peft,\n",
    "            train_dataset=ds_train_with_assistant_content,\n",
    "            eval_dataset=ds_valid_with_assistant_content,\n",
    "            formatting_func=formatting_func,\n",
    "            args=training_args,\n",
    "            optimizers=(build_optimizer(model_peft), None),  # (optimizer, scheduler)\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        for log in trainer.state.log_history:\n",
    "            if 'eval_loss' in log:\n",
    "                wandb.log({\n",
    "                    \"eval_loss\": log['eval_loss'],\n",
    "                    \"eval_perplexity\": math.exp(log['eval_loss']),\n",
    "                    \"step\": log['step'],\n",
    "                    \"learning_rate\": config.learning_rate,\n",
    "                    \"weight_decay\": config.weight_decay,\n",
    "                    \"betas\": config.betas,\n",
    "                    \"warmup_ratio\": config.warmup_ratio,\n",
    "                    \"effective_batch_size\": config.effective_batch_size,\n",
    "                    \"optimizer\": config.optimizer\n",
    "                })\n",
    "        wandb.finish(); # finish the run\n",
    "        del trainer\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "wandb.agent(sweep_id, function=sweep_train, count=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aeaf78d-1e8d-4820-967c-cd442cb31f10",
   "metadata": {},
   "source": [
    "# LoRA - step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fe64009-33a0-4d0e-b986-b2bfabe92528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU 0 load: 0.00\n",
      "CPU 1 load: 0.00\n",
      "CPU 2 load: 1.00\n",
      "CPU 3 load: 0.00\n",
      "RAM Total: 27.41 GB, Used: 1.73 GB\n",
      "GPU 0 (Tesla T4) load: 0.0%\n",
      "GPU 0 (Tesla T4) VRAM Total: 16384.0 MB, Used 2407.0 MB\n",
      "Disk Total: 60.95 GB, Used: 35.50 GB\n",
      "8192 2048 5851\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2852c2d5ec2640bbabe59174ed002b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8192 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62934266ec23442195f3fbb7902528dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2048 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "get_vm_usage_metrics()\n",
    "\n",
    "\n",
    "TRAIN_SIZE = 8192\n",
    "VALID_SIZE = 2048\n",
    "\n",
    "ds_train_sample = ds_train.take(TRAIN_SIZE)\n",
    "ds_valid_sample = ds_valid.take(VALID_SIZE)\n",
    "\n",
    "print(len(ds_train_sample), len(ds_valid_sample), len(ds_test))\n",
    "\n",
    "ds_train_with_assistant_content = ds_train_sample.map(construct_message_with_assistant_content)\n",
    "ds_valid_with_assistant_content = ds_valid_sample.map(construct_message_with_assistant_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a06fd2-7478-465c-8fb2-f695f972e58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: ub224w5u\n",
      "Sweep URL: https://wandb.ai/olialeshka-none/text-to-sql/sweeps/ub224w5u\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: d1jy1p86 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbetas: [0.9, 0.9999]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teffective_batch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_dropout: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_r: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamw\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: ðŸš€ View run \u001b[33mfloral-sweep-1\u001b[0m at: \u001b[34mhttps://wandb.ai/olialeshka-none/text-to-sql/runs/r450607u\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251004_103745-r450607u/logs\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/olialeshka_1/wandb/run-20251004_104025-d1jy1p86</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/olialeshka-none/text-to-sql/runs/d1jy1p86' target=\"_blank\">dainty-sweep-1</a></strong> to <a href='https://wandb.ai/olialeshka-none/text-to-sql' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/olialeshka-none/text-to-sql/sweeps/ub224w5u' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql/sweeps/ub224w5u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/olialeshka-none/text-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/olialeshka-none/text-to-sql/sweeps/ub224w5u' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql/sweeps/ub224w5u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/olialeshka-none/text-to-sql/runs/d1jy1p86' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql/runs/d1jy1p86</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='513' max='512' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [512/512 13:03, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "sweep_config = {\n",
    "    'name': f'sweep-lora-step2-epochs1-samples{TRAIN_SIZE}-{timestamp}',\n",
    "    'method': 'grid',\n",
    "    'metric': {\n",
    "        'name': 'eval_loss',\n",
    "        'goal': 'minimize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'optimizer': {'values': ['adamw', 'nadam']},\n",
    "        'effective_batch_size': {'values': [16]},\n",
    "        'learning_rate': {'values': [1e-5]}, # best results from bayes search have 1e-4 -> setting to a lower value\n",
    "        'weight_decay': {'values': [0.0]},\n",
    "        'betas': {'values': [(0.9, 0.9999)]},\n",
    "        'warmup_ratio': {'values': [0.05, 0.1, 0.2]},\n",
    "        'epochs': {'values': [1]},\n",
    "        'lora_r': {'values': [8, 16, 32]},\n",
    "        'lora_alpha': {'values': [64]},\n",
    "        'lora_dropout': {'values': [0.01, 0.05, 0.1, 0.2]}\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config)\n",
    "# sweep_id = '9a4oj3so' # continue the crashed sweep\n",
    "\n",
    "optimizer_map = {\n",
    "    \"adam\": torch.optim.Adam,\n",
    "    \"adamw\": torch.optim.AdamW,\n",
    "    \"nadam\": torch.optim.NAdam,\n",
    "    \"adamax\": torch.optim.Adamax\n",
    "}\n",
    "\n",
    "def sweep_train():\n",
    "    with wandb.init() as run:\n",
    "        model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "        config = wandb.config  \n",
    "        PER_DEVICE_BATCH_SIZE = 2  # higher values --> OOM\n",
    "        gradient_accumulation_steps = int(config.effective_batch_size / PER_DEVICE_BATCH_SIZE)\n",
    "        \n",
    "        training_args = TrainingArguments(\n",
    "            per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            learning_rate=config.learning_rate,\n",
    "            num_train_epochs=config.epochs,\n",
    "            weight_decay=config.weight_decay,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            warmup_ratio=config.warmup_ratio,\n",
    "            save_strategy=\"no\",\n",
    "            eval_strategy=\"epoch\",\n",
    "            logging_strategy=\"steps\",\n",
    "            logging_steps=1,\n",
    "            report_to=['wandb'],\n",
    "            fp16=True,\n",
    "            fp16_full_eval=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False,\n",
    "            max_grad_norm=1,\n",
    "            # load_best_model_at_end=True\n",
    "        )\n",
    "        \n",
    "        def build_optimizer(model):\n",
    "            optimizer_class = optimizer_map[config.optimizer]\n",
    "            return optimizer_class(\n",
    "                model.parameters(),\n",
    "                lr=config.learning_rate,\n",
    "                weight_decay=config.weight_decay,\n",
    "                betas=config.betas\n",
    "            )\n",
    "\n",
    "\n",
    "        peft_config = LoraConfig(\n",
    "            r=config.lora_r,\n",
    "            lora_alpha=config.lora_alpha,\n",
    "            lora_dropout=config.lora_dropout,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\"\n",
    "        )\n",
    "        model.requires_grad_(False)                     # freeze base weights (precautionary)\n",
    "        model_peft = get_peft_model(model, peft_config) # inject a LoRA adapter\n",
    "        \n",
    "        trainer = SFTTrainer(\n",
    "            model=model_peft,\n",
    "            train_dataset=ds_train_with_assistant_content,\n",
    "            eval_dataset=ds_valid_with_assistant_content,\n",
    "            formatting_func=formatting_func,\n",
    "            args=training_args,\n",
    "            optimizers=(build_optimizer(model_peft), None),  # (optimizer, scheduler)\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        for log in trainer.state.log_history:\n",
    "            if 'eval_loss' in log:\n",
    "                wandb.log({\n",
    "                    \"eval_loss\": log['eval_loss'],\n",
    "                    \"eval_perplexity\": math.exp(log['eval_loss']),\n",
    "                    \"step\": log['step'],\n",
    "                    \"learning_rate\": config.learning_rate,\n",
    "                    \"weight_decay\": config.weight_decay,\n",
    "                    \"betas\": config.betas,\n",
    "                    \"warmup_ratio\": config.warmup_ratio,\n",
    "                    \"effective_batch_size\": config.effective_batch_size,\n",
    "                    \"optimizer\": config.optimizer\n",
    "                })\n",
    "        wandb.finish(); # finish the run\n",
    "        del trainer\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "wandb.agent(sweep_id, function=sweep_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23bb48a-3244-40ea-b5fa-6b4bc71a0066",
   "metadata": {},
   "source": [
    "# LoRA - step 3 - the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e5b932a-7198-425c-a060-634c1ca40c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU 0 load: 0.00\n",
      "CPU 1 load: 0.00\n",
      "CPU 2 load: 0.00\n",
      "CPU 3 load: 0.00\n",
      "RAM Total: 27.41 GB, Used: 1.84 GB\n",
      "GPU 0 (Tesla T4) load: 0.0%\n",
      "GPU 0 (Tesla T4) VRAM Total: 16384.0 MB, Used 2407.0 MB\n",
      "Disk Total: 60.95 GB, Used: 32.44 GB\n",
      "97500 2500 5851\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "get_vm_usage_metrics()\n",
    "\n",
    "print(len(ds_train), len(ds_valid), len(ds_test))\n",
    "\n",
    "ds_train_with_assistant_content = ds_train.map(construct_message_with_assistant_content)\n",
    "ds_valid_with_assistant_content = ds_valid.map(construct_message_with_assistant_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd44b5e5-d0a5-4139-8a99-a8c954262d07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33molialeshka\u001b[0m (\u001b[33molialeshka-none\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/olialeshka_1/wandb/run-20251005_062940-073oc50f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/olialeshka-none/text-to-sql/runs/073oc50f' target=\"_blank\">lora-final-model-2025-10-05_06-29-38</a></strong> to <a href='https://wandb.ai/olialeshka-none/text-to-sql' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/olialeshka-none/text-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/olialeshka-none/text-to-sql/runs/073oc50f' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql/runs/073oc50f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "934b720e92a2401dbbcfe5384e48f90c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to train dataset:   0%|          | 0/97500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c432c1deae4a979804a0fe9273d3a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/97500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17e292b68e3b40e798cd685d56addf60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/97500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02df0da87c8e46d09d169d9f6ac8c21b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to eval dataset:   0%|          | 0/2500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90677b868d294e23840fc62e9940bc4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/2500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9bd6c4286cc4de7afb3cfe014b00285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/2500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Training Setup Summary =====\n",
      "Num epochs:            1\n",
      "Effective batch size:  16\n",
      "Per-device batch size: 2\n",
      "Gradient accumulation: 8\n",
      "Dataset size:          97500\n",
      "Steps per epoch:       6093\n",
      "Total training steps:  6093\n",
      "Warmup steps:          1218\n",
      "Logging steps:         40\n",
      "===================================\n",
      "Start time: 2025-10-05_06-32-05\n",
      "Starting fresh training run\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='681' max='6094' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 681/6094 53:34 < 7:07:06, 0.21 it/s, Epoch 0.11/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.211400</td>\n",
       "      <td>2.221138</td>\n",
       "      <td>1.160082</td>\n",
       "      <td>124369.000000</td>\n",
       "      <td>0.661763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.188700</td>\n",
       "      <td>2.151783</td>\n",
       "      <td>1.170986</td>\n",
       "      <td>246617.000000</td>\n",
       "      <td>0.666132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.063300</td>\n",
       "      <td>2.045051</td>\n",
       "      <td>1.174300</td>\n",
       "      <td>372357.000000</td>\n",
       "      <td>0.674203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.960600</td>\n",
       "      <td>1.898500</td>\n",
       "      <td>1.163218</td>\n",
       "      <td>496224.000000</td>\n",
       "      <td>0.690094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.763900</td>\n",
       "      <td>1.712549</td>\n",
       "      <td>1.134681</td>\n",
       "      <td>624347.000000</td>\n",
       "      <td>0.711704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.575200</td>\n",
       "      <td>1.465905</td>\n",
       "      <td>1.063029</td>\n",
       "      <td>749722.000000</td>\n",
       "      <td>0.730832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.279200</td>\n",
       "      <td>1.131720</td>\n",
       "      <td>0.885653</td>\n",
       "      <td>875852.000000</td>\n",
       "      <td>0.776071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.986700</td>\n",
       "      <td>0.885480</td>\n",
       "      <td>0.773703</td>\n",
       "      <td>999432.000000</td>\n",
       "      <td>0.808772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.822400</td>\n",
       "      <td>0.773341</td>\n",
       "      <td>0.707053</td>\n",
       "      <td>1123822.000000</td>\n",
       "      <td>0.836914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.735400</td>\n",
       "      <td>0.721101</td>\n",
       "      <td>0.662706</td>\n",
       "      <td>1249546.000000</td>\n",
       "      <td>0.845503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.693600</td>\n",
       "      <td>0.698866</td>\n",
       "      <td>0.660254</td>\n",
       "      <td>1373855.000000</td>\n",
       "      <td>0.847415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.691400</td>\n",
       "      <td>0.684146</td>\n",
       "      <td>0.649987</td>\n",
       "      <td>1494927.000000</td>\n",
       "      <td>0.849226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.668600</td>\n",
       "      <td>0.673223</td>\n",
       "      <td>0.656020</td>\n",
       "      <td>1621680.000000</td>\n",
       "      <td>0.850468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.663700</td>\n",
       "      <td>0.663680</td>\n",
       "      <td>0.646828</td>\n",
       "      <td>1745550.000000</td>\n",
       "      <td>0.851690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.658200</td>\n",
       "      <td>0.655895</td>\n",
       "      <td>0.639160</td>\n",
       "      <td>1868568.000000</td>\n",
       "      <td>0.852410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.646000</td>\n",
       "      <td>0.648730</td>\n",
       "      <td>0.630689</td>\n",
       "      <td>1992525.000000</td>\n",
       "      <td>0.853594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/313 01:50 < 00:04, 2.72 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "# resuming the prev run\n",
    "# timestamp = '2025-08-19_08-33-30'\n",
    "RUN_NAME = f'lora-final-model-{timestamp}'\n",
    "# run_id = 'imoh6jtd'\n",
    "wandb.init(\n",
    "    project=os.environ[\"WANDB_PROJECT\"],\n",
    "    name=RUN_NAME,\n",
    "    # id=run_id,         # resume previous run if available\n",
    "    resume=\"allow\",    # allows resuming crashed run\n",
    ")\n",
    "\n",
    "\n",
    "RESUME_TRAINING = True\n",
    "OUTPUT_DIR = \"./lora-final_model-output\"\n",
    "PER_DEVICE_BATCH_SIZE = 2  # higher values --> OOM\n",
    "\n",
    "optimizer = 'nadam'\n",
    "effective_batch_size = 16\n",
    "learning_rate = 1e-5\n",
    "weight_decay = 0.0\n",
    "betas = (0.9, 0.9999)\n",
    "warmup_ratio = 0.2\n",
    "epochs = 1\n",
    "gradient_accumulation_steps = int(effective_batch_size / PER_DEVICE_BATCH_SIZE)\n",
    "lora_r = 16\n",
    "lora_alpha = 64\n",
    "lora_dropout = 0.01\n",
    "\n",
    "optimizer_map = {\n",
    "    \"adam\": torch.optim.Adam,\n",
    "    \"adamw\": torch.optim.AdamW,\n",
    "    \"nadam\": torch.optim.NAdam,\n",
    "    \"adamax\": torch.optim.Adamax\n",
    "}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    num_train_epochs=epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=gradient_accumulation_steps*5,\n",
    "    save_total_limit=2,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=gradient_accumulation_steps*5,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=gradient_accumulation_steps*5,\n",
    "    report_to=['wandb'],\n",
    "    run_name=RUN_NAME,\n",
    "    fp16=True,\n",
    "    fp16_full_eval=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    max_grad_norm=1,\n",
    "    load_best_model_at_end=True,\n",
    "    gradient_checkpointing=True\n",
    ")\n",
    "\n",
    "def build_optimizer(model):\n",
    "    optimizer_class = optimizer_map[optimizer]\n",
    "    return optimizer_class(\n",
    "        model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        betas=betas\n",
    "    )\n",
    "\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model.requires_grad_(False)                     # freeze base weights (precautionary)\n",
    "model_peft = get_peft_model(model, peft_config) # inject a LoRA adapter\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model_peft,\n",
    "    train_dataset=ds_train_with_assistant_content,\n",
    "    eval_dataset=ds_valid_with_assistant_content,\n",
    "    formatting_func=formatting_func,\n",
    "    args=training_args,\n",
    "    optimizers=(build_optimizer(model_peft), None),  # (optimizer, scheduler)\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=25)]\n",
    ")\n",
    "\n",
    "\n",
    "# Training setup summary\n",
    "dataset_size = len(ds_train_with_assistant_content)\n",
    "steps_per_epoch = dataset_size // (PER_DEVICE_BATCH_SIZE * gradient_accumulation_steps)\n",
    "total_steps = steps_per_epoch * epochs\n",
    "warmup_steps = int(total_steps * warmup_ratio)\n",
    "\n",
    "print(\"===== Training Setup Summary =====\")\n",
    "print(f\"Num epochs:            {epochs}\")\n",
    "print(f\"Effective batch size:  {effective_batch_size}\")\n",
    "print(f\"Per-device batch size: {PER_DEVICE_BATCH_SIZE}\")\n",
    "print(f\"Gradient accumulation: {gradient_accumulation_steps}\")\n",
    "print(f\"Dataset size:          {dataset_size}\")\n",
    "print(f\"Steps per epoch:       {steps_per_epoch}\")\n",
    "print(f\"Total training steps:  {total_steps}\")\n",
    "print(f\"Warmup steps:          {warmup_steps}\")\n",
    "print(f\"Logging steps:         {training_args.logging_steps}\")\n",
    "print(\"===================================\")\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\")\n",
    "\n",
    "\n",
    "# Training\n",
    "last_checkpoint = None\n",
    "if RESUME_TRAINING and os.path.isdir(OUTPUT_DIR):\n",
    "    last_checkpoint = get_last_checkpoint(OUTPUT_DIR)\n",
    "\n",
    "if last_checkpoint is not None:\n",
    "    print(f\"Resuming training from checkpoint: {last_checkpoint}\")\n",
    "    trainer.train(resume_from_checkpoint=last_checkpoint)\n",
    "else:\n",
    "    print(\"Starting fresh training run\")\n",
    "    trainer.train()\n",
    "\n",
    "print(f\"End time: {datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\")\n",
    "\n",
    "\n",
    "# WandB logging of eval metrics\n",
    "for log in trainer.state.log_history:\n",
    "    if 'eval_loss' in log:\n",
    "        wandb.log({\n",
    "            \"eval_loss\": log['eval_loss'],\n",
    "            \"eval_perplexity\": math.exp(log['eval_loss']),\n",
    "            \"step\": log['step'],\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"betas\": betas,\n",
    "            \"warmup_ratio\": warmup_ratio,\n",
    "            \"effective_batch_size\": effective_batch_size,\n",
    "            \"optimizer\": optimizer\n",
    "        })\n",
    "\n",
    "wandb.finish()  # finish the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a85071-939a-4139-a48e-acaf10eeb482",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(OUTPUT_DIR, 'final')\n",
    "trainer.save_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3646cbc6-22fe-425d-944b-becf67217666",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79e4fc47-001c-40a5-a6fb-edc174736baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU 0 load: 2.00\n",
      "CPU 1 load: 0.00\n",
      "CPU 2 load: 1.00\n",
      "CPU 3 load: 0.00\n",
      "RAM Total: 27.41 GB, Used: 3.82 GB\n",
      "GPU 0 (Tesla T4) load: 0.0%\n",
      "GPU 0 (Tesla T4) VRAM Total: 16384.0 MB, Used 2515.0 MB\n",
      "Disk Total: 60.95 GB, Used: 33.67 GB\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "get_vm_usage_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f673e50-7a86-4f6d-8a90-ca295c05019b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "    (rotary_emb): Qwen3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = './lora-final_model-output/final'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f13328-2275-416b-b75e-3252b39f6f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 2025-10-05_15-15-10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d3ceaf45955449ea28f7e7ac61ef7de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "ENABLE_THINKING = False\n",
    "MAX_NEW_TOKENS = 512\n",
    "\n",
    "\n",
    "prompts = [ds_test[id]['sql_prompt'] for id in range(len(ds_test))]\n",
    "contexts = [ds_test[id]['sql_context'] for id in range(len(ds_test))]\n",
    "\n",
    "responses = []\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\")\n",
    "for i in tqdm(range(0, len(prompts), BATCH_SIZE)):\n",
    "    batch_prompts = prompts[i : i + BATCH_SIZE]\n",
    "    batch_contexts = contexts[i : i + BATCH_SIZE]\n",
    "\n",
    "    messages_list = [\n",
    "        construct_message(prompt=p, context=c)\n",
    "        for p, c in zip(batch_prompts, batch_contexts)\n",
    "    ]\n",
    "\n",
    "    batch_responses = generate_model_response_batch(model, tokenizer, messages_list, enable_thinking=ENABLE_THINKING, max_new_tokens=MAX_NEW_TOKENS)\n",
    "\n",
    "    responses.extend(batch_responses)\n",
    "\n",
    "print(f\"End time: {datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd50f297-d73c-4fb2-81a4-3952da67e019",
   "metadata": {},
   "outputs": [],
   "source": [
    "references = [ds_test[id]['sql'] for id in range(len(ds_test))]\n",
    "predictions = [responses[id]['content'] for id in range(len(ds_test))]\n",
    "\n",
    "scores = [\n",
    "    evaluate_sql_response(\n",
    "        reference=reference,\n",
    "        prediction=prediction,\n",
    "        sql_context=context\n",
    "    )\n",
    "    for reference, prediction, context in tqdm(zip(references, predictions, contexts), total=len(ds_test))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c89f771-1522-4f51-881b-4ddd41249e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean test set score: 0.736\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean test set score: {np.mean([score['final_score'] for score in scores]):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92efc8d8-1149-47b4-8dfa-3c068af3f82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lora_test_predictions.pkl', 'wb') as f:\n",
    "    pickle.dump(predictions, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546476cb-9f07-4da4-8d73-aba698badef7",
   "metadata": {},
   "source": [
    "# LoRA - step 4 - the final model (full LoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25cd64aa-1143-4101-9a86-b36cd8f3d4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU 0 load: 0.00\n",
      "CPU 1 load: 0.00\n",
      "CPU 2 load: 0.00\n",
      "CPU 3 load: 0.00\n",
      "RAM Total: 27.41 GB, Used: 1.78 GB\n",
      "GPU 0 (Tesla T4) load: 0.0%\n",
      "GPU 0 (Tesla T4) VRAM Total: 16384.0 MB, Used 2407.0 MB\n",
      "Disk Total: 60.95 GB, Used: 33.68 GB\n",
      "97500 2500 5851\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "get_vm_usage_metrics()\n",
    "\n",
    "print(len(ds_train), len(ds_valid), len(ds_test))\n",
    "\n",
    "ds_train_with_assistant_content = ds_train.map(construct_message_with_assistant_content)\n",
    "ds_valid_with_assistant_content = ds_valid.map(construct_message_with_assistant_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb93dafe-f869-4ddd-adf0-b7189c4332d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lora-final-model-all-layers-2025-10-06_06-58-49</strong> at: <a href='https://wandb.ai/olialeshka-none/text-to-sql/runs/u54nl2o2' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql/runs/u54nl2o2</a><br> View project at: <a href='https://wandb.ai/olialeshka-none/text-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251006_065851-u54nl2o2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/olialeshka_1/wandb/run-20251006_065944-g1hte8ko</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/olialeshka-none/text-to-sql/runs/g1hte8ko' target=\"_blank\">lora-final-model-all-linear-2025-10-06_06-59-44</a></strong> to <a href='https://wandb.ai/olialeshka-none/text-to-sql' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/olialeshka-none/text-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/olialeshka-none/text-to-sql/runs/g1hte8ko' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql/runs/g1hte8ko</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olialeshka_1/myenv/lib/python3.12/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/home/olialeshka_1/myenv/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Training Setup Summary =====\n",
      "Num epochs:            1\n",
      "Effective batch size:  16\n",
      "Per-device batch size: 2\n",
      "Gradient accumulation: 8\n",
      "Dataset size:          97500\n",
      "Steps per epoch:       6093\n",
      "Total training steps:  6093\n",
      "Warmup steps:          1218\n",
      "Logging steps:         40\n",
      "===================================\n",
      "Start time: 2025-10-06_06-59-49\n",
      "Starting fresh training run\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3521' max='6094' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3521/6094 6:00:52 < 4:23:51, 0.16 it/s, Epoch 0.58/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.206400</td>\n",
       "      <td>2.196743</td>\n",
       "      <td>1.164162</td>\n",
       "      <td>124369.000000</td>\n",
       "      <td>0.664029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.106300</td>\n",
       "      <td>1.990358</td>\n",
       "      <td>1.163912</td>\n",
       "      <td>246617.000000</td>\n",
       "      <td>0.681711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.812600</td>\n",
       "      <td>1.678701</td>\n",
       "      <td>1.103828</td>\n",
       "      <td>372357.000000</td>\n",
       "      <td>0.717856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.452700</td>\n",
       "      <td>1.201856</td>\n",
       "      <td>0.853513</td>\n",
       "      <td>496224.000000</td>\n",
       "      <td>0.779109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.981900</td>\n",
       "      <td>0.827730</td>\n",
       "      <td>0.746682</td>\n",
       "      <td>624347.000000</td>\n",
       "      <td>0.818777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.754700</td>\n",
       "      <td>0.714610</td>\n",
       "      <td>0.652080</td>\n",
       "      <td>749722.000000</td>\n",
       "      <td>0.845053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.679500</td>\n",
       "      <td>0.675604</td>\n",
       "      <td>0.634108</td>\n",
       "      <td>875852.000000</td>\n",
       "      <td>0.849026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.651000</td>\n",
       "      <td>0.650463</td>\n",
       "      <td>0.627987</td>\n",
       "      <td>999432.000000</td>\n",
       "      <td>0.851038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.637900</td>\n",
       "      <td>0.627056</td>\n",
       "      <td>0.621941</td>\n",
       "      <td>1123822.000000</td>\n",
       "      <td>0.853274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.608400</td>\n",
       "      <td>0.603315</td>\n",
       "      <td>0.621214</td>\n",
       "      <td>1249546.000000</td>\n",
       "      <td>0.859687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.577000</td>\n",
       "      <td>0.576999</td>\n",
       "      <td>0.596036</td>\n",
       "      <td>1373855.000000</td>\n",
       "      <td>0.860871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.568500</td>\n",
       "      <td>0.564891</td>\n",
       "      <td>0.564618</td>\n",
       "      <td>1494927.000000</td>\n",
       "      <td>0.862281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.554300</td>\n",
       "      <td>0.557143</td>\n",
       "      <td>0.559879</td>\n",
       "      <td>1621680.000000</td>\n",
       "      <td>0.863250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.551254</td>\n",
       "      <td>0.550406</td>\n",
       "      <td>1745550.000000</td>\n",
       "      <td>0.864635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.547600</td>\n",
       "      <td>0.545678</td>\n",
       "      <td>0.547816</td>\n",
       "      <td>1868568.000000</td>\n",
       "      <td>0.865071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.537700</td>\n",
       "      <td>0.561622</td>\n",
       "      <td>0.541905</td>\n",
       "      <td>1992525.000000</td>\n",
       "      <td>0.860735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.527000</td>\n",
       "      <td>0.536938</td>\n",
       "      <td>0.532602</td>\n",
       "      <td>2117736.000000</td>\n",
       "      <td>0.866405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.530600</td>\n",
       "      <td>0.532896</td>\n",
       "      <td>0.521216</td>\n",
       "      <td>2243838.000000</td>\n",
       "      <td>0.867377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.524000</td>\n",
       "      <td>0.528868</td>\n",
       "      <td>0.538161</td>\n",
       "      <td>2369773.000000</td>\n",
       "      <td>0.867516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.513400</td>\n",
       "      <td>0.524748</td>\n",
       "      <td>0.517158</td>\n",
       "      <td>2493245.000000</td>\n",
       "      <td>0.868443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.509000</td>\n",
       "      <td>0.521906</td>\n",
       "      <td>0.511863</td>\n",
       "      <td>2617170.000000</td>\n",
       "      <td>0.869131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.510300</td>\n",
       "      <td>0.518665</td>\n",
       "      <td>0.516094</td>\n",
       "      <td>2744130.000000</td>\n",
       "      <td>0.869663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.513000</td>\n",
       "      <td>0.515804</td>\n",
       "      <td>0.514144</td>\n",
       "      <td>2869215.000000</td>\n",
       "      <td>0.869995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.514600</td>\n",
       "      <td>0.512943</td>\n",
       "      <td>0.505923</td>\n",
       "      <td>2994541.000000</td>\n",
       "      <td>0.870563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.506900</td>\n",
       "      <td>0.510563</td>\n",
       "      <td>0.504188</td>\n",
       "      <td>3119995.000000</td>\n",
       "      <td>0.871059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.502400</td>\n",
       "      <td>0.507780</td>\n",
       "      <td>0.490771</td>\n",
       "      <td>3242892.000000</td>\n",
       "      <td>0.871241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.508500</td>\n",
       "      <td>0.505169</td>\n",
       "      <td>0.507620</td>\n",
       "      <td>3368807.000000</td>\n",
       "      <td>0.871547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.495700</td>\n",
       "      <td>0.502438</td>\n",
       "      <td>0.502972</td>\n",
       "      <td>3494234.000000</td>\n",
       "      <td>0.872127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>0.490800</td>\n",
       "      <td>0.500656</td>\n",
       "      <td>0.490636</td>\n",
       "      <td>3620158.000000</td>\n",
       "      <td>0.872709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.492600</td>\n",
       "      <td>0.497875</td>\n",
       "      <td>0.486284</td>\n",
       "      <td>3745218.000000</td>\n",
       "      <td>0.873333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>0.492300</td>\n",
       "      <td>0.496193</td>\n",
       "      <td>0.493359</td>\n",
       "      <td>3870248.000000</td>\n",
       "      <td>0.873371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.493300</td>\n",
       "      <td>0.493470</td>\n",
       "      <td>0.494371</td>\n",
       "      <td>3991422.000000</td>\n",
       "      <td>0.873997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>0.485300</td>\n",
       "      <td>0.491630</td>\n",
       "      <td>0.476801</td>\n",
       "      <td>4113951.000000</td>\n",
       "      <td>0.874311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.486500</td>\n",
       "      <td>0.489803</td>\n",
       "      <td>0.480917</td>\n",
       "      <td>4234571.000000</td>\n",
       "      <td>0.874343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.485900</td>\n",
       "      <td>0.488053</td>\n",
       "      <td>0.481723</td>\n",
       "      <td>4358402.000000</td>\n",
       "      <td>0.874907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.465800</td>\n",
       "      <td>0.486076</td>\n",
       "      <td>0.480913</td>\n",
       "      <td>4482389.000000</td>\n",
       "      <td>0.875167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>0.485800</td>\n",
       "      <td>0.484454</td>\n",
       "      <td>0.490685</td>\n",
       "      <td>4604801.000000</td>\n",
       "      <td>0.875468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.474800</td>\n",
       "      <td>0.482937</td>\n",
       "      <td>0.478531</td>\n",
       "      <td>4730995.000000</td>\n",
       "      <td>0.875638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>0.476200</td>\n",
       "      <td>0.481497</td>\n",
       "      <td>0.479630</td>\n",
       "      <td>4855282.000000</td>\n",
       "      <td>0.875991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.472700</td>\n",
       "      <td>0.480112</td>\n",
       "      <td>0.475769</td>\n",
       "      <td>4980167.000000</td>\n",
       "      <td>0.876041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>0.479600</td>\n",
       "      <td>0.478975</td>\n",
       "      <td>0.469626</td>\n",
       "      <td>5104199.000000</td>\n",
       "      <td>0.876683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.470800</td>\n",
       "      <td>0.477848</td>\n",
       "      <td>0.467909</td>\n",
       "      <td>5227583.000000</td>\n",
       "      <td>0.876802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>0.463800</td>\n",
       "      <td>0.476472</td>\n",
       "      <td>0.470841</td>\n",
       "      <td>5351854.000000</td>\n",
       "      <td>0.876792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.466500</td>\n",
       "      <td>0.475277</td>\n",
       "      <td>0.479242</td>\n",
       "      <td>5476499.000000</td>\n",
       "      <td>0.877161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.470300</td>\n",
       "      <td>0.474460</td>\n",
       "      <td>0.469437</td>\n",
       "      <td>5602578.000000</td>\n",
       "      <td>0.877481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.466200</td>\n",
       "      <td>0.472903</td>\n",
       "      <td>0.476561</td>\n",
       "      <td>5729224.000000</td>\n",
       "      <td>0.877699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1880</td>\n",
       "      <td>0.454800</td>\n",
       "      <td>0.471760</td>\n",
       "      <td>0.458426</td>\n",
       "      <td>5853606.000000</td>\n",
       "      <td>0.878087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.459900</td>\n",
       "      <td>0.471244</td>\n",
       "      <td>0.454048</td>\n",
       "      <td>5978974.000000</td>\n",
       "      <td>0.877999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1960</td>\n",
       "      <td>0.462900</td>\n",
       "      <td>0.469987</td>\n",
       "      <td>0.473743</td>\n",
       "      <td>6104097.000000</td>\n",
       "      <td>0.878003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.466300</td>\n",
       "      <td>0.469037</td>\n",
       "      <td>0.460819</td>\n",
       "      <td>6227167.000000</td>\n",
       "      <td>0.878521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>0.457300</td>\n",
       "      <td>0.468328</td>\n",
       "      <td>0.456773</td>\n",
       "      <td>6351483.000000</td>\n",
       "      <td>0.878224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.464300</td>\n",
       "      <td>0.467566</td>\n",
       "      <td>0.464210</td>\n",
       "      <td>6477267.000000</td>\n",
       "      <td>0.878426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2120</td>\n",
       "      <td>0.464500</td>\n",
       "      <td>0.466494</td>\n",
       "      <td>0.454650</td>\n",
       "      <td>6600959.000000</td>\n",
       "      <td>0.878874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.467100</td>\n",
       "      <td>0.465157</td>\n",
       "      <td>0.467374</td>\n",
       "      <td>6725226.000000</td>\n",
       "      <td>0.879168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.453800</td>\n",
       "      <td>0.465058</td>\n",
       "      <td>0.453066</td>\n",
       "      <td>6851631.000000</td>\n",
       "      <td>0.878997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.448900</td>\n",
       "      <td>0.464228</td>\n",
       "      <td>0.458164</td>\n",
       "      <td>6975550.000000</td>\n",
       "      <td>0.879362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2280</td>\n",
       "      <td>0.453500</td>\n",
       "      <td>0.463262</td>\n",
       "      <td>0.458114</td>\n",
       "      <td>7099082.000000</td>\n",
       "      <td>0.879437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2320</td>\n",
       "      <td>0.460900</td>\n",
       "      <td>0.462394</td>\n",
       "      <td>0.467978</td>\n",
       "      <td>7223859.000000</td>\n",
       "      <td>0.879215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2360</td>\n",
       "      <td>0.459600</td>\n",
       "      <td>0.461553</td>\n",
       "      <td>0.453345</td>\n",
       "      <td>7348438.000000</td>\n",
       "      <td>0.879769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.447500</td>\n",
       "      <td>0.460805</td>\n",
       "      <td>0.464969</td>\n",
       "      <td>7474714.000000</td>\n",
       "      <td>0.879777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2440</td>\n",
       "      <td>0.467500</td>\n",
       "      <td>0.460121</td>\n",
       "      <td>0.463692</td>\n",
       "      <td>7598702.000000</td>\n",
       "      <td>0.879944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2480</td>\n",
       "      <td>0.450300</td>\n",
       "      <td>0.459512</td>\n",
       "      <td>0.463326</td>\n",
       "      <td>7726839.000000</td>\n",
       "      <td>0.880195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2520</td>\n",
       "      <td>0.451200</td>\n",
       "      <td>0.458998</td>\n",
       "      <td>0.450943</td>\n",
       "      <td>7851428.000000</td>\n",
       "      <td>0.880165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2560</td>\n",
       "      <td>0.457500</td>\n",
       "      <td>0.458387</td>\n",
       "      <td>0.448439</td>\n",
       "      <td>7976490.000000</td>\n",
       "      <td>0.880412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.447900</td>\n",
       "      <td>0.457739</td>\n",
       "      <td>0.453394</td>\n",
       "      <td>8102626.000000</td>\n",
       "      <td>0.880357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2640</td>\n",
       "      <td>0.447900</td>\n",
       "      <td>0.457019</td>\n",
       "      <td>0.454013</td>\n",
       "      <td>8228131.000000</td>\n",
       "      <td>0.880499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2680</td>\n",
       "      <td>0.445900</td>\n",
       "      <td>0.457095</td>\n",
       "      <td>0.449205</td>\n",
       "      <td>8354846.000000</td>\n",
       "      <td>0.880387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2720</td>\n",
       "      <td>0.447900</td>\n",
       "      <td>0.456252</td>\n",
       "      <td>0.449187</td>\n",
       "      <td>8478501.000000</td>\n",
       "      <td>0.880582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2760</td>\n",
       "      <td>0.456000</td>\n",
       "      <td>0.455548</td>\n",
       "      <td>0.459681</td>\n",
       "      <td>8603724.000000</td>\n",
       "      <td>0.880760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.442100</td>\n",
       "      <td>0.455209</td>\n",
       "      <td>0.446441</td>\n",
       "      <td>8731542.000000</td>\n",
       "      <td>0.880790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2840</td>\n",
       "      <td>0.444100</td>\n",
       "      <td>0.454919</td>\n",
       "      <td>0.449398</td>\n",
       "      <td>8857356.000000</td>\n",
       "      <td>0.880722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2880</td>\n",
       "      <td>0.448100</td>\n",
       "      <td>0.454791</td>\n",
       "      <td>0.445954</td>\n",
       "      <td>8979958.000000</td>\n",
       "      <td>0.880876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2920</td>\n",
       "      <td>0.449800</td>\n",
       "      <td>0.453900</td>\n",
       "      <td>0.446356</td>\n",
       "      <td>9106720.000000</td>\n",
       "      <td>0.880962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2960</td>\n",
       "      <td>0.443400</td>\n",
       "      <td>0.453812</td>\n",
       "      <td>0.444354</td>\n",
       "      <td>9233825.000000</td>\n",
       "      <td>0.881067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.447900</td>\n",
       "      <td>0.452848</td>\n",
       "      <td>0.450287</td>\n",
       "      <td>9356038.000000</td>\n",
       "      <td>0.881087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3040</td>\n",
       "      <td>0.450300</td>\n",
       "      <td>0.452374</td>\n",
       "      <td>0.448652</td>\n",
       "      <td>9480919.000000</td>\n",
       "      <td>0.881317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3080</td>\n",
       "      <td>0.445400</td>\n",
       "      <td>0.452134</td>\n",
       "      <td>0.451246</td>\n",
       "      <td>9599777.000000</td>\n",
       "      <td>0.881423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3120</td>\n",
       "      <td>0.453000</td>\n",
       "      <td>0.451742</td>\n",
       "      <td>0.452504</td>\n",
       "      <td>9722903.000000</td>\n",
       "      <td>0.881467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3160</td>\n",
       "      <td>0.442900</td>\n",
       "      <td>0.451479</td>\n",
       "      <td>0.446021</td>\n",
       "      <td>9851517.000000</td>\n",
       "      <td>0.881412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.441900</td>\n",
       "      <td>0.450978</td>\n",
       "      <td>0.446395</td>\n",
       "      <td>9973473.000000</td>\n",
       "      <td>0.881347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3240</td>\n",
       "      <td>0.454900</td>\n",
       "      <td>0.450618</td>\n",
       "      <td>0.450971</td>\n",
       "      <td>10097788.000000</td>\n",
       "      <td>0.881644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3280</td>\n",
       "      <td>0.444700</td>\n",
       "      <td>0.450237</td>\n",
       "      <td>0.448353</td>\n",
       "      <td>10223447.000000</td>\n",
       "      <td>0.881628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3320</td>\n",
       "      <td>0.444500</td>\n",
       "      <td>0.449669</td>\n",
       "      <td>0.450329</td>\n",
       "      <td>10348416.000000</td>\n",
       "      <td>0.881664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3360</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>0.449774</td>\n",
       "      <td>0.445191</td>\n",
       "      <td>10474324.000000</td>\n",
       "      <td>0.881639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.443900</td>\n",
       "      <td>0.449092</td>\n",
       "      <td>0.446884</td>\n",
       "      <td>10600928.000000</td>\n",
       "      <td>0.881846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3440</td>\n",
       "      <td>0.441400</td>\n",
       "      <td>0.448637</td>\n",
       "      <td>0.440908</td>\n",
       "      <td>10726205.000000</td>\n",
       "      <td>0.881987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3480</td>\n",
       "      <td>0.447400</td>\n",
       "      <td>0.448565</td>\n",
       "      <td>0.444915</td>\n",
       "      <td>10850551.000000</td>\n",
       "      <td>0.881968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='74' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 74/313 00:32 < 01:44, 2.28 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "# resuming the prev run\n",
    "# timestamp = '2025-08-19_08-33-30'\n",
    "RUN_NAME = f'lora-final-model-all-linear-{timestamp}'\n",
    "# run_id = 'imoh6jtd'\n",
    "wandb.init(\n",
    "    project=os.environ[\"WANDB_PROJECT\"],\n",
    "    name=RUN_NAME,\n",
    "    # id=run_id,         # resume previous run if available\n",
    "    resume=\"allow\",    # allows resuming crashed run\n",
    ")\n",
    "\n",
    "\n",
    "RESUME_TRAINING = True\n",
    "OUTPUT_DIR = \"./lora-final_model_all_linear-output\"\n",
    "PER_DEVICE_BATCH_SIZE = 2  # higher values --> OOM\n",
    "\n",
    "optimizer = 'nadam'\n",
    "effective_batch_size = 16\n",
    "learning_rate = 1e-5\n",
    "weight_decay = 0.0\n",
    "betas = (0.9, 0.9999)\n",
    "warmup_ratio = 0.2\n",
    "epochs = 1\n",
    "gradient_accumulation_steps = int(effective_batch_size / PER_DEVICE_BATCH_SIZE)\n",
    "lora_r = 16\n",
    "lora_alpha = 64\n",
    "lora_dropout = 0.01\n",
    "\n",
    "optimizer_map = {\n",
    "    \"adam\": torch.optim.Adam,\n",
    "    \"adamw\": torch.optim.AdamW,\n",
    "    \"nadam\": torch.optim.NAdam,\n",
    "    \"adamax\": torch.optim.Adamax\n",
    "}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    num_train_epochs=epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=gradient_accumulation_steps*5,\n",
    "    save_total_limit=2,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=gradient_accumulation_steps*5,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=gradient_accumulation_steps*5,\n",
    "    report_to=['wandb'],\n",
    "    run_name=RUN_NAME,\n",
    "    fp16=True,\n",
    "    fp16_full_eval=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    max_grad_norm=1,\n",
    "    load_best_model_at_end=True,\n",
    "    gradient_checkpointing=True\n",
    ")\n",
    "\n",
    "def build_optimizer(model):\n",
    "    optimizer_class = optimizer_map[optimizer]\n",
    "    return optimizer_class(\n",
    "        model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        betas=betas\n",
    "    )\n",
    "\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules='all-linear'\n",
    ")\n",
    "model.requires_grad_(False)                     # freeze base weights (precautionary)\n",
    "model_peft = get_peft_model(model, peft_config) # inject a LoRA adapter\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model_peft,\n",
    "    train_dataset=ds_train_with_assistant_content,\n",
    "    eval_dataset=ds_valid_with_assistant_content,\n",
    "    formatting_func=formatting_func,\n",
    "    args=training_args,\n",
    "    optimizers=(build_optimizer(model_peft), None),  # (optimizer, scheduler)\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=25)]\n",
    ")\n",
    "\n",
    "\n",
    "# Training setup summary\n",
    "dataset_size = len(ds_train_with_assistant_content)\n",
    "steps_per_epoch = dataset_size // (PER_DEVICE_BATCH_SIZE * gradient_accumulation_steps)\n",
    "total_steps = steps_per_epoch * epochs\n",
    "warmup_steps = int(total_steps * warmup_ratio)\n",
    "\n",
    "print(\"===== Training Setup Summary =====\")\n",
    "print(f\"Num epochs:            {epochs}\")\n",
    "print(f\"Effective batch size:  {effective_batch_size}\")\n",
    "print(f\"Per-device batch size: {PER_DEVICE_BATCH_SIZE}\")\n",
    "print(f\"Gradient accumulation: {gradient_accumulation_steps}\")\n",
    "print(f\"Dataset size:          {dataset_size}\")\n",
    "print(f\"Steps per epoch:       {steps_per_epoch}\")\n",
    "print(f\"Total training steps:  {total_steps}\")\n",
    "print(f\"Warmup steps:          {warmup_steps}\")\n",
    "print(f\"Logging steps:         {training_args.logging_steps}\")\n",
    "print(\"===================================\")\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\")\n",
    "\n",
    "\n",
    "# Training\n",
    "last_checkpoint = None\n",
    "if RESUME_TRAINING and os.path.isdir(OUTPUT_DIR):\n",
    "    last_checkpoint = get_last_checkpoint(OUTPUT_DIR)\n",
    "\n",
    "if last_checkpoint is not None:\n",
    "    print(f\"Resuming training from checkpoint: {last_checkpoint}\")\n",
    "    trainer.train(resume_from_checkpoint=last_checkpoint)\n",
    "else:\n",
    "    print(\"Starting fresh training run\")\n",
    "    trainer.train()\n",
    "\n",
    "print(f\"End time: {datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\")\n",
    "\n",
    "\n",
    "# WandB logging of eval metrics\n",
    "for log in trainer.state.log_history:\n",
    "    if 'eval_loss' in log:\n",
    "        wandb.log({\n",
    "            \"eval_loss\": log['eval_loss'],\n",
    "            \"eval_perplexity\": math.exp(log['eval_loss']),\n",
    "            \"step\": log['step'],\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"betas\": betas,\n",
    "            \"warmup_ratio\": warmup_ratio,\n",
    "            \"effective_batch_size\": effective_batch_size,\n",
    "            \"optimizer\": optimizer\n",
    "        })\n",
    "\n",
    "wandb.finish()  # finish the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfe5325e-f203-4b31-a0b4-4e3d2ef8f478",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(OUTPUT_DIR, 'final')\n",
    "trainer.save_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897a2353-ba06-4823-b9f5-1a39da9ab233",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e690b40b-ebfb-4cb3-8e77-75dd2f66faa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU 0 load: 2.00\n",
      "CPU 1 load: 1.00\n",
      "CPU 2 load: 2.00\n",
      "CPU 3 load: 0.00\n",
      "RAM Total: 27.41 GB, Used: 2.87 GB\n",
      "GPU 0 (Tesla T4) load: 0.0%\n",
      "GPU 0 (Tesla T4) VRAM Total: 16384.0 MB, Used 2647.0 MB\n",
      "Disk Total: 60.95 GB, Used: 33.99 GB\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "get_vm_usage_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb3306b4-2b2a-44b1-8d74-e5f26769112d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (k_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (v_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (o_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (up_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (down_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "    (rotary_emb): Qwen3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = './lora-final_model_all_linear-output/final'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df38661-ec2a-43b0-9019-1e7ce501dd75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 2025-10-06_18-55-47\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f102dec34f410897efc7396475b4ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "ENABLE_THINKING = False\n",
    "MAX_NEW_TOKENS = 512\n",
    "\n",
    "\n",
    "prompts = [ds_test[id]['sql_prompt'] for id in range(len(ds_test))]\n",
    "contexts = [ds_test[id]['sql_context'] for id in range(len(ds_test))]\n",
    "\n",
    "responses = []\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\")\n",
    "for i in tqdm(range(0, len(prompts), BATCH_SIZE)):\n",
    "    batch_prompts = prompts[i : i + BATCH_SIZE]\n",
    "    batch_contexts = contexts[i : i + BATCH_SIZE]\n",
    "\n",
    "    messages_list = [\n",
    "        construct_message(prompt=p, context=c)\n",
    "        for p, c in zip(batch_prompts, batch_contexts)\n",
    "    ]\n",
    "\n",
    "    batch_responses = generate_model_response_batch(model, tokenizer, messages_list, enable_thinking=ENABLE_THINKING, max_new_tokens=MAX_NEW_TOKENS)\n",
    "\n",
    "    responses.extend(batch_responses)\n",
    "\n",
    "print(f\"End time: {datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163e915d-25dd-4013-8277-367b53d1d502",
   "metadata": {},
   "outputs": [],
   "source": [
    "references = [ds_test[id]['sql'] for id in range(len(ds_test))]\n",
    "predictions = [responses[id]['content'] for id in range(len(ds_test))]\n",
    "\n",
    "scores = [\n",
    "    evaluate_sql_response(\n",
    "        reference=reference,\n",
    "        prediction=prediction,\n",
    "        sql_context=context\n",
    "    )\n",
    "    for reference, prediction, context in tqdm(zip(references, predictions, contexts), total=len(ds_test))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2637ee51-d2de-4e19-ae85-636fd270d13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean test set score: 0.756\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean test set score: {np.mean([score['final_score'] for score in scores]):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3349a5-e952-4115-9e64-db1ffa74b935",
   "metadata": {},
   "source": [
    "# LoRA - step 5 - higher rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ec775d7-78d9-4c90-9324-ab3f0ab77992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU 0 load: 1.00\n",
      "CPU 1 load: 2.00\n",
      "CPU 2 load: 0.00\n",
      "CPU 3 load: 1.00\n",
      "RAM Total: 27.41 GB, Used: 1.90 GB\n",
      "GPU 0 (Tesla T4) load: 0.0%\n",
      "GPU 0 (Tesla T4) VRAM Total: 16384.0 MB, Used 2407.0 MB\n",
      "Disk Total: 60.95 GB, Used: 35.11 GB\n",
      "97500 2500 5851\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "get_vm_usage_metrics()\n",
    "\n",
    "print(len(ds_train), len(ds_valid), len(ds_test))\n",
    "\n",
    "ds_train_with_assistant_content = ds_train.map(construct_message_with_assistant_content)\n",
    "ds_valid_with_assistant_content = ds_valid.map(construct_message_with_assistant_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffb3ba9-6a2c-4a6d-84e8-9c2552279e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "# resuming the prev run\n",
    "timestamp = '2025-10-07_08-21-41'\n",
    "RUN_NAME = f'lora-final-model-all-linear-r64-{timestamp}'\n",
    "run_id = 'sla5zrpx'\n",
    "wandb.init(\n",
    "    project=os.environ[\"WANDB_PROJECT\"],\n",
    "    name=RUN_NAME,\n",
    "    id=run_id,         # resume previous run if available\n",
    "    resume=\"allow\",    # allows resuming crashed run\n",
    ")\n",
    "\n",
    "\n",
    "RESUME_TRAINING = True\n",
    "OUTPUT_DIR = \"./lora-final_model_all_linear_r64-output\"\n",
    "PER_DEVICE_BATCH_SIZE = 2  # higher values --> OOM\n",
    "\n",
    "optimizer = 'nadam'\n",
    "effective_batch_size = 16\n",
    "learning_rate = 1e-5\n",
    "weight_decay = 0.0\n",
    "betas = (0.9, 0.9999)\n",
    "warmup_ratio = 0.2\n",
    "epochs = 1\n",
    "gradient_accumulation_steps = int(effective_batch_size / PER_DEVICE_BATCH_SIZE)\n",
    "lora_r = 16*4\n",
    "lora_alpha = 64*4\n",
    "lora_dropout = 0.01\n",
    "\n",
    "optimizer_map = {\n",
    "    \"adam\": torch.optim.Adam,\n",
    "    \"adamw\": torch.optim.AdamW,\n",
    "    \"nadam\": torch.optim.NAdam,\n",
    "    \"adamax\": torch.optim.Adamax\n",
    "}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    num_train_epochs=epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=gradient_accumulation_steps*5,\n",
    "    save_total_limit=2,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=gradient_accumulation_steps*5,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=gradient_accumulation_steps*5,\n",
    "    report_to=['wandb'],\n",
    "    run_name=RUN_NAME,\n",
    "    fp16=True,\n",
    "    fp16_full_eval=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    max_grad_norm=1,\n",
    "    load_best_model_at_end=True,\n",
    "    gradient_checkpointing=True\n",
    ")\n",
    "\n",
    "def build_optimizer(model):\n",
    "    optimizer_class = optimizer_map[optimizer]\n",
    "    return optimizer_class(\n",
    "        model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        betas=betas\n",
    "    )\n",
    "\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules='all-linear'\n",
    ")\n",
    "model.requires_grad_(False)                     # freeze base weights (precautionary)\n",
    "model_peft = get_peft_model(model, peft_config) # inject a LoRA adapter\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model_peft,\n",
    "    train_dataset=ds_train_with_assistant_content,\n",
    "    eval_dataset=ds_valid_with_assistant_content,\n",
    "    formatting_func=formatting_func,\n",
    "    args=training_args,\n",
    "    optimizers=(build_optimizer(model_peft), None),  # (optimizer, scheduler)\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=25)]\n",
    ")\n",
    "\n",
    "\n",
    "# Training setup summary\n",
    "dataset_size = len(ds_train_with_assistant_content)\n",
    "steps_per_epoch = dataset_size // (PER_DEVICE_BATCH_SIZE * gradient_accumulation_steps)\n",
    "total_steps = steps_per_epoch * epochs\n",
    "warmup_steps = int(total_steps * warmup_ratio)\n",
    "\n",
    "print(\"===== Training Setup Summary =====\")\n",
    "print(f\"Num epochs:            {epochs}\")\n",
    "print(f\"Effective batch size:  {effective_batch_size}\")\n",
    "print(f\"Per-device batch size: {PER_DEVICE_BATCH_SIZE}\")\n",
    "print(f\"Gradient accumulation: {gradient_accumulation_steps}\")\n",
    "print(f\"Dataset size:          {dataset_size}\")\n",
    "print(f\"Steps per epoch:       {steps_per_epoch}\")\n",
    "print(f\"Total training steps:  {total_steps}\")\n",
    "print(f\"Warmup steps:          {warmup_steps}\")\n",
    "print(f\"Logging steps:         {training_args.logging_steps}\")\n",
    "print(\"===================================\")\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\")\n",
    "\n",
    "\n",
    "# Training\n",
    "last_checkpoint = None\n",
    "if RESUME_TRAINING and os.path.isdir(OUTPUT_DIR):\n",
    "    last_checkpoint = get_last_checkpoint(OUTPUT_DIR)\n",
    "\n",
    "if last_checkpoint is not None:\n",
    "    print(f\"Resuming training from checkpoint: {last_checkpoint}\")\n",
    "    trainer.train(resume_from_checkpoint=last_checkpoint)\n",
    "else:\n",
    "    print(\"Starting fresh training run\")\n",
    "    trainer.train()\n",
    "\n",
    "print(f\"End time: {datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\")\n",
    "\n",
    "\n",
    "# WandB logging of eval metrics\n",
    "for log in trainer.state.log_history:\n",
    "    if 'eval_loss' in log:\n",
    "        wandb.log({\n",
    "            \"eval_loss\": log['eval_loss'],\n",
    "            \"eval_perplexity\": math.exp(log['eval_loss']),\n",
    "            \"step\": log['step'],\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"betas\": betas,\n",
    "            \"warmup_ratio\": warmup_ratio,\n",
    "            \"effective_batch_size\": effective_batch_size,\n",
    "            \"optimizer\": optimizer\n",
    "        })\n",
    "\n",
    "wandb.finish()  # finish the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ffe445a-220e-40b8-92e5-820640ba1e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(OUTPUT_DIR, 'final')\n",
    "trainer.save_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31eafca-9a7f-4005-be86-8b458be95ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "ENABLE_THINKING = False\n",
    "MAX_NEW_TOKENS = 512\n",
    "\n",
    "\n",
    "prompts = [ds_test[id]['sql_prompt'] for id in range(len(ds_test))]\n",
    "contexts = [ds_test[id]['sql_context'] for id in range(len(ds_test))]\n",
    "\n",
    "responses = []\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\")\n",
    "for i in tqdm(range(0, len(prompts), BATCH_SIZE)):\n",
    "    batch_prompts = prompts[i : i + BATCH_SIZE]\n",
    "    batch_contexts = contexts[i : i + BATCH_SIZE]\n",
    "\n",
    "    messages_list = [\n",
    "        construct_message(prompt=p, context=c)\n",
    "        for p, c in zip(batch_prompts, batch_contexts)\n",
    "    ]\n",
    "\n",
    "    batch_responses = generate_model_response_batch(model, tokenizer, messages_list, enable_thinking=ENABLE_THINKING, max_new_tokens=MAX_NEW_TOKENS)\n",
    "\n",
    "    responses.extend(batch_responses)\n",
    "\n",
    "print(f\"End time: {datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdbb6b9-7fe1-4193-b553-ab92ebe46174",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "300719a4-93f1-4131-a6b5-68d393e125dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU 0 load: 0.00\n",
      "CPU 1 load: 3.00\n",
      "CPU 2 load: 0.00\n",
      "CPU 3 load: 0.00\n",
      "RAM Total: 27.41 GB, Used: 1.93 GB\n",
      "GPU 0 (Tesla T4) load: 0.0%\n",
      "GPU 0 (Tesla T4) VRAM Total: 16384.0 MB, Used 2407.0 MB\n",
      "Disk Total: 60.95 GB, Used: 35.11 GB\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "get_vm_usage_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc26f5a6-a630-4235-bdce-ad186ec7638e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 0 || All params: 636420096 || Trainable %: 0.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1024, out_features=64, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=64, out_features=2048, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (k_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1024, out_features=64, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (v_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1024, out_features=64, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (o_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1024, out_features=64, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=64, out_features=3072, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (up_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=1024, out_features=64, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=64, out_features=3072, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (down_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.01, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=3072, out_features=64, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "    (rotary_emb): Qwen3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = './lora-final_model_all_linear_r64-output/final'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(device)\n",
    "\n",
    "print_trainable_parameters(model)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa6ea4a7-10b2-43f3-9ce5-5e228cb26f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 2025-10-08_06-35-48\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48460301617d46c3bbb069e56e7566a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End time: 2025-10-08_07-25-10\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "ENABLE_THINKING = False\n",
    "MAX_NEW_TOKENS = 512\n",
    "\n",
    "\n",
    "prompts = [ds_test[id]['sql_prompt'] for id in range(len(ds_test))]\n",
    "contexts = [ds_test[id]['sql_context'] for id in range(len(ds_test))]\n",
    "\n",
    "responses = []\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\")\n",
    "for i in tqdm(range(0, len(prompts), BATCH_SIZE)):\n",
    "    batch_prompts = prompts[i : i + BATCH_SIZE]\n",
    "    batch_contexts = contexts[i : i + BATCH_SIZE]\n",
    "\n",
    "    messages_list = [\n",
    "        construct_message(prompt=p, context=c)\n",
    "        for p, c in zip(batch_prompts, batch_contexts)\n",
    "    ]\n",
    "\n",
    "    batch_responses = generate_model_response_batch(model, tokenizer, messages_list, enable_thinking=ENABLE_THINKING, max_new_tokens=MAX_NEW_TOKENS)\n",
    "\n",
    "    responses.extend(batch_responses)\n",
    "\n",
    "print(f\"End time: {datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f1d4b76-ee09-474c-85a9-ed7423ec0b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a45d9a86de0466c9e7f6cd14feed707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5851 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "references = [ds_test[id]['sql'] for id in range(len(ds_test))]\n",
    "predictions = [responses[id]['content'] for id in range(len(ds_test))]\n",
    "\n",
    "scores = [\n",
    "    evaluate_sql_response(\n",
    "        reference=reference,\n",
    "        prediction=prediction,\n",
    "        sql_context=context\n",
    "    )\n",
    "    for reference, prediction, context in tqdm(zip(references, predictions, contexts), total=len(ds_test))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72695579-53a6-4566-a258-9ebe46317369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean test set score: 0.774\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean test set score: {np.mean([score['final_score'] for score in scores]):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python t4",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
