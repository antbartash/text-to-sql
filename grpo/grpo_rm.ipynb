{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfe20728-8bcc-47fc-8148-a2144c58e259",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49d56f23-d923-4899-85c1-b91adf512c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import datasets\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import evaluate\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import sqlite3\n",
    "import sqlparse\n",
    "import _config\n",
    "\n",
    "import os\n",
    "import psutil\n",
    "import GPUtil\n",
    "import gc\n",
    "\n",
    "\n",
    "# Set the verbosity to WARNING to suppress INFO messages\n",
    "evaluate.logging.set_verbosity_warning()\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"WANDB_API_KEY\"] = _config.WANDB_API_KEY\n",
    "os.environ[\"WANDB_PROJECT\"] = _config.WANDB_PROJECT\n",
    "\n",
    "ENABLE_THINKING = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de55c654-917a-406c-bc81-201d4397e62f",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "525fe697-fbe6-4939-a379-b320f0bf0ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "CPU 0 load: 0.00\n",
      "CPU 1 load: 0.00\n",
      "CPU 2 load: 0.00\n",
      "CPU 3 load: 0.00\n",
      "CPU 4 load: 3.00\n",
      "CPU 5 load: 0.00\n",
      "CPU 6 load: 0.00\n",
      "CPU 7 load: 1.00\n",
      "CPU 8 load: 0.00\n",
      "CPU 9 load: 0.00\n",
      "CPU 10 load: 0.00\n",
      "CPU 11 load: 0.00\n",
      "CPU 12 load: 0.00\n",
      "CPU 13 load: 0.00\n",
      "CPU 14 load: 0.00\n",
      "CPU 15 load: 1.00\n",
      "CPU 16 load: 0.00\n",
      "CPU 17 load: 0.00\n",
      "CPU 18 load: 0.00\n",
      "CPU 19 load: 0.00\n",
      "CPU 20 load: 0.00\n",
      "CPU 21 load: 0.00\n",
      "CPU 22 load: 0.00\n",
      "CPU 23 load: 0.00\n",
      "CPU 24 load: 0.00\n",
      "CPU 25 load: 0.00\n",
      "CPU 26 load: 0.00\n",
      "CPU 27 load: 0.00\n",
      "CPU 28 load: 0.00\n",
      "CPU 29 load: 0.00\n",
      "CPU 30 load: 0.00\n",
      "CPU 31 load: 0.00\n",
      "CPU 32 load: 0.00\n",
      "CPU 33 load: 0.00\n",
      "CPU 34 load: 0.00\n",
      "CPU 35 load: 0.00\n",
      "CPU 36 load: 0.00\n",
      "CPU 37 load: 0.00\n",
      "CPU 38 load: 0.00\n",
      "CPU 39 load: 0.00\n",
      "RAM Total: 314.69 GB, Used: 5.92 GB\n",
      "GPU 0 (NVIDIA H100 NVL) load: 0.0%\n",
      "GPU 0 (NVIDIA H100 NVL) VRAM Total: 95830.0 MB, Used 4.0 MB\n",
      "Disk Total: 122.95 GB, Used: 38.72 GB\n"
     ]
    }
   ],
   "source": [
    "def get_vm_usage_metrics():\n",
    "    # CPU usage\n",
    "    cpu_load = psutil.cpu_percent(interval=1, percpu=True)\n",
    "    for id, load in enumerate(cpu_load):\n",
    "        print(f\"CPU {id} load: {load:.2f}\")\n",
    "    # RAM usage\n",
    "    ram = psutil.virtual_memory()\n",
    "    print(f\"RAM Total: {ram.total/(1024**3):.2f} GB, Used: {(ram.used)/(1024**3):.2f} GB\")\n",
    "    # GPU\n",
    "    if torch.cuda.is_available():\n",
    "        gpus = GPUtil.getGPUs()\n",
    "        for gpu in gpus:\n",
    "            print(f\"GPU {gpu.id} ({gpu.name}) load: {gpu.load*100}%\")\n",
    "            print(f\"GPU {gpu.id} ({gpu.name}) VRAM Total: {gpu.memoryTotal} MB, Used {gpu.memoryUsed} MB\")\n",
    "    # Disk \n",
    "    disk = psutil.disk_usage('/')\n",
    "    print(f\"Disk Total: {disk.total/(1024**3):.2f} GB, Used: {(disk.used)/(1024**3):.2f} GB\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f'Device: {device}')\n",
    "get_vm_usage_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a546b1f-a4ba-4371-9b18-15ab009d0ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_response_batch(model, tokenizer, messages_list, enable_thinking=False, max_new_tokens=512):\n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=enable_thinking\n",
    "        )\n",
    "        for messages in messages_list\n",
    "    ]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        padding_side='left'\n",
    "    ).to(model.device)\n",
    "\n",
    "    model.eval()\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=max_new_tokens\n",
    "    )\n",
    "\n",
    "    responses = []\n",
    "    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids):\n",
    "        # Slice to get only generated part\n",
    "        output_only_ids = output_ids[len(input_ids):].tolist()\n",
    "\n",
    "        # Try to find `</think>` (id 151668)\n",
    "        try:\n",
    "            index = len(output_only_ids) - output_only_ids[::-1].index(151668)\n",
    "        except ValueError:\n",
    "            index = 0\n",
    "\n",
    "        if enable_thinking:\n",
    "            thinking_content = tokenizer.decode(\n",
    "                output_only_ids[:index],\n",
    "                skip_special_tokens=True\n",
    "            ).strip(\"\\n\")\n",
    "            content = tokenizer.decode(\n",
    "                output_only_ids[index:],\n",
    "                skip_special_tokens=True\n",
    "            ).strip(\"\\n\")\n",
    "        else:\n",
    "            thinking_content = None\n",
    "            content = tokenizer.decode(\n",
    "                output_only_ids,\n",
    "                skip_special_tokens=True\n",
    "            ).strip(\"\\n\")\n",
    "\n",
    "        responses.append({\n",
    "            'thinking_content': thinking_content,\n",
    "            'content': content\n",
    "        })\n",
    "\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f2fc14-5482-4a8c-b9cb-41410dbc89ec",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95c19a40-edd8-4972-9a7e-b3ee05c35df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'domain', 'domain_description', 'sql_complexity', 'sql_complexity_description', 'sql_task_type', 'sql_task_type_description', 'sql_prompt', 'sql_context', 'sql', 'sql_explanation'],\n",
       "    num_rows: 9750\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = datasets.load_dataset('gretelai/synthetic_text_to_sql', streaming=False)\n",
    "ds_train, ds_test = ds['train'], ds['test']\n",
    "\n",
    "ds_train_subset = ds_train.train_test_split(test_size=0.1, seed=42)['test']\n",
    "split = ds_train_subset.train_test_split(test_size=0.025, seed=42)\n",
    "ds_train = split['train']\n",
    "ds_valid = split['test']\n",
    "\n",
    "ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c786aa8c-a47f-4f32-ac54-6d36e4fa3c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'domain', 'domain_description', 'sql_complexity', 'sql_complexity_description', 'sql_task_type', 'sql_task_type_description', 'sql_prompt', 'sql_context', 'ground_truth', 'sql_explanation', 'prompt'],\n",
       "    num_rows: 9750\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# datasets for GRPO must include a column \"prompt\"\n",
    "def construct_message(example):\n",
    "    return {\"prompt\": [\n",
    "        {\"role\": \"system\", \"content\": f\"The user asks a question. Your task is to generate the SQL query to answer that question. Return SQL query only. The context of the question is the following: '{example['sql_context']}'\"},\n",
    "        {\"role\": \"user\", \"content\": example['sql_prompt']}\n",
    "    ]}\n",
    "ds_train = ds_train.map(construct_message)\n",
    "ds_valid = ds_valid.map(construct_message)\n",
    "\n",
    "# rename the ground_truth column\n",
    "ds_train = ds_train.rename_column(\"sql\", \"ground_truth\")\n",
    "ds_valid = ds_valid.rename_column(\"sql\", \"ground_truth\")\n",
    "\n",
    "ds_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4491c8-ac1b-4cac-a00b-09901446e3e8",
   "metadata": {},
   "source": [
    "# Reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35250160-1ad4-40c4-884e-6639185690eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc2e8390c88f4f75a2790df5fb97480a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"rm-output\",\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-0.6B\", padding_side='left')\n",
    "\n",
    "\n",
    "def get_reward_model_scores(completions, batch_size=8):\n",
    "    scores = []\n",
    "    for i in range(0, len(completions), batch_size):\n",
    "        batch = completions[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = reward_model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            if logits.shape[-1] == 1:\n",
    "                probs = torch.sigmoid(logits.squeeze(-1))\n",
    "            else:\n",
    "                probs = torch.softmax(logits, dim=-1)[:, 1]\n",
    "            batch_scores = probs.cpu().float().tolist()\n",
    "        scores.extend(batch_scores)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52d04f4a-a806-4ab7-be3c-288295542163",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def normalize_sql(sql):\n",
    "    return sqlparse.format(sql, reindent=True, keyword_case='upper').strip()\n",
    "\n",
    "def compute_rouge(reference, prediction):\n",
    "    result = rouge.compute(predictions=[prediction], references=[reference])\n",
    "    return result['rougeL']\n",
    "\n",
    "\n",
    "def evaluate_sql_response(reference, prediction, sql_context):\n",
    "    # ROUGE-L\n",
    "    rouge_score = compute_rouge(reference, prediction)\n",
    "\n",
    "    for ref, pred,  in zip(reference, prediction):\n",
    "        # execution check\n",
    "        try:\n",
    "            conn = sqlite3.connect(\":memory:\")\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            cursor.executescript(sql_context)\n",
    "            cursor.execute(reference)\n",
    "            ref_result = cursor.fetchall()\n",
    "            \n",
    "            cursor.execute(prediction)\n",
    "            model_result = cursor.fetchall()\n",
    "            \n",
    "            execution_match = ref_result == model_result\n",
    "        except Exception:\n",
    "            execution_match = False\n",
    "        finally:\n",
    "            conn.close()\n",
    "        \n",
    "        # final score\n",
    "        if execution_match:\n",
    "            final_score = 1.0\n",
    "        else:\n",
    "            final_score = 0.7 * rouge_score\n",
    "\n",
    "    return {\n",
    "        \"rougeL\": round(rouge_score, 4),\n",
    "        \"execution_match\": execution_match,\n",
    "        \"final_score\": final_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06493cbd-405a-48f8-9d34-517597268387",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_func(completions, ground_truth, sql_context, **kwargs):\n",
    "    scores = []\n",
    "    rm_scores = []\n",
    "\n",
    "    for reference, prediction, context in zip(ground_truth, completions, sql_context):\n",
    "        result = evaluate_sql_response(\n",
    "            reference=reference,\n",
    "            prediction=prediction[0][\"content\"],\n",
    "            sql_context=context\n",
    "        )\n",
    "        scores.append(result[\"final_score\"])\n",
    "\n",
    "    completion_texts = [c[0][\"content\"] for c in completions]\n",
    "    rm_scores = get_reward_model_scores(completion_texts)\n",
    "\n",
    "    combined_scores = [\n",
    "        0.2 * rm + 0.8 * score\n",
    "        for rm, score in zip(rm_scores, scores)\n",
    "    ]\n",
    "    return combined_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd8f046-2cc9-4c40-b9a5-2ee9d98e19c1",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "859dac18-d643-4903-ba52-5848afc209b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e27df81b91941528e34eb26686f53a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU 0 load: 0.00\n",
      "CPU 1 load: 0.00\n",
      "CPU 2 load: 0.00\n",
      "CPU 3 load: 0.00\n",
      "CPU 4 load: 0.00\n",
      "CPU 5 load: 0.00\n",
      "CPU 6 load: 0.00\n",
      "CPU 7 load: 0.00\n",
      "CPU 8 load: 0.00\n",
      "CPU 9 load: 0.00\n",
      "CPU 10 load: 0.00\n",
      "CPU 11 load: 1.00\n",
      "CPU 12 load: 0.00\n",
      "CPU 13 load: 0.00\n",
      "CPU 14 load: 0.00\n",
      "CPU 15 load: 0.00\n",
      "CPU 16 load: 0.00\n",
      "CPU 17 load: 0.00\n",
      "CPU 18 load: 0.00\n",
      "CPU 19 load: 0.00\n",
      "CPU 20 load: 0.00\n",
      "CPU 21 load: 0.00\n",
      "CPU 22 load: 0.00\n",
      "CPU 23 load: 0.00\n",
      "CPU 24 load: 1.00\n",
      "CPU 25 load: 0.00\n",
      "CPU 26 load: 0.00\n",
      "CPU 27 load: 0.00\n",
      "CPU 28 load: 0.00\n",
      "CPU 29 load: 0.00\n",
      "CPU 30 load: 0.00\n",
      "CPU 31 load: 0.00\n",
      "CPU 32 load: 0.00\n",
      "CPU 33 load: 0.00\n",
      "CPU 34 load: 0.00\n",
      "CPU 35 load: 0.00\n",
      "CPU 36 load: 0.00\n",
      "CPU 37 load: 0.00\n",
      "CPU 38 load: 0.00\n",
      "CPU 39 load: 0.00\n",
      "RAM Total: 314.69 GB, Used: 5.23 GB\n",
      "GPU 0 (NVIDIA H100 NVL) load: 0.0%\n",
      "GPU 0 (NVIDIA H100 NVL) VRAM Total: 95830.0 MB, Used 3101.0 MB\n",
      "Disk Total: 122.95 GB, Used: 36.23 GB\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, padding_side='left')\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    checkpoint,\n",
    "    # attn_implementation=\"sdpa\",\n",
    "    device_map=\"auto\",\n",
    "    dtype=torch.float16,\n",
    "    # quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.gradient_checkpointing_enable()\n",
    "# model = prepare_model_for_kbit_training(model)\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "get_vm_usage_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21da278c-ac63-4dc4-903a-f65a3e1de9dc",
   "metadata": {},
   "source": [
    "# GRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81214b5-35fd-4109-b4b7-4a36c258b177",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: [wandb.login()] Loaded credentials for https://api.wandb.ai from WANDB_API_KEY.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33molialeshka\u001b[0m (\u001b[33molialeshka-none\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.25.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/olialeshka/wandb/run-20260215_201420-g7sle00o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/olialeshka-none/text-to-sql/runs/g7sle00o' target=\"_blank\">grpo-rm-lr1e8-epochs1-2026-02-15_20-14-19</a></strong> to <a href='https://wandb.ai/olialeshka-none/text-to-sql' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/olialeshka-none/text-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/olialeshka-none/text-to-sql/runs/g7sle00o' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql/runs/g7sle00o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Training Setup Summary =====\n",
      "Num epochs:            1\n",
      "Effective batch size:  128\n",
      "Per-device batch size: 128\n",
      "Gradient accumulation: 1\n",
      "Dataset size:          9750\n",
      "Steps per epoch:       76\n",
      "Total training steps:  76\n",
      "Warmup steps:          7\n",
      "Logging steps:         30\n",
      "===================================\n",
      "Start time: 2026-02-15_20-14-22\n",
      "Starting fresh training run\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing `generation_config` together with generation-related arguments=({'disable_compile'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='42' max='609' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 42/609 11:19 < 2:40:28, 0.06 it/s, Epoch 0.07/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.005769</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "# timestamp = '2026-01-25_09-24-49'\n",
    "RUN_NAME = f'grpo-rm-lr1e8-epochs1-{timestamp}'\n",
    "OUTPUT_DIR = './grpo-rm-lr1e8-output'\n",
    "RESUME_TRAINING = True\n",
    "\n",
    "PER_DEVICE_BATCH_SIZE = 128\n",
    "effective_batch_size = 128\n",
    "epochs=1\n",
    "learning_rate = 1e-8 # changed from 1e-5\n",
    "warmup_ratio = 0.1\n",
    "lora_r = 16*4\n",
    "lora_alpha = 64*4\n",
    "lora_dropout = 0.01\n",
    "\n",
    "gradient_accumulation_steps = int(effective_batch_size / PER_DEVICE_BATCH_SIZE)\n",
    "\n",
    "wandb.init(\n",
    "    project=os.environ[\"WANDB_PROJECT\"],\n",
    "    name=RUN_NAME,\n",
    "    # id='yrm8qwl9' ,         # resume previous run if available\n",
    "    # resume=\"allow\",    # allows resuming crashed run\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    \n",
    "    chat_template_kwargs = {\"enable_thinking\": False},\n",
    "    num_train_epochs=epochs,\n",
    "    num_generations=8,\n",
    "    # use_liger_kernel=True,\n",
    "    \n",
    "    per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=30,\n",
    "    save_total_limit=2,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=30,\n",
    "    per_device_eval_batch_size=PER_DEVICE_BATCH_SIZE*4,\n",
    "    eval_accumulation_steps=1,\n",
    "    # eval_kwargs={\"num_generations\": 1},\n",
    "    num_generations_eval=1,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=30,\n",
    "    report_to=['wandb'],\n",
    "    run_name=RUN_NAME,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    max_grad_norm=1,\n",
    "    load_best_model_at_end=True,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    # generate_during_eval=True\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules='all-linear'\n",
    ")\n",
    "# model.requires_grad_(False)                     # freeze base weights (precautionary)\n",
    "model_peft = get_peft_model(model, peft_config) # inject a LoRA adapter\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    processing_class=tokenizer,\n",
    "    model=model_peft,\n",
    "    args=training_args,\n",
    "    reward_funcs=[reward_func],\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_valid,\n",
    "    # callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "\n",
    "# Training setup summary\n",
    "dataset_size = len(ds_train)\n",
    "steps_per_epoch = dataset_size // (PER_DEVICE_BATCH_SIZE * gradient_accumulation_steps)\n",
    "total_steps = steps_per_epoch * epochs\n",
    "warmup_steps = int(total_steps * warmup_ratio)\n",
    "\n",
    "print(\"===== Training Setup Summary =====\")\n",
    "print(f\"Num epochs:            {epochs}\")\n",
    "print(f\"Effective batch size:  {effective_batch_size}\")\n",
    "print(f\"Per-device batch size: {PER_DEVICE_BATCH_SIZE}\")\n",
    "print(f\"Gradient accumulation: {gradient_accumulation_steps}\")\n",
    "print(f\"Dataset size:          {dataset_size}\")\n",
    "print(f\"Steps per epoch:       {steps_per_epoch}\")\n",
    "print(f\"Total training steps:  {total_steps}\")\n",
    "print(f\"Warmup steps:          {warmup_steps}\")\n",
    "print(f\"Logging steps:         {training_args.logging_steps}\")\n",
    "print(\"===================================\")\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\")\n",
    "\n",
    "\n",
    "# Training\n",
    "last_checkpoint = None\n",
    "if RESUME_TRAINING and os.path.isdir(OUTPUT_DIR):\n",
    "    last_checkpoint = get_last_checkpoint(OUTPUT_DIR)\n",
    "\n",
    "if last_checkpoint is not None:\n",
    "    print(f\"Resuming training from checkpoint: {last_checkpoint}\")\n",
    "    trainer.train(resume_from_checkpoint=last_checkpoint)\n",
    "else:\n",
    "    print(\"Starting fresh training run\")\n",
    "    trainer.train()\n",
    "\n",
    "print(f\"End time: {datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8978293a-2f92-4370-b6f1-e4bd7bb959dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bd5c091f556443299654b537fbba38f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.save_pretrained(f\"{OUTPUT_DIR}/best_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7495893-c8fb-4d81-a16a-09346fe6aa9b",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc97fe8b-51ec-4679-b391-0fe44ae89737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22af9ba4d836408fab8c67485a29482a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d67a8e9979b14a239ec8f107426c483c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/392 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "OUTPUT_DIR = './grpo-rm-lr1e8-output'\n",
    "checkpoint = f\"{OUTPUT_DIR}/checkpoint-609/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, dtype=torch.float16).to(device)\n",
    "model.eval()\n",
    "\n",
    "ds = datasets.load_dataset('gretelai/synthetic_text_to_sql', streaming=False)\n",
    "ds_train, ds_test = ds['train'], ds['test']\n",
    "\n",
    "\n",
    "def construct_message(prompt, context):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": f\"The user asks a question. Your task is to generate the SQL query to answer that question. Return SQL query only. The context of the question is the following: '{context}'\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "def generate_model_response_batch(messages_list, enable_thinking=True, max_new_tokens=512):\n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=enable_thinking\n",
    "        )\n",
    "        for messages in messages_list\n",
    "    ]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        padding_side='left'\n",
    "    ).to(model.device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=max_new_tokens\n",
    "    )\n",
    "\n",
    "    responses = []\n",
    "    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids):\n",
    "        # Slice to get only generated part\n",
    "        output_only_ids = output_ids[len(input_ids):].tolist()\n",
    "\n",
    "        # Try to find `</think>` (id 151668)\n",
    "        try:\n",
    "            index = len(output_only_ids) - output_only_ids[::-1].index(151668)\n",
    "        except ValueError:\n",
    "            index = 0\n",
    "\n",
    "        if enable_thinking:\n",
    "            thinking_content = tokenizer.decode(\n",
    "                output_only_ids[:index],\n",
    "                skip_special_tokens=True\n",
    "            ).strip(\"\\n\")\n",
    "            content = tokenizer.decode(\n",
    "                output_only_ids[index:],\n",
    "                skip_special_tokens=True\n",
    "            ).strip(\"\\n\")\n",
    "        else:\n",
    "            thinking_content = None\n",
    "            content = tokenizer.decode(\n",
    "                output_only_ids,\n",
    "                skip_special_tokens=True\n",
    "            ).strip(\"\\n\")\n",
    "\n",
    "        responses.append({\n",
    "            'thinking_content': thinking_content,\n",
    "            'content': content\n",
    "        })\n",
    "\n",
    "    return responses\n",
    "\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def normalize_sql(sql):\n",
    "    return sqlparse.format(sql, reindent=True, keyword_case='upper').strip()\n",
    "\n",
    "def compute_rouge(reference, prediction):\n",
    "    result = rouge.compute(predictions=[prediction], references=[reference])\n",
    "    return result['rougeL']\n",
    "\n",
    "def evaluate_sql_response(reference, prediction, sql_context):\n",
    "    # ROUGE-L\n",
    "    rouge_score = compute_rouge(reference, prediction)\n",
    "    \n",
    "    # execution check\n",
    "    try:\n",
    "        conn = sqlite3.connect(\":memory:\")\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.executescript(sql_context)\n",
    "        cursor.execute(reference)\n",
    "        ref_result = cursor.fetchall()\n",
    "        \n",
    "        cursor.execute(prediction)\n",
    "        model_result = cursor.fetchall()\n",
    "        \n",
    "        execution_match = ref_result == model_result\n",
    "    except Exception:\n",
    "        execution_match = False\n",
    "    finally:\n",
    "        conn.close()\n",
    "    \n",
    "    # final score\n",
    "    if execution_match:\n",
    "        final_score = 1.0\n",
    "    else:\n",
    "        final_score = 0.7 * rouge_score\n",
    "\n",
    "    return {\n",
    "        \"rougeL\": round(rouge_score, 4),\n",
    "        \"execution_match\": execution_match,\n",
    "        \"final_score\": final_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "081b17b1-4e0b-4df0-8377-21bb25aa93b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: Sun Feb 15 23:34:58 2026\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72b0fe2fa9304d428b47e42908a47b9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End time: Sun Feb 15 23:43:10 2026\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 256\n",
    "ENABLE_THINKING = False\n",
    "MAX_NEW_TOKENS = 512\n",
    "\n",
    "\n",
    "prompts = [ds_test[id]['sql_prompt'] for id in range(len(ds_test))]\n",
    "contexts = [ds_test[id]['sql_context'] for id in range(len(ds_test))]\n",
    "\n",
    "responses = []\n",
    "print(f\"Start time: {time.ctime(time.time())}\")\n",
    "for i in tqdm(range(0, len(prompts), BATCH_SIZE)):\n",
    "    batch_prompts = prompts[i : i + BATCH_SIZE]\n",
    "    batch_contexts = contexts[i : i + BATCH_SIZE]\n",
    "\n",
    "    messages_list = [\n",
    "        construct_message(prompt=p, context=c)\n",
    "        for p, c in zip(batch_prompts, batch_contexts)\n",
    "    ]\n",
    "\n",
    "    batch_responses = generate_model_response_batch(messages_list, enable_thinking=ENABLE_THINKING, max_new_tokens=MAX_NEW_TOKENS)\n",
    "\n",
    "    responses.extend(batch_responses)\n",
    "\n",
    "print(f\"End time: {time.ctime(time.time())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "941358bb-1995-4b19-abf1-3b8b48201366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a251ffe22e4f4d89b291e2b3e268debd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5851 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "references = [ds_test[id]['sql'] for id in range(len(ds_test))]\n",
    "predictions = [responses[id]['content'] for id in range(len(ds_test))]\n",
    "\n",
    "scores = [\n",
    "    evaluate_sql_response(\n",
    "        reference=reference,\n",
    "        prediction=prediction,\n",
    "        sql_context=context\n",
    "    )\n",
    "    for reference, prediction, context in tqdm(zip(references, predictions, contexts), total=len(ds_test))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9495348-a4b1-42fd-a86e-94b49095729b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean test set score: 0.680\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean test set score: {np.mean([score['final_score'] for score in scores]):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bc5fb4-0fcd-422a-b6af-4e950febf344",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python t4",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
