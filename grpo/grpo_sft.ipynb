{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfe20728-8bcc-47fc-8148-a2144c58e259",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49d56f23-d923-4899-85c1-b91adf512c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-27 09:00:59.917388: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[2026-01-27 09:01:18] INFO _client.py:1025: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "[2026-01-27 09:01:18] INFO _client.py:1025: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "[2026-01-27 09:01:18] INFO _client.py:1025: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "[2026-01-27 09:01:19] INFO _client.py:1025: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "[2026-01-27 09:01:19] INFO _client.py:1025: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "[2026-01-27 09:01:19] INFO _client.py:1025: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import datasets\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import evaluate\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import sqlite3\n",
    "import sqlparse\n",
    "import _config\n",
    "\n",
    "import os\n",
    "import psutil\n",
    "import GPUtil\n",
    "import gc\n",
    "\n",
    "\n",
    "# Set the verbosity to WARNING to suppress INFO messages\n",
    "evaluate.logging.set_verbosity_warning()\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"WANDB_API_KEY\"] = _config.WANDB_API_KEY\n",
    "os.environ[\"WANDB_PROJECT\"] = _config.WANDB_PROJECT\n",
    "\n",
    "ENABLE_THINKING = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de55c654-917a-406c-bc81-201d4397e62f",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "525fe697-fbe6-4939-a379-b320f0bf0ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "CPU 0 load: 0.00\n",
      "CPU 1 load: 0.00\n",
      "CPU 2 load: 1.00\n",
      "CPU 3 load: 0.00\n",
      "RAM Total: 27.40 GB, Used: 1.85 GB\n",
      "GPU 0 (Tesla T4) load: 0.0%\n",
      "GPU 0 (Tesla T4) VRAM Total: 16384.0 MB, Used 3.0 MB\n",
      "Disk Total: 60.95 GB, Used: 51.38 GB\n"
     ]
    }
   ],
   "source": [
    "def get_vm_usage_metrics():\n",
    "    # CPU usage\n",
    "    cpu_load = psutil.cpu_percent(interval=1, percpu=True)\n",
    "    for id, load in enumerate(cpu_load):\n",
    "        print(f\"CPU {id} load: {load:.2f}\")\n",
    "    # RAM usage\n",
    "    ram = psutil.virtual_memory()\n",
    "    print(f\"RAM Total: {ram.total/(1024**3):.2f} GB, Used: {(ram.used)/(1024**3):.2f} GB\")\n",
    "    # GPU\n",
    "    if torch.cuda.is_available():\n",
    "        gpus = GPUtil.getGPUs()\n",
    "        for gpu in gpus:\n",
    "            print(f\"GPU {gpu.id} ({gpu.name}) load: {gpu.load*100}%\")\n",
    "            print(f\"GPU {gpu.id} ({gpu.name}) VRAM Total: {gpu.memoryTotal} MB, Used {gpu.memoryUsed} MB\")\n",
    "    # Disk \n",
    "    disk = psutil.disk_usage('/')\n",
    "    print(f\"Disk Total: {disk.total/(1024**3):.2f} GB, Used: {(disk.used)/(1024**3):.2f} GB\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f'Device: {device}')\n",
    "get_vm_usage_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a546b1f-a4ba-4371-9b18-15ab009d0ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_response_batch(model, tokenizer, messages_list, enable_thinking=False, max_new_tokens=512):\n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=enable_thinking\n",
    "        )\n",
    "        for messages in messages_list\n",
    "    ]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        padding_side='left'\n",
    "    ).to(model.device)\n",
    "\n",
    "    model.eval()\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=max_new_tokens\n",
    "    )\n",
    "\n",
    "    responses = []\n",
    "    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids):\n",
    "        # Slice to get only generated part\n",
    "        output_only_ids = output_ids[len(input_ids):].tolist()\n",
    "\n",
    "        # Try to find `</think>` (id 151668)\n",
    "        try:\n",
    "            index = len(output_only_ids) - output_only_ids[::-1].index(151668)\n",
    "        except ValueError:\n",
    "            index = 0\n",
    "\n",
    "        if enable_thinking:\n",
    "            thinking_content = tokenizer.decode(\n",
    "                output_only_ids[:index],\n",
    "                skip_special_tokens=True\n",
    "            ).strip(\"\\n\")\n",
    "            content = tokenizer.decode(\n",
    "                output_only_ids[index:],\n",
    "                skip_special_tokens=True\n",
    "            ).strip(\"\\n\")\n",
    "        else:\n",
    "            thinking_content = None\n",
    "            content = tokenizer.decode(\n",
    "                output_only_ids,\n",
    "                skip_special_tokens=True\n",
    "            ).strip(\"\\n\")\n",
    "\n",
    "        responses.append({\n",
    "            'thinking_content': thinking_content,\n",
    "            'content': content\n",
    "        })\n",
    "\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f2fc14-5482-4a8c-b9cb-41410dbc89ec",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95c19a40-edd8-4972-9a7e-b3ee05c35df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'domain', 'domain_description', 'sql_complexity', 'sql_complexity_description', 'sql_task_type', 'sql_task_type_description', 'sql_prompt', 'sql_context', 'sql', 'sql_explanation'],\n",
       "    num_rows: 19500\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = datasets.load_dataset('gretelai/synthetic_text_to_sql', streaming=False)\n",
    "ds_train, ds_test = ds['train'], ds['test']\n",
    "\n",
    "ds_train_subset = ds_train.train_test_split(test_size=0.2, seed=42)['test']\n",
    "split = ds_train_subset.train_test_split(test_size=0.025, seed=42)\n",
    "ds_train = split['train']\n",
    "ds_valid = split['test']\n",
    "\n",
    "ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c786aa8c-a47f-4f32-ac54-6d36e4fa3c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'domain', 'domain_description', 'sql_complexity', 'sql_complexity_description', 'sql_task_type', 'sql_task_type_description', 'sql_prompt', 'sql_context', 'ground_truth', 'sql_explanation', 'prompt'],\n",
       "    num_rows: 19500\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# datasets for GRPO must include a column \"prompt\"\n",
    "def construct_message(example):\n",
    "    return {\"prompt\": [\n",
    "        {\"role\": \"system\", \"content\": f\"The user asks a question. Your task is to generate the SQL query to answer that question. Return SQL query only. The context of the question is the following: '{example['sql_context']}'\"},\n",
    "        {\"role\": \"user\", \"content\": example['sql_prompt']}\n",
    "    ]}\n",
    "ds_train = ds_train.map(construct_message)\n",
    "ds_valid = ds_valid.map(construct_message)\n",
    "\n",
    "# rename the ground_truth column\n",
    "ds_train = ds_train.rename_column(\"sql\", \"ground_truth\")\n",
    "ds_valid = ds_valid.rename_column(\"sql\", \"ground_truth\")\n",
    "\n",
    "ds_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4491c8-ac1b-4cac-a00b-09901446e3e8",
   "metadata": {},
   "source": [
    "# Reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52d04f4a-a806-4ab7-be3c-288295542163",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def normalize_sql(sql):\n",
    "    return sqlparse.format(sql, reindent=True, keyword_case='upper').strip()\n",
    "\n",
    "def compute_rouge(reference, prediction):\n",
    "    result = rouge.compute(predictions=[prediction], references=[reference])\n",
    "    return result['rougeL']\n",
    "\n",
    "\n",
    "def evaluate_sql_response(reference, prediction, sql_context):\n",
    "    # ROUGE-L\n",
    "    rouge_score = compute_rouge(reference, prediction)\n",
    "\n",
    "    for ref, pred,  in zip(reference, prediction):\n",
    "        # execution check\n",
    "        try:\n",
    "            conn = sqlite3.connect(\":memory:\")\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            cursor.executescript(sql_context)\n",
    "            cursor.execute(reference)\n",
    "            ref_result = cursor.fetchall()\n",
    "            \n",
    "            cursor.execute(prediction)\n",
    "            model_result = cursor.fetchall()\n",
    "            \n",
    "            execution_match = ref_result == model_result\n",
    "        except Exception:\n",
    "            execution_match = False\n",
    "        finally:\n",
    "            conn.close()\n",
    "        \n",
    "        # final score\n",
    "        if execution_match:\n",
    "            final_score = 1.0\n",
    "        else:\n",
    "            final_score = 0.7 * rouge_score\n",
    "\n",
    "    return {\n",
    "        \"rougeL\": round(rouge_score, 4),\n",
    "        \"execution_match\": execution_match,\n",
    "        \"final_score\": final_score\n",
    "    }\n",
    "\n",
    "\n",
    "def reward_func(completions, ground_truth, sql_context, **kwargs):\n",
    "    scores = [\n",
    "        evaluate_sql_response(\n",
    "            reference=reference,\n",
    "            prediction=prediction[0][\"content\"],\n",
    "            sql_context=context\n",
    "        )[\"final_score\"]\n",
    "        for reference, prediction, context in zip(ground_truth, completions, sql_context)\n",
    "    ]\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd8f046-2cc9-4c40-b9a5-2ee9d98e19c1",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "859dac18-d643-4903-ba52-5848afc209b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-27 09:01:34] INFO modeling.py:1004: We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU 0 load: 0.00\n",
      "CPU 1 load: 0.00\n",
      "CPU 2 load: 1.00\n",
      "CPU 3 load: 0.00\n",
      "RAM Total: 27.40 GB, Used: 2.08 GB\n",
      "GPU 0 (Tesla T4) load: 0.0%\n",
      "GPU 0 (Tesla T4) VRAM Total: 16384.0 MB, Used 3371.0 MB\n",
      "Disk Total: 60.95 GB, Used: 51.38 GB\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"Qwen/Qwen3-0.6B\"\n",
    "model_path = \"./qlora-final_model_all_linear_r64-output/final/\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, padding_side='left')\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    # attn_implementation=\"sdpa\",\n",
    "    device_map=\"auto\",\n",
    "    dtype=torch.float32,\n",
    "    # quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "# Manually set LoRA parameters as trainable\n",
    "for name, param in model.named_parameters():\n",
    "    if \"lora\" in name:\n",
    "        param.requires_grad = True\n",
    "\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.gradient_checkpointing_enable()\n",
    "# model = prepare_model_for_kbit_training(model)\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "get_vm_usage_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21da278c-ac63-4dc4-903a-f65a3e1de9dc",
   "metadata": {},
   "source": [
    "# GRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81214b5-35fd-4109-b4b7-4a36c258b177",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: [wandb.login()] Loaded credentials for https://api.wandb.ai from WANDB_API_KEY.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33molialeshka\u001b[0m (\u001b[33molialeshka-none\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.24.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/olialeshka_1/wandb/run-20260127_090215-yrm8qwl9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/olialeshka-none/text-to-sql/runs/yrm8qwl9' target=\"_blank\">sft-grpo-lora-lr1e6-ngen4-epochs1-2026-01-25_09-24-49</a></strong> to <a href='https://wandb.ai/olialeshka-none/text-to-sql' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/olialeshka-none/text-to-sql' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/olialeshka-none/text-to-sql/runs/yrm8qwl9' target=\"_blank\">https://wandb.ai/olialeshka-none/text-to-sql/runs/yrm8qwl9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference, openai] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "/home/olialeshka_1/myenv/lib/python3.12/site-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/home/olialeshka_1/myenv/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:285: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "[2026-01-27 09:02:19] INFO spawn.py:77: x86_64-linux-gnu-gcc -fno-strict-overflow -Wsign-compare -DNDEBUG -g -O2 -Wall -fPIC -c /tmp/tmpdbwy_1wc/test.c -o /tmp/tmpdbwy_1wc/test.o\n",
      "[2026-01-27 09:02:19] INFO spawn.py:77: x86_64-linux-gnu-gcc /tmp/tmpdbwy_1wc/test.o -laio -o /tmp/tmpdbwy_1wc/a.out\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "[2026-01-27 09:02:19] INFO spawn.py:77: x86_64-linux-gnu-gcc -fno-strict-overflow -Wsign-compare -DNDEBUG -g -O2 -Wall -fPIC -c /tmp/tmppkophacb/test.c -o /tmp/tmppkophacb/test.o\n",
      "[2026-01-27 09:02:19] INFO spawn.py:77: x86_64-linux-gnu-gcc /tmp/tmppkophacb/test.o -L/usr -L/usr/lib64 -lcufile -o /tmp/tmppkophacb/a.out\n",
      "/usr/bin/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Training Setup Summary =====\n",
      "Num epochs:            1\n",
      "Effective batch size:  16\n",
      "Per-device batch size: 8\n",
      "Gradient accumulation: 2\n",
      "Dataset size:          19500\n",
      "Steps per epoch:       1218\n",
      "Total training steps:  1218\n",
      "Warmup steps:          121\n",
      "Logging steps:         30\n",
      "===================================\n",
      "Start time: 2026-01-27_09-02-20\n",
      "Resuming training from checkpoint: ./sft-grpo-lr1e6-ngen4-output/checkpoint-4020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-27 09:02:39] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:02:39] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:02:40] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:02:40] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:02:40] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:02:40] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:02:40] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:02:40] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:02:40] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:02:40] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:02:41] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:02:41] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:02:41] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:02:41] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:02:41] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:02:41] INFO rouge_scorer.py:83: Using default tokenizer.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4030' max='4875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4030/4875 01:50 < 3:15:14, 0.07 it/s, Epoch 0.83/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-27 09:02:50] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:02:51] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:02:51] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:02:51] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:02:51] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:02:51] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:02:51] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:02:51] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:02:51] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:02:52] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:02:52] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:02:52] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:02:52] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:02:52] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:02:52] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:02:52] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:02] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:02] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:02] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:02] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:02] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:02] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:02] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:02] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:03] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:03] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:03] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:03] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:03] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:03] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:03] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:03] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:14] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:14] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:14] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:14] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:14] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:15] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:15] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:15] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:15] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:15] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:15] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:15] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:15] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:16] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:16] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:16] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:26] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:26] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:26] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:26] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:26] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:26] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:26] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:27] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:27] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:27] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:27] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:27] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:27] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:27] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:27] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:28] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:40] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:41] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:41] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:41] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:41] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:41] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:41] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:41] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:41] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:42] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:42] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:42] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:42] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:42] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:42] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:42] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:56] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:56] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:57] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:57] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:57] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:57] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:57] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:58] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:58] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:58] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:58] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:58] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:58] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:58] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:58] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:03:59] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:04:15] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:04:15] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:04:15] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:04:15] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:04:15] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:04:15] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:04:15] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:04:16] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:04:16] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:04:16] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:04:16] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:04:16] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:04:16] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:04:16] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:04:16] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:04:17] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:04:29] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:04:30] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:04:30] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:04:30] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:04:30] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:04:30] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:04:30] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:04:30] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:04:30] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:04:31] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:04:31] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:04:31] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:04:31] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:04:31] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:04:31] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:04:31] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:04:42] INFO rouge_scorer.py:83: Using default tokenizer.\n",
      "[2026-01-27 09:04:43] INFO rouge_scorer.py:83: Using default tokenizer.\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "# timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "timestamp = '2026-01-25_09-24-49'\n",
    "RUN_NAME = f'sft-grpo-lora-lr1e6-ngen4-epochs1-{timestamp}'\n",
    "OUTPUT_DIR = './sft-grpo-lr1e6-ngen4-output'\n",
    "RESUME_TRAINING = True\n",
    "\n",
    "PER_DEVICE_BATCH_SIZE = 8\n",
    "effective_batch_size = 16\n",
    "epochs=1\n",
    "learning_rate = 1e-6 # changed from 1e-5\n",
    "warmup_ratio = 0.1\n",
    "lora_r = 16*4\n",
    "lora_alpha = 64*4\n",
    "lora_dropout = 0.01\n",
    "\n",
    "gradient_accumulation_steps = int(effective_batch_size / PER_DEVICE_BATCH_SIZE)\n",
    "\n",
    "wandb.init(\n",
    "    project=os.environ[\"WANDB_PROJECT\"],\n",
    "    name=RUN_NAME,\n",
    "    id='yrm8qwl9' ,         # resume previous run if available\n",
    "    resume=\"allow\",    # allows resuming crashed run\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    \n",
    "    chat_template_kwargs = {\"enable_thinking\": False},\n",
    "    num_train_epochs=epochs,\n",
    "    num_generations=4,\n",
    "    # use_liger_kernel=True,\n",
    "    \n",
    "    per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=30,\n",
    "    save_total_limit=2,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=30,\n",
    "    per_device_eval_batch_size=PER_DEVICE_BATCH_SIZE*4,\n",
    "    eval_accumulation_steps=1,\n",
    "    # eval_kwargs={\"num_generations\": 1},\n",
    "    num_generations_eval=1,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=30,\n",
    "    report_to=['wandb'],\n",
    "    run_name=RUN_NAME,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    max_grad_norm=1,\n",
    "    load_best_model_at_end=True,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    # generate_during_eval=True\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules='all-linear'\n",
    ")\n",
    "# model.requires_grad_(False)                     # freeze base weights (precautionary)\n",
    "model_peft = get_peft_model(model, peft_config) # inject a LoRA adapter\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    processing_class=tokenizer,\n",
    "    model=model_peft,\n",
    "    args=training_args,\n",
    "    reward_funcs=[reward_func],\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_valid,\n",
    "    # callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "\n",
    "# Training setup summary\n",
    "dataset_size = len(ds_train)\n",
    "steps_per_epoch = dataset_size // (PER_DEVICE_BATCH_SIZE * gradient_accumulation_steps)\n",
    "total_steps = steps_per_epoch * epochs\n",
    "warmup_steps = int(total_steps * warmup_ratio)\n",
    "\n",
    "print(\"===== Training Setup Summary =====\")\n",
    "print(f\"Num epochs:            {epochs}\")\n",
    "print(f\"Effective batch size:  {effective_batch_size}\")\n",
    "print(f\"Per-device batch size: {PER_DEVICE_BATCH_SIZE}\")\n",
    "print(f\"Gradient accumulation: {gradient_accumulation_steps}\")\n",
    "print(f\"Dataset size:          {dataset_size}\")\n",
    "print(f\"Steps per epoch:       {steps_per_epoch}\")\n",
    "print(f\"Total training steps:  {total_steps}\")\n",
    "print(f\"Warmup steps:          {warmup_steps}\")\n",
    "print(f\"Logging steps:         {training_args.logging_steps}\")\n",
    "print(\"===================================\")\n",
    "print(f\"Start time: {datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\")\n",
    "\n",
    "\n",
    "# Training\n",
    "last_checkpoint = None\n",
    "if RESUME_TRAINING and os.path.isdir(OUTPUT_DIR):\n",
    "    last_checkpoint = get_last_checkpoint(OUTPUT_DIR)\n",
    "\n",
    "if last_checkpoint is not None:\n",
    "    print(f\"Resuming training from checkpoint: {last_checkpoint}\")\n",
    "    trainer.train(resume_from_checkpoint=last_checkpoint)\n",
    "else:\n",
    "    print(\"Starting fresh training run\")\n",
    "    trainer.train()\n",
    "\n",
    "print(f\"End time: {datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8978293a-2f92-4370-b6f1-e4bd7bb959dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained(f\"{OUTPUT_DIR}/best_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7495893-c8fb-4d81-a16a-09346fe6aa9b",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc97fe8b-51ec-4679-b391-0fe44ae89737",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = './sft-grpo-lr1e6-ngen4-output'\n",
    "checkpoint = f\"{OUTPUT_DIR}/checkpoint-4875/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, dtype=torch.float16).to(device)\n",
    "model.eval()\n",
    "\n",
    "ds = datasets.load_dataset('gretelai/synthetic_text_to_sql', streaming=False)\n",
    "ds_train, ds_test = ds['train'], ds['test']\n",
    "\n",
    "\n",
    "def construct_message(prompt, context):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": f\"The user asks a question. Your task is to generate the SQL query to answer that question. Return SQL query only. The context of the question is the following: '{context}'\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "def generate_model_response_batch(messages_list, enable_thinking=True, max_new_tokens=512):\n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=enable_thinking\n",
    "        )\n",
    "        for messages in messages_list\n",
    "    ]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        padding_side='left'\n",
    "    ).to(model.device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=max_new_tokens\n",
    "    )\n",
    "\n",
    "    responses = []\n",
    "    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids):\n",
    "        # Slice to get only generated part\n",
    "        output_only_ids = output_ids[len(input_ids):].tolist()\n",
    "\n",
    "        # Try to find `</think>` (id 151668)\n",
    "        try:\n",
    "            index = len(output_only_ids) - output_only_ids[::-1].index(151668)\n",
    "        except ValueError:\n",
    "            index = 0\n",
    "\n",
    "        if enable_thinking:\n",
    "            thinking_content = tokenizer.decode(\n",
    "                output_only_ids[:index],\n",
    "                skip_special_tokens=True\n",
    "            ).strip(\"\\n\")\n",
    "            content = tokenizer.decode(\n",
    "                output_only_ids[index:],\n",
    "                skip_special_tokens=True\n",
    "            ).strip(\"\\n\")\n",
    "        else:\n",
    "            thinking_content = None\n",
    "            content = tokenizer.decode(\n",
    "                output_only_ids,\n",
    "                skip_special_tokens=True\n",
    "            ).strip(\"\\n\")\n",
    "\n",
    "        responses.append({\n",
    "            'thinking_content': thinking_content,\n",
    "            'content': content\n",
    "        })\n",
    "\n",
    "    return responses\n",
    "\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def normalize_sql(sql):\n",
    "    return sqlparse.format(sql, reindent=True, keyword_case='upper').strip()\n",
    "\n",
    "def compute_rouge(reference, prediction):\n",
    "    result = rouge.compute(predictions=[prediction], references=[reference])\n",
    "    return result['rougeL']\n",
    "\n",
    "def evaluate_sql_response(reference, prediction, sql_context):\n",
    "    # ROUGE-L\n",
    "    rouge_score = compute_rouge(reference, prediction)\n",
    "    \n",
    "    # execution check\n",
    "    try:\n",
    "        conn = sqlite3.connect(\":memory:\")\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.executescript(sql_context)\n",
    "        cursor.execute(reference)\n",
    "        ref_result = cursor.fetchall()\n",
    "        \n",
    "        cursor.execute(prediction)\n",
    "        model_result = cursor.fetchall()\n",
    "        \n",
    "        execution_match = ref_result == model_result\n",
    "    except Exception:\n",
    "        execution_match = False\n",
    "    finally:\n",
    "        conn.close()\n",
    "    \n",
    "    # final score\n",
    "    if execution_match:\n",
    "        final_score = 1.0\n",
    "    else:\n",
    "        final_score = 0.7 * rouge_score\n",
    "\n",
    "    return {\n",
    "        \"rougeL\": round(rouge_score, 4),\n",
    "        \"execution_match\": execution_match,\n",
    "        \"final_score\": final_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081b17b1-4e0b-4df0-8377-21bb25aa93b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "ENABLE_THINKING = False\n",
    "MAX_NEW_TOKENS = 512\n",
    "\n",
    "\n",
    "prompts = [ds_test[id]['sql_prompt'] for id in range(len(ds_test))]\n",
    "contexts = [ds_test[id]['sql_context'] for id in range(len(ds_test))]\n",
    "\n",
    "responses = []\n",
    "print(f\"Start time: {time.ctime(time.time())}\")\n",
    "for i in tqdm(range(0, len(prompts), BATCH_SIZE)):\n",
    "    batch_prompts = prompts[i : i + BATCH_SIZE]\n",
    "    batch_contexts = contexts[i : i + BATCH_SIZE]\n",
    "\n",
    "    messages_list = [\n",
    "        construct_message(prompt=p, context=c)\n",
    "        for p, c in zip(batch_prompts, batch_contexts)\n",
    "    ]\n",
    "\n",
    "    batch_responses = generate_model_response_batch(messages_list, enable_thinking=ENABLE_THINKING, max_new_tokens=MAX_NEW_TOKENS)\n",
    "\n",
    "    responses.extend(batch_responses)\n",
    "\n",
    "print(f\"End time: {time.ctime(time.time())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941358bb-1995-4b19-abf1-3b8b48201366",
   "metadata": {},
   "outputs": [],
   "source": [
    "references = [ds_test[id]['sql'] for id in range(len(ds_test))]\n",
    "predictions = [responses[id]['content'] for id in range(len(ds_test))]\n",
    "\n",
    "scores = [\n",
    "    evaluate_sql_response(\n",
    "        reference=reference,\n",
    "        prediction=prediction,\n",
    "        sql_context=context\n",
    "    )\n",
    "    for reference, prediction, context in tqdm(zip(references, predictions, contexts), total=len(ds_test))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9495348-a4b1-42fd-a86e-94b49095729b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean test set score: 0.756\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean test set score: {np.mean([score['final_score'] for score in scores]):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bc5fb4-0fcd-422a-b6af-4e950febf344",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python t4",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
